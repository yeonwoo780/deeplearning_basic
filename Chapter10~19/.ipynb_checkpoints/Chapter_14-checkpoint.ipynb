{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aee6782",
   "metadata": {},
   "source": [
    "## 베스트 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57444ad6",
   "metadata": {},
   "source": [
    "###  레드와인 샘플 1,599개, 화이트와인 샘플 4,898개의 등급과 맛, 산도 등을 측정해 분석한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77f8832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 13)\n",
      "(975, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037</td>\n",
       "      <td>24.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.99094</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10.65</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>10.2</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.99760</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>8.4</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.076</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99735</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.51</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.044</td>\n",
       "      <td>62.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.99760</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.44</td>\n",
       "      <td>8.80</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.086</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.99824</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1     2     3      4     5      6        7     8     9      10  \\\n",
       "6245   5.9  0.180  0.28   1.0  0.037  24.0   88.0  0.99094  3.29  0.55  10.65   \n",
       "486   10.2  0.670  0.39   1.9  0.054   6.0   17.0  0.99760  3.17  0.47  10.00   \n",
       "273    8.4  0.715  0.20   2.4  0.076  10.0   38.0  0.99735  3.31  0.64   9.40   \n",
       "1993   6.8  0.370  0.51  11.8  0.044  62.0  163.0  0.99760  3.19  0.44   8.80   \n",
       "885    8.9  0.750  0.14   2.5  0.086   9.0   30.0  0.99824  3.34  0.64  10.50   \n",
       "\n",
       "      11  12  \n",
       "6245   7   0  \n",
       "486    5   1  \n",
       "273    5   1  \n",
       "1993   5   0  \n",
       "885    5   1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 3\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "df_pre = pd.read_csv('../dataset/wine.csv', header=None)\n",
    "print(df_pre.shape)\n",
    "\n",
    "df = df_pre.sample(frac = 0.15) # 15%\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee900e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 975 entries, 6245 to 2308\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       975 non-null    float64\n",
      " 1   1       975 non-null    float64\n",
      " 2   2       975 non-null    float64\n",
      " 3   3       975 non-null    float64\n",
      " 4   4       975 non-null    float64\n",
      " 5   5       975 non-null    float64\n",
      " 6   6       975 non-null    float64\n",
      " 7   7       975 non-null    float64\n",
      " 8   8       975 non-null    float64\n",
      " 9   9       975 non-null    float64\n",
      " 10  10      975 non-null    float64\n",
      " 11  11      975 non-null    int64  \n",
      " 12  12      975 non-null    int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 106.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f27917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "\n",
    "X = dataset[:, 0:12]\n",
    "Y = dataset[:, 12]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = 12, activation = 'relu'))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa50560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "51085e94",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAMrCAYAAADnaI6xAAAgAElEQVR4Aexdq5LrMJDdfzIyCgsaZGYUFhQUFhQUZpQfCAoLCgpL1VaFGRllt7YqS4KyyMjsbHVLsiVZfs3jztw7PVX3xpb1aB3J0nGr1foPyJ8gIAgIAoKAICAICAKCwLci8B/fWroULggIAoKAICAICAKCgCAAIWTSCQQBQUAQEAQEAUFAEPhmBISQfXMDSPGCgCAgCAgCgoAgIAgIIZM+IAgIAoKAICAICAKCwDcjIITsmxtAihcEBAFBQBAQBAQBQUAImfQBQUAQEAQEAUFAEBAEvhkBIWTf3ABSvCAgCAgCgoAgIAgIAkLIpA8IAoKAICAICAKCgCDwzQgIIfvmBpDiBQFBQBAQBAQBQUAQEEImfUAQEAQEAUFAEBAEBIFvRkAI2Tc3gBQvCAgCgoAgIAgIAoKAEDLpA4KAICAICAKCgCAgCHwzAkLIvrkBpHhBQBAQBAQBQUAQEASEkEkfEAQEAUFAEBAEBAFB4JsRGEXI/vt//hfyTzCQPiB9QPqA9AHpA9IHpA+M7wNTON5oQjYlU4n77yNAL6T8CQKCgCAgCAgCgkAYganzpBCyMI4SOoDA1I42kJ08FgQEAUFAEBAE/ikEps6TQsj+qeb/c5WZ2tH+nGRSkiAgCAgCgoAg8P0ITJ0nhZB9f5v9lRJM7Wh/ZSVFaEFAEBAEBAFB4J0ITJ0nhZC9E+jfnmxqR/vteEn9BQFBQBAQBH4XAlPnSSFkv6t/fFptp3a0TytYMhIEBAFBQBAQBP4CBKbOk0LIBhr1dd0iTVNsr6+BmACeV+z3e+yvz+G4f3mMqR3tL6+uiC8IfC8C5QN5niN/lJYcBY5pijQ9orBCOy9/0fjUiYE8EAT+IAJT50khZNQ4zxuydYJZFCGK51hsTij0uPc6rxBFEVZnImQVnkWuBkYaHJ1/BZ63jONG2ajh8Q92i88vampH+3wJJEdB4BchUITGlgIZjVlRpgmZuacw+59+HszjF2EoVRUE/jACU+dJIWTVDbuYiFiK7emGy2GpiNnihAcAl5C9cF7ZA519vcL5HBo0/3AP+EPFTe1of0gsKUYQ+CcRKC9rRbLWF5SvM1YhwgVDyHa4lSXK8oadE0+PV7/gg/Gf7ARSqb8Oganz5K8nZOV1wwNdeiL6RX8VbjsauN5wuPuETEehn9DXZijMSvIvXU7taP9S3aUugsAfRaAqkCXm4y9Blr9QOoTL15D595qg3Xa/RoP/R9tHChMEOhCYOk8KIXsvIcu1Nmx3Q2UaQwiZQUJ+BQFB4MMIVCifOfaLGFEUY3U4YEPa/GiG5eGGJxG14JKlELIPQy8ZCAKfgIAQsqkgTlqybDK/H97U1+bbAXcTbAiZXib4l1cGpnY0A5H8CgKCwFgESlw3iowtDnf14fe8YJvGiDdXlPUSpU/AjDat4/dfHpjGQivxBIE/gMDUefLXa8i4TUYb9esWpC/TNzPYxVhd9A5MQ8jWRzb4dzZE/YHG/5NFTO1of1I2KUsQ+GcQKB+4vypUvERJdmH074XXi5YtX3g+Hng8XrWW3o1n4msdvhmfhJD9M91DKvKzEZg6TwohG2hP16ifIlcosoS1Y+vzWW8IWOBwr8J2ZQP5/62Pp3a0v7WeIrcg8P0ImKVJ8xHo/xoNWVtSm6BVj9/jlqeNhIQIAn8eganzpBAy3UZFpgY5/+PR8UNWPXHZpoijCPFS7cKsigwJL1HOsNnonVB+Jn++H3x5iVM72pcLJAUIAv8wAjaxUloyexelT8ieuOwWmLO9mU3eZkjWx9qlzz8Ml1RNEPgRCEydJ4WQ6WbrImRuqyrt2Gx5BCnEzF/1OGO3u+BpDP2FkBlo5FcQEAQ+jMA0DdnztGAN/up4x6sepyq8bhne6GNyl9dLnB8WTTIQBASBTgSEkHVC0//AELLdzdhd+L/1yNad0S+y0Zja0bpBkyeCgCDQj4AhZMa/mD82lQ7BMj7Lku0JxVPHfT2QH1as3X8jfz7yJwgIAl+OwNR5UjRkukkMIXM9XFvq/tUZg4cnCSH78g4uBQgCvw8BQ8is8chz+Ooq5Ss8Lntslqm1bBljni6xOebD49jvA1hqLAh8CQJCyL4E1rGZVmoX1Ahl2tgcf2q8qR3tp9ZD5BIEBAFBQBAQBL4CganzpGjIvqIVfkGeUzvaL4BEqigICAKCgCAgCNQITJ0nhZDV0MnFFASmdrQpeUtcQUAQEAQEAUHgb0dg6jwphOxvb/Fvkn9qR/smMaVYQUAQEAQEAUHgWxCYOk8KIfuWZvr7C53a0f7+GksNBAFBQBAQBASB8QhMnSeFkI3HVmJaCEztaFZSuRQEBAFBQBAQBP55BKbOk6MJGWUs/wQD6QPSB6QPSB+QPiB9QPrAuD4whXWOJmRTMpW4/z4C9DLKnyAgCAgCgoAgIAiEEZg6TwohC+MooQMITO1oA9nJY0FAEBAEBAFB4J9CYOo8KYTsn2r+P1eZqR3tz0kmJQkCgoAgIAgIAt+PwNR5UgjZ97fZXynB1I72V1ZShBYEBAFBQBAQBN6JwNR5UgjZO4H+7cmmdrTfjpfUXxAQBAQBQeB3ITB1nhRC9rv6x6fVdmpH+7SCJSNBQBAQBAQBQeAvQGDqPCmE7C9o1J8o4tSONrUO1bNAnuco7jec9nvsTwXKQCZVWaoD3en3o4e6l0VvWW7xBY5pijQ9onAfAEWGKIoQZdaTUJifru++OCKl8o5Wnn3x/+Qzxu2EItRAITnKB/L8EWjPF86rCKvzy0qlcJ5S7fKRI3+MFcYqSi4FAUFAEPhEBKbOk0LIPhH835RVb0cz5INISce/mqu8zlj5cVZnFOcVp11lmXq+OqOepqs7TpsFZn46up8tsDndEeZmJR55zkSPyJ7zr3iiMrKYsnrqkRUFMi4/04TM3IfrnGUBkuZ1mCILpI33Kn8jSw0cGuIXwoHDVrC5zeu6VaSOiSSRyebfPPaJkCfcK8dhnSBJUqRJgnR9QG4ahHGzyjLk0co/3V6b9uO6GNzscsKEjHC2qw1UTMLDbUyweGTYLsK7flE/M+3tPZNbQUAQEAQ+gkDvPBnIWAhZABQJGkagt6MZ8rC7Ndorrcm67RTpqCdYQ4KWJ9wpzuuF5/OF+2nZSciKw4yfzbObq2Upb8jmlP8Mh6AiqYc00aRsZKknaDXxl+UNOyY4O9x0PSqYvBpiUWvrXk88Hg88ni+8TPxcE7L1EXle4BliE5Wl7cv3mIeIVg0ctZGRz0rH5d1xWhIOFkkC8H5C9sAxibA6PeuO8TytECVHPCjEJ2R1LHVhSM/lqAlgQu1ncHvhum2IoU0S6fqoia9T7YHyXEKmSF4898rQBNHIZrilJ7rcCgKCgCDwbgR658lArkLIAqBI0DACvR3NEDJnFlV5Gi1Q/cgnQeZek5HP1ZA19WrJQY9M2TUh0/EfJ6QsT4oTMxAKN4RMEUy1zPZkchHbRGq2xPFeedoslyg1UjVXj2PKpHN9fipSe9vxvbMMOpWQvVcbFCJAdph93VShvmqRnoCGrCyOWKcJkjRFMp9jsbtA0T+F84w0c+kWV2JOA+WFCJm7DFqLhpZszSO5EgQEAUHgQwj0zpOBnIWQBUCRoGEEejvaRwiZ1hI9+pYsLfFqrRRphkJaJytuc1nBaOp2twpgm6Yc+SVTxMshZBXy7E2RoShCvDo7RCGK1jiSrRupvIqDWkbd3tSS6eOIBZGz9QWvHkwaudRVVWRIKF2S4fbU2q8QITN52gTQuXaJH5OPeO4sVToaqU5DrYkaMpZrxuSqzn97xTWoISNFX45d/IYsNw34wmUdYcEaOUXIagJPEAkh87uM3AsCgsAPRKB3ngzIK4QsAIoEDSPQ29EMUZiyZOkRiSwL25AxqXDiKg1VyFbNmcSdKt1xeFPpYopk5DX51oSsxP20ViRrccBhFzMxixcZzsVZL2OapTcqYISGrFsoVK8HboelKi/Z4vrMtZ2aVUc7vZHbDnPq+Yk3zxuy1RzzUTZktDxr4+LJwXJbz7+bkBmSatu5eSLLrSAgCAgCUxHonScDmQkhC4AiQcMI9HY0QxQMwQn8NhyibQdFepLOXZa2nZW2zyo7fo2+xa8NaaDejEzxBlezIY81L5E28q5wP6hlw2i2xoXXzwzhekNW3DyjfrsUUydLAqOF69r9V16xiSPE8xWyc9EYwJtsjaG8rcWagHNhtFO2oX3wOrBr1MjQ9Vs9Udh2cSyXpyFLU6zMeq9PyAD0LVnSbla72qIh62oICRcEBIGfhEDvPBkQVAhZABQJGkZgfEcztlaWRiSQ/aDmq9Zajdld2FNWVSBLSOO0xvm8A9l71cuQDiEjVnjH5XRDzaE0GXw9n3hVD1zJHcf+qpcwVaUG69Ew0RYKleZv5f2CbJ0imRnNGJGbNbLLw9s9aohfifJ+wpJIptkcoUmqRQl7jfp5adFhPS3xuu2tBpYQWzk9ry3czAYFW16TjpalnfBgeYTFE49n5e2yDO3cpJwpfoXne+3qjHDyKwgIAoJABwLj50mVgRCyDiAluB+B8R1tHCEDTaa0M9H/lx+UHZZNyKpXOx6nu2DLmq8OQva6IUvVsmNyuJNeBtet2rEZpzvcCu2Cwy7LaG94Z6AhSOp3lqxx9J1vdWnwQjZgIYifJ1XfZItToQ36nwVO24SXS5VdVSChTyYDUfqCXEP4cEwmmx42HDNEkF5XbIMaOLPb0dPEhfLgzLtsyGLMvfyXmwyH2zNIyOxdluy6I02xPhS4CyELN7aECgKCwIcRGD9PqqKEkH0Y8t+ZQVdHM0uNjY+vI9ZMkpTxexOuDeE1fIOaJZsIDC7VdRAy1o7FWBxsJ7Mv5Ic11udHeJcl2zdFiN4y3F6NnqYickd2aPEOtS0625sr27eQTVvLWWyg6xgcWrsCPcLVwtlsSEgzXDwfa0bD1+f2grVxPdo7EpVlM/ZWDhmaI/ZcbASq1gQFlizVMmSbZKVpwjZ1A6I1ebMC9QN+yKoc2SzCLMtdrZxTgtwIAoKAIDCMQNc82ZVSCFkXMhLei0BXRzOEopOQGNutyHVEatItT/eW7zJjI1bTIUPIApsGWnH9Wph1QT+c7j3Sw1GIxDHxWuGQP7RfsRce+QGrmIjaAaRrM3+mHotDHtbiPY3Bmknh/Y7UkJlyxuBsyAynsYmtV/TQbWf6Tu1WR46dhMzdFapSBzRkHdma4DHaPhO3XacHTusUa2PvZiLKryAgCAgCExHomie7shFC1oWMhPciMLWj9WZmtC8WWWsTDWuyNoSsJ74hIUPlOs9DhIwivHIcN0ukc7XcSbLREthyc2y81euMholSh/bOEqTbhuzuOsK10oy57NOQKfcU3jKil2l/eu0jzEsTvA0Ssv4lzgHzNqeYSYSMTi+Q3ZUOfnIjCAgCn4PA1HlSCNnn4P7rcpna0f4KgHi3YI6cjlH6KwQWIQUBQUAQEAR+KgJT50khZD+1JX+4XFM72g+vjognCAgCgoAgIAh8KgJT50khZJ8K/+/JbFJHo52H71U5jU1bFjjtT/A3PfIy2+glqRLFaY+Tn8kPaNZp9QCe1z321+bsyR9QhT8ognIH8t4u1yko76AdmeuUuJ0Fvu8Bb/h4h5aXT72otA/Ad6R/n7SSShD4dxGYNE8CEEL27/aFL63ZlI42xabHF3p02g7Dctdou+Mg65qwdfmsAl7FGcfdWh87tMbueEYROJG6384qRdppDNUhm47v1kOjVN5x2W+wpB2Pyw32l8bGbDRuPuBfcF/etpjFKY71OaBfUIidZUdfsKO851rZBw7bAFLeU+K+R5bnLcNqPkeSJpjPF9idGx91wb4yWEjT99+XfrAAiSAI/DoEpsyTBI4Qsl/XRT6nwr0dzXiV164R2KXCLHHOUGx4SZuIKBcMOQs6mljcD3iLYihj/gLk3Z0N1ckQv95Z2Ew6BgWefCxXDvPY3f1JDkSLLEG82COn8yr5r8Iz32MRJ8gKE2Zy7P7tr0tbNjt+e5K845BEWB01CSvvOK4iKP9qdBpUhEhj3mDdLduXPnnlOB0veGioyuvGapN3luz1sTQl1xtLnEkp+K8TsscRSbQCH/VJ8JU5srcYW33kRLuv2BirXat8WPt2g1UUQV0fsF+pvt+f3s5LrgUBQaAPgd55MpBQCFkAFAkaRmB8R3vivFLHEY1bQFMHf7+x41ZNLAa3TFKaGHGsyJddjju5tEnP0HMQ0Yt3oDPI/b/qtkPsub3w49j3NsGyw9V1WzY7visnwHJFrlwkj3HDYadtl/W9Ia26fIY4jxPSSB+D9Y8TshB+dnuHnjcQ225E7Oum//Wnb3KSK0FAEOhHYPw8qfIRQtaPpzztQGBcR3visk0Qr/bYr2Ik24tzzFA46wJZHGOnva3aE004Pvj4m5jOpHwW2CcRZusT7trdlzu5NJOOyWvac5NK/wYmfs7P0rgpdxLGO33/kiURV9shrF13O9/t9fVBQqZwWO7POKxIsxRhttjh+ngiP6xAWsJotsDOtkGrHrhkayRJgmQ+x8Jx+aEm9t1JHUBe52eYMbu5UG5L7seFOhLKYGTUd+xaZMH5z+cJ1lmjUVOHv2c4X3dY0HFSmdKe2q3B+CxOqn8F2qWJW+KyjhDvbMev1OcirC//h+ftiM2S2ivBbJZgfbzXO265jL5D05tC2kuWffWr7jhtVN3pFIF1dgUrY7vCP0FDZjTJWRRprXLzbnA9a62yVSm5FAQEgUkIjJsnmyyFkDVYyNUEBPo6GvnROu5WmM/mWB1yfVA2ecTXYbsjLoYxeWXyZPCWwawEMinRS29MRJz4L1w3M0TxCiezHlY9cN6miBcnkMmSO7k0k47Jhp8bcpCmTEZsUjRVQ+aWZ0oZ89uWrUXInEnygdPCXrIscFhEWGiHpjZuhvM0UqiyGDduB7X8GUUJtnxeZok8e0MU71FwohLXTdyc+Ql6niBKjFNcRciiZIsLt8MDx0WEeK9SK0LV+JFj2Zy6qPKTLNd+1kirGiPeXNW99jsXrxTRbvn21c5763ZjQma8/rd9q5WXtXvCAi93r3EpgcftUpN5POgYqzdoZW2bZDWAtq64H9Tkrb9+98MbovTI/RV44X5Xxold4VRYY0OWIkmm2JDZWjH7WvUJPmLKWeZvVU0CBAFBYCQCffNkKAshZCFUJGwQgd6OVj5QPF61ZsHNrMLrUTQHdtsPn2es4hgbbQtDj2xSYkc119XzoQmfCXF/3R1nbXs11mD1GvWXuO26bch2N9fz/kcI2XVradKMDZxmU8F8SWvlGPU3ht39uGlCZi0Ft0iSpdUiYpJaxIQRLi98JNaO13LVxF4TItNuhnTZefnP6JhvPudTEaK69ZgkpWB+yen1dR3BXCgbv4YcjrAhK6/Y1PaGpGx8Q2TIn862Kl94FOrYL1Mvl2SZ8sO/dtyh+imCuMLJ+0jpCg+X2IS6fb4JV1c2CbOvmw+CYF/zs5F7QUAQGESgd54MpBZCFgBFgoYR6OxoLWPrAMnQZMPR3DzPWM9ipPvCIXL9xMLI2UG0qJxJX/vdbi/et8uSlgONpkbjUJM/I/u436mTZPnIkZtDLFtFTCRkHqFS2dkkzL7WT2lTwUhCZpOXWlTWcunltGD5FLNEvk8Rz9a4mOVRCu5dsqQIyk4xZkJ6x+Etqj8CXnQkFi/JZjhdPoeQDdaPROZyY8RzOqKr2b7bFU6atCCBH+zzNgmzr4WQ1X1PLgSBT0Kgc57syF8IWQcwEtyPwJSO1k+qXihOWyTRjM8P9G3n+9P2y0hPu4jMKze2QpookW3UcoOjNRkO5z4QY5AYtNN3+RsL1uPVfdxQ/2HhEwnZj9CQNUuehFr5uGG/iBGndOi7h+MI3Kt8hzjOULAmTm+O4HNE35DVp8W7RDNIrLyiza0dd0hDZtLQ7/O8Qhwtmh2U+mFXuJ3WXAf7inkIVacZ9fdEHdyurpdY0K7d86vznamzkAtBQBAYhcCUeZIyFEI2ClaJ5CMwpaP1kqqqwGFzaJ0JacrrTWsiQREMtn8xS332r6eV4sn4bQeyjXf+yhu7DzAbCgZ9itVldJzjOIIYOOX3EEg/3tB9P24TCRleuJBN1+qsN2UYG7K9tvVziQvJxuV3aMgep9SymSKFldqM0bIhW13UcnRAQ/a67LA7N37XHDzG4F7l2MVvWK3SxsBfL5Maf2llccAiajZa2CTLKS9w48QdqN/zdm76/+NYLw93havipvX5gIhekMqvRcjKG7azqLHn81LJrSAgCHQjMGWepFyEkHVjKU96EJjS0frJQU8hZmK3bJ3CsZvJJPzcDVWELINn/gV4hMxNNXTXs4RUEzezfNs2NDe592s3TKzh337MpxIyIk3NLss0SYK7LI2tFUnXR8jwumK3mCGeJ83B3vUuRDJSD+2ydDVkvQiMIWTsOeQNUdTs6OUl0GzJu0jTxQLr7IBd2hAysqVbzmIkx3tv8fzQj9tTP6WtVRor28lrV7gqfFqfHxa4yc/tg2ozzlu9QWE4J4khCAgCCoEp8ySlEEImPeddCEzpaB85xqefWBjRh8hQW4P1R5YsjXgTfoe0co7dXU++43DryeBvfjSSkP3NVey1IeMPgHaf769vhWeRo3hWrSVLWi6dbfWO1/5M5KkgIAhYCEyZJymZEDILPLkcj8DUjjY+Z4kpCPx0BPo/AMaS5p9eS5KvKvZIl0fcfePOv0F4kVEQ+GYEps6TQsi+ucH+1uKndrS/tZ4ityAgCAgCgoAg8B4Eps6Tn0LIaIv9+bjHng9c9j6lqieKPEeeP7TTx/dU6yNpzLmGASPuj2T7y9NO7WjvgasqS1RollLek0dZnLA/Fd/S9xzDbhL+cdIHQnfbkL2njn9XGqVdms9SZC0jPrcmj9MK5LW/+0B2N374rkLJ/Sj81AmtSpSlN345EX7ADbmVSWaYLZXj48+VqG1bODb/rt3BY9NPjTfNDKJ7DAnlEwrrlY9d/XzwnX5esd9f8dS7YAfNZnsFGn5Y+6rjndojl7fLAqf9CYXrejFQWOM+yLVHDER9T9A7sZo2Fzxx3e9hH1byHlGnzpMfJmSk0k6iCBH/m2F/uWBNu3LSg9qBxfYckeX1+z3VGpvmgVPql6V2gEXRChd/V93YbCVeC4HejtbjjkEdJWQPAIHlH17zUe2WFY2xcUsICugoyywbtQcEQ9CNgb3/a8vWnX/74PK2dFx27e1eOyDd3Rw/a+1UXxvSIolOcSVu2RLL7NYQ2FeOw5qOS0qQLHY4mxMR8EJ+3GCZpmwEn6wz3GxfYE6+bOUPegfP+h18npeIzC5KPy7fK/9gyvFsMMK4wMAOza6E/dh0pRob/sBpnbJrl3YK1cfdXcLWBK83QZB7inlGRz7lyKIY5iAEN78hUtVXln4WzDjwnpKtmn7R2u+ZK9X0uwqP8w6LJOFjrJJkid25y/FxWDYzBpC9nX8smZEnZGvZCuvwrVjnz30s06damJy7f5/XLVI+nmyJo1kLrvMw455Jr8erZIYomiExG4Tse28XuUlp//rtU9979pZBG1aTvxeX86/uOC5Jtpg36KghoMG7LkcLY/Kn49ns/m7f17iSG5hJWJkah8mULwvFJvc5p90aNJ4vN3ucC0MS/HYweU/77Z0nA1l9mJAV+5jJ2OqsR+OXImSzxXcQMk2+rEkQ+otDCFmg9T8Q1NvRQi+uKav1rHl5TRT1a16Irudu7PqOfWYtUXfH86pxUFpH6rhoyfYxQsZHBhnXD2bn4Vd/+nZUzQTzoNS5Y+6J8zJGvDTuLRT2b0wClI+sSB9r9bqsMXPcYLwhMmdJmsLsX48YldfNQLuY9rczece1V25fDv3Y9KUc8azKkc0izDSWbgrC3drN6T5UJwmktkZMkdVwV1JtFoUf0hTXWxaREXu3rCeKc2sTl9BkZyZf9RHmffgwoegmMeV1i/gtQ260MWXOLmm2+hQPu2xHKL7RfefS+OmjCd/Uy5aL/fXpo9lITjqerZV3TZZMSSr/WaLrNFAXk4p/+YSLBFlRKp9zb/r4sbqMjn5fP9e5+fdOIebGIqrkILs+Im6Lw16Pi96YF2pHk1vI4TKdcsHucMoCWULnwVKDNWN2OL/mucrbv9clvherDi1jS5b7AUmUINOdrHrYJ8V0tEMNxriL3nkykMWHCBl33Fo7Rp65L3hxA1taKv+ehKDzBncLzChtPMdid62Pv1F5ptgdMqyoE5Gn89WhUZO+CpzpkGM6ZDgilr1SXxmmHF+empCl2GxNuhkWu1tdZgAXCRpAoLejeS+5k1Xrmfcy1l+jymnloIbMyZxWBRfWwDPRU39LNhpbzlhZmh2vuP5bUq1by6Xsf6tzouzP6rOe8qDUSci8UpjcWkcWVTfsoghBrRVPEI0GzMuJ/GA4GjKWwzuuyE2jtN0fhssr1y3DvZuEjZv0g3dq8F91qPB5TLSIvfrIbM7YdAtX71M3Iesvi5brTsNrUlykTVxak50rVPtugFDYeZvEdph9bZ43v/5k6o0xTcTgVSvvlqxe/q3nwWw5kD9EzAcPv0+6Hes8vLxNVvVzHeDfm3gjf+v28sa3OjyUD8c1J4+QBld/GOQqMo9v/E43eIfza56rlP69Cn03ViMJGR+ZtnNXLJo6dLRDCJeesN55MpDuQ4SMXt7NQhEjWrLgyYcbrY+Q0cHBlCZGusmwY3VnhER7Y+SXQRO11S7DJlUaOLO8QY0UE4nbZNhvF4g57h4FrW/vVnjj+wW2+72eDBWwhrztso1SF0cJjAPIAC4SNIBAb0fzXnInq9az0MtIlmPmhQg9d3Ksbyqy0Ypj2BNbe0BQ+dZft2YJoP71lyyJkJlByPvK5zTW0lItSfiiLcsTN/q40F7T1/VB7PqEgXSHQ0bq9ASzWYL10bKFs/yCJXzUz7FxLkrF0/uwWbANlvLrdcWz0vlGWxxOG8MV70QAACAASURBVCzooyaeY3W8B5dRlYd57cWeq6RIUqoPMLdrqeJuUB9Dqj+66IB5WmZZLxdtQtbLttx258HT0cCp52+HO6rnDcfNkpcdktkMydraFcgTVw9RtCrB7WOT1V6MK9wJQ9126TrDlQBGV7hVUOvS9PXWA9yPC/XxqbUbajmH4nfVSeESbQ/c/vTRW3+0cvbdZbVL7w+xiUu7b/enVQT9szRkqs72Ehhpu47XsIZMSVbhcdljs6R3eo3sXNQf6Ha9OG6L/HgYtp531537cd3vldx8dm+dh5c3RixZ2mt8raL10u98zqYFi90ZZHVQt5c3HtfhXj4V2VZ6ccFnwlr9kOrAGj9Vr5aTYV5w2PJ7ai9RUls19834Ox0rLTTLpY9Ds01avGP0/jlCRtU3BKqeBLnReghZkTGJio39CHvMjhAlRzzs/MyaEx9nQgRurW3AKjR2t9pmzJArv2xuH9XBIzqOhAoAkNM5e+SBu+OLVMWS//sQ+GxC5g6mCxzvZmBqXu5ueUrcz1uksxTby1PZHRiC5b2EZgm7HhO7M/30J8X+DdHOfJGVuG5ixBvtjR5PnFYRFidthUFLrdECmTnKiQ9eN0sCOq2zZJigOWD7jkMSofF8/8L9rmwjFOmIsTgoclfedog77CtVXHvSVG3R1r5UyHextQTpe/YHaInTtiFjArc6ac//IagL7N8sbRx70beOFOLlDK1deNxwqQ/mfuC0iEBEjf94orMmjVBROsyt7wDGnlf/1/2uJvSu8J5yoTWP6iNhjtl8gZ1lTcxjrKMho3GPjngy63l25rqN4gUOrOkqceO20f1soCw7J/da5eu+p54NmVkSMzZHVga+QTW3v7MMa0XmS01s5/NRNmRdNmIq1/YY8rqsECd6SbS847SKkehzdBlvs4xJZKcmS0ZGNTbVH3UTliw573rwsTCt8zDjninL+23J4j33b6k/xhs9d75w2cT8bnBf5/ai83ab96MJ9z8+9yhoDLLitggay6bs3MwSMefn9F1bwHa72E/fi1XJY02EaH1pbGF5scMzX/nXliwJPAbNJjc+KfLuWSUYRVgc77z7qaSXYdkQrlZ+9ZJjY8RqPFjzkicvUepnXlmqcQ0ha4z622XY3UCuxyDQT8iaL1P6+vHtNNK0+QrqLqvEI8/xKJsdO6G4z8uatUDr7IJ6TrYitgeEgQHPSot6+dQfnNr3vR+p9J4caQm2sVdQg5m1HMjnGC4R6WU8ltvW1ugDsfn5wNmSSlvVDLJ2lVr56nemniOsyK242jakRch4oI6bpcyQfD4xIpugJMIsOaIw7y2/y2Tr85840gRlJkyWSS2PGDsgHnSN/Y2RuSrxehQ4rrsPNjdRQ79OfUN1YBKoSSJfx1idvOObusJDBVph5eNRa2gUSW7IJ49X3qRGzlpJu7lpnf+lJjm7jZx6sSFzd1mWSN5l/+TJZXgy2hn4z4c3ddip29e0sz9/GEJqERvzIca/5jQOX3b/nicyRPp9Y7ztF6JjHKjf+Qkkiee/nfkoUwoFNgGo87DHp8AGJCZulnG/qe/2iv/ijzilbDAfPz7u5r4wtrX87oXHihbqflwm980Y1pDsBl9THn0O2rZ7NC+4mjF3TCVsp2FlpKWP0Rib85mPRmvOpbW0giYq6bMt7Tppuf9+o35f22QGV2NY790bMqR2ZZrOQxo1pQEzzxvtlSFUaonxfkhYuzVbHnC555rMCSGz+tgfuewlZJ4ErQHOe863HYOeeWnNRNxOSsub9FficTthr3fMULr17ojz7YFXo1KlkRdHM4gFyWKKeqClbG2Vt5XODChpOn7JkgcYoxnmATiwFKq1CzyQOYRMf/zQpOeTG66/ek/qZQIvLUcxX4n2M/1+2vOPiauIXXvJcmm01xxRmSDEtj1YSD4vjDQU0dZMTKZE+1dNVs04oAdonvRLXNaWFuyV47CaY77YIDtdPoeQefIqyRqM+V6XSyYUK2u5GV3hdvV6r2kXZaPB5/fHITsF9nHaYXKhJsM+QuYW7ZblPrPvmknWDqXrqqrYQD1yZHRj2RMzLesW+wSzUKdzktnkxHnQNrx3H3t37Y86Wq6Kd2Y3sTqb1fThUeOVXQL3FVuTbD/0rh9HJNEKp2eFMs9Ye8XL/HUe3XX2chp326cho/bySZbJlWy1jzuszZi3pHfrhpoDc7wS120M2vRTVkrDn7BmuukrbrubzEf+TsZKn7G7UBtglHmT0RSHCVm3JJ/TDlPmSZLlQzZklEGLQHkETGkCmiVMNdFEiIO7jQL58WG7lH6La0kDEZE4w+jVjiGyR+Od2qZse8KpNWyiIevufNOfTOlokwc4R5zm5XaCnRs1wMe0+YPteNTD8nHBLnFtypxkpv8OTgx+Kn1fD6Idz71gZ3B6nrGMugyz9eDh9GMz0ebszyz101raG0Wk1uANTyEZ7HwdQua5vWBzgeYLWC2t+Wc/0lJppg8Z14WFtEsewWEsMm0N7MmobgPtzvKscL5fsYmMXE+1RGmNJw6B8coNFqUD1dikJ9ZQHSyMnXz0crJZbq6fOeF9bi/qFOpCLyuybZHpow7Zocmi2Unspjb9pKiDnXrVofrCK8t/3NyrfFtLlvThs89Ra1yaBM6V0/edJ3033ZNiaEwpjq6WpfloUuG0g7L+0zaOCe2UJPuq9bHeOBbKm9Jx/s7Xms5t4jjwvGzYZiqaLXAwy851Hn6drZ2Shhx5v0696gqai2k2ZJzqdcEqTrA93/EybvmqJ/L9AnGyd9/1qsBhodxezDfu7uz649Duuz0f3qq93A/c8Vi9cN34WnXtNmOmlD1DfZD8XpblC4+CtK8X/ih679Rg0J8yT1Karydk2sAuihJsyC6iUttjld+SE263M/abJXbaSSS/DGyYn2KTZVgnRMBipIeCXITy4EvaNdpEkG1SZdRvCJne9aE2DOywOVIa1cFttxemDPvr2wAov+MQmNLRugY4pyQekAKqeD349A86/iDW5Dz0Eg7K1qchq+0+mvL6rlRZhoT4Nkqkvnqw4T3loSZRYzdJr03GW7T3BY2Qvo2W/jI0g2VVYO/YkJUoLmpXscrX+pp3CNl4txes5cgSRPGqdjHS1F3ZsPGXMweWKA6eUT9pyOyBukmsr3SbGrg4VL3/q9XKcrGhljJTs0OnpAniE5YshzB+3nA29n144JhqjV1XuOf2gpbs4liRqudlh925Wfqk5cg4bjZIcL9xsFIfpmayqPIMs2gGtTzTT8iGymo1w8gA/z1rLU+RLaexMXMIRZ/5guoDta2WlY7NIAwAgzIGyH1Pmq4xoSvcz+pxpHkpgTFj9J8H7zsJWTC2DpxWLzunur34/TcKDhWDn5mdoHYi9n8XYRj2Rq66HCefjpsag47nJriO5437FTkS7/5zZGmN60tssj32+z2O59s/TMiIEh2Ui4vZRru3eOU40g4w0nbRS7rc4KgHN+70RLiWK7UbUrvFMD4nq/tRu8Og3VQHnPdLJmzGl2FVHNTuMdoZx96sVaMJIevuqO950knIBr+Amq9Y52OzfsneI81IDdkE2WoCGBiw3iMhpWGffY5WSO+yJMNl2q233OCkHUXy4BEtsFynSJM55skame151doBSGkXG2+XJS2brVNrl6WysVP5dhGyQM04n7Zj2MdRmQ5ExvjZTJamUZ83ZGu1O5R2sdFu0UazTUb+RMjMhoZAudCkwyFkgDHYtbVRtPSzJIzSBRbrDIdd2pC98oYsjVGPPaGiTNjjhOUsRnLUGwL6MOYxbKl2yNIuV717jZYraccn75y1w00Z+re8ZVgutY2TTsMaAtbWZLjUDnitpeo6D8KmsamlEyDW6VpvWuonZEa+rrLqIiZeOJPdxLTd0b0J14o4lhypJA1BsLLovOzKuyvcz6gizfyi8XvmPw/e1+Nfd53b6abVy05ft1dofGNjd3InZWz0VMrXddcssdqZta4buepyWnECATUGgWd2UB1vCla/ZMnSxukzrg0hE+3VZ6D5dXl0ErL3FjlElgI7t9yiOmzIas/LbuzRd60vqYZQmiURw0GG8qSPiSW5ZRhhd9YiTkOZ/zXP1RIMucPY9O5yrnA/Ltndh/EG/9dU8SsFpfeEjk4yjre/sqwJebNGbPAdnZAhR3XtPc37Vv+OffEsR6VjJOg6Oql/SdTS9PEq0HvdKoW9zIflbtvGheP1hHYch2Q2zs3narwj1zpk9G5/KPTkWj/yd9fWD0IX9ZFIoYehsClY6U0Fo/voNLIXko7Cps6TH16y7BLkveFCyN6L3J9NN7Wj/Vnp/v7S/l1C9n1t01pGM1o9+h09uX+f/FLy34DAE5f1AlvLbcnfILXI6CNgdvn74dPup86TQsim4SuxNQJTO5oANw0BIWTT8JLYgoAgIAj8NASmzpM/jpD9NEBFnjACUztaOBc3lHe59Fll1tEr9mE3Kmqd5rdeEFYVOdxBkRf1poEhNLqWbYbSqee6zFGRpy07UJbTZPucL91RVakjNXUaa3dUJ5WLvw4BGrfGjUVNvxhbyWl9nQ7Ltv2zjS1F4n0VAlPnSSFkX9US/3i+UzvaMBxqzb7b35iVAxt1ujuDrKefezlUFj93d4fWGwLMVnl7acy7tuO2BB+yq7Nt0QK2brwKZwxgQ8a72midbHKW3qaADxEJU6auUOdSIdtzhGw1lFFwy8WCtv9oyVbRdvWuKTGUfwvpOqCVd/2k4+KlNg1E0QzL+giqpkwnP8f4vp3fKz9gnSSY0yYNs0lARysfN5yzFW+EGn5HtKsDnZd9JJdfKpdJfVJvHDnUO0fpyOEzdgu9oWO5g70KVz0LXI76+C1v2x0ZtWe0GSVNMJ8vsDlZR3PRh8HliA27SrA2l/iCdTkhruOF+ojrMqGOSvuSr+rIntr+zHsP379k3bQ1lddZzmf1dfR/jDr9zQYgcD3J4D6QXoKGEZg6TwohG8ZUYgQQmNrRAll4QWpgG55s2DGTs2PPy+hzb4cIWU6HZvdNLF3iNDuRumK0j2yxYnqkp3WMiYlq4rUImXKdYY5XYlcLxm0GQxzBdi5qsuNfiyg2Z9DRBKwNm02ZTiK6cScv9TgU1o+NP+n0Tyx+/uq+5UpB25D5ebeq4AQo1yXs3oN9jhlD7qZMOz/XPYWTEW0Bwyqi45CIWCpnu28Zue1Rf6z5yI9Yk8PYs+VPy8uGbslFyhv5aqSt6eRu460rzQOn5QonvZtOHW+l3W3oI+1W2gmwcsWxA4vHHs4L5PkFWer1E/JLlVgnUjxPfBbsrkmIIs9xydKB90b1gc4+SBgtu+oVAKUnyG6jdjRrY4HnukN9TDVt7aYNhYfCpvX1zvdcF+7WReXd9WHT/964tZG79yEwdZ4UQvY+nH99qqkdbRgwNVgNTTaczxBJGi5sfIyBsnhQM6dSjM+VfYn1n79niGcH2fNJj0e46h1hxlea9xx85mLbC7/jjNTTfLSr1zGZ+LKZhCxDc16nClbtrgiS2anWka/Ox510hraz+5Ogf2+E09LQySOD9dZp2MeicVBb4bajg7yVZoiOdaNsfFnd0po7PsXBOtuxfUoCxR3zjig5mjNT9QkHVt5Nqd6Vbh+Su12+8vdm+odKqdqpH69wHH5vej9kwukaiTUWvTt1m9h9V2PbKExgOvrTF/X19xCyrjE1XJ8+pOTZVASmzpNCyKYiLPEZga6ORkeSRAv70Gg1sPJhz3QcR7bmM8yS+Qzzhb0MMmay0eC3SFKFx4WcCCdIaOll0fi1oxTPm3mmfH0d+dDl7nCnibmsJbJjVvu/W+yu9aHYQ4MaD/a+ry5ruaR/ybJH++aTHp9wUSWqihiB0kR4z5UfsHN9fiJFJ1lj7dBv3CSl2rY14JsyHSABOvYsThK8xTYRDE1ojXdy5xzUjiXL/jbw8/fvXSHH1VunYVKbwbhK4/MZ+Qippoxx+bVJFPkWS+vTCIyMKt8W3uYx/6ojp9LTow5tk6v6kXvBZSpTAMbUIUxaRoesDpEmyl7JUx/2rkts5++KQs6P6YMl2h5w2ig/lvGcfGMZnWGDsZ8ydM/lBR3TKrcOY5Ysw/0sLMdX9XUhZKHW/blhXfNkl8RCyLqQkfBeBDo7Gk9SzcHI4ONm9BFBZYFL/tTLMDTAx/Vh2uO+/rVIPOE3NmTqzLLGYzw5Ck2Mp2x9UsT2pgby8nFXhu1d4X6tuawIyfYC9tX5OGJRnwyhtQ96oE/Ix9j6iHrO0CSnX4PgF2jddxAbjuE/Y8LlnY25OOJu4vmEzBwu7BTXaIfGEQl1DqLSxjQkivxk+cu4z+uWz/A7P0sU5OE/2WqfRuEJzYjFctCZg3ysyRP3gpzpNnJSvPBEWefgHYEyojxDoIdcYRhsdVGNHE0Z43AMEBtuL98rusq3n5AF4rCczftikHF+yztOqxixdtbbJnEVblvrBAROHJDbyZS+hk78vjg8zrSZQ/j8hDrv2JxFWKrxwjgT1kc+Kc3qHLP5AjvbyM3Lrmkb78GE25zPbVYfE42tWAKjDTVZfWVf/3RCZkiq/tgxdZDfz0Ggc57syF4IWQcwEtyPQHdHU0sbZtJgz+pvB2jf5zrTCuXz7tmRBCaSLhGcCSb0Ba4On1ZEQcmT7G7eDsOucK9Qpyx65sv5wqM+P/OB4yJCvMtr259xE7JXprm1bLXIGNm116Iv+24jZpMFygfy/IHSdwBZ7BHH9nKomgCNQ+YxctOkHc9mmEXeuZkOUaGTOtRRMrubsX16IT8sMeNDgBWezYRNE6/WWrAmsTnSZH+6IP8ThKwRpoYxeMEapc/QkAWIzUhCxu1ER83xP2pPv3/yV0GvzSUvbxOJJg2U1h6jvGIbR1gd6Ugn2iSwZuLhHncVkNsBqkKeKY15o69TEZggjSFkVlv4acrHo9bwltQXI+tD0JFjiLR7kUO3RADjGWazCGvnkFi7/359X/90QuYcyRWquIR9BIHueTKcqxCyMC4SOoBAX0djexh+0RUxqpcr9IG+8/kKu+MZZ8ewNzCRdMngkKRwOp6ozGBT3nHe0bLHzN291hVul+uURQ/C5ZkkvGxl2ZTVtly8k400R+6OzDFLJSpvTZgGDLqd8qyl0TSdIyZDb8OJ6NxXOnhdG23zWZn++YnWZGjqV/+yN/KYz6elSTdemYOFDQGwyN7rgYd7CkudDRmwX/d7ZwefetjtruJVXHCxTmAIaj9o5+WTzga1J0zK2b83olSgjZpjiKhJAT47N9bn8CoyrjQ2jdZkXH5dS5b+AeL9fU/J1bVk2RDHRn7vig3wE6jzUmmX5YXPGU7TJTbHG8470kyaBVpK20/IaBPDm9FUe0X55Mp7HMy7P43S1poPCj+/RqNFZJ/eBU+b3KshqlirG2+ueFKdnPNbvf70xX09TMho5+WTPwzd/tY1Zii3NLRRwyXYPmpy/1EE+ubJUN5CyEKoSNggAr0djZcpVjjfr9jUdjAVclqitCZud4AdM9losRySNKQhs6pS5sgSV4PFT7vC6aFTFgfwEpjRAFq58yWT0ZZGUMfivCyi4ifuve8aXHsTNQ+9JUt+8Lxit6AzIMk9gW2fM0BMvOUt0GHmdF7k+gTesNdRz/J+xm6ZIkm0Boxs/lLvjM5aYm+iq8PbsnE/MksvNQklzdoBt6efj7pvdlmS3SHJs8Tm8pxIyAD7bN3GtrAp05kge9xePE8LRJbhPS8Zxs2uRlV9lW9X31Nx9Hu2u9VaWu6Tjl2nBaZz2UewlEbZ1Q71xOddpw3hd4qZsmRpfRS444WXo17CdDcdeHHMbehdMM9avyXuJzrsfQW1f4DOzU0Rz9Z6d2rT1nbSr+rripB5ZJLd1mQ43Pz+q9rH3mVp3r31ocBdCJndZF9y3TtPBkoUQhYARYKGEejvaE+c6HDd1coy8CcXAWSkayaKJy6bN8vWaMxko+XySBIZqNOAqZU9MDZk/KVf3XG50LIL/Wk7GDK87grn7fQx4qXW+HhlGQ0LT4q0xX9zQO26SRO75OAu0NZodhCV+nnvxThCxgTA2EDV5MQsAZpdjL0F8UOHSHjR6cs63V7qjQ38mLWfGa4EdKiemqSfyBWD/UeYvcWo3SLUz1R/aIiTqUMKNvS3Juo6SfAiPGEGo7Lorn1ar6uKrkwsLZyNY29eTBK63V6oosa9I31uL0iDG8da8/a8YLfTtpFUgCZRitRUeN4f+r2hRyvEllsUJU8HIeN8IiSW2w4fqhC5cvFp522neV522J3Ne63lszS8fnnO/RRCRnVJt7g4/Vb5ectUZ/dsFI3d3Ap/vq+3P1acens3jKdZRaBn5CJlFmGWNSYXXhK5nYhA/zzZzkwIWRsTCRmBwFBHY9uxyHVxQI4mN+kc82SBxWqDY7a2CFmJW0Zfnhtc62W1DkFK5YxztrlqGxJrl2WaILF3WZZkq7ZGStqY+RzJWhOornCQHEsss5uajPoIGUrcaXcnEx/tBPOYa5ks/0UtYtSQC3ZUGVgucZdYvPhOfi7JsglAB3qjgj+UzycSshDvmibbxwgZ7XZcp2tYmxZH4NeUOUXWbsewpi+ppVCj8ejec9DtGLa8ZVguM9z4C+WF/LjBkjWV2jD+/FCaNctpML07qXlvuPbNBg6ya4z0BwDvGNa2Z5G/JKh91NX9mnx6meV70/8drPsJGV45jpsl79hO+b3uOvi6kbXTKWz9Po2wyWy1ftPW9aN3fnx8vK9/kJDRjth1ivW0zl5XWy7aCAzNk34KIWQ+InI/CoGpHW1UphLpwwh025ApUtfrZsMqfeqRLVbSsIYMwNRlnGM9UQYIaTcbcUQxGs3QZOdF5FvGb3TeoRzcsA/h6GYldz8SgQAh+7a+PpGQ0ekFhgz/SGz/fqGmzpNCyP7+Nv+WGkztaFOErL+iQxPyJ06WU2SSuFMQmHKW5ZR8Ja4g8PMQGH+W5c+TXST6WgSmzpNCyL62Pf7Z3Kd2tH8WCKmYICAICAKCgCAQQGDqPCmELACiBA0jMLWjDecoMQQBQUAQEAQEgX8HganzpBCyf6ft/2hNpna0PyqcFCYICAKCgCAgCHwzAlPnSSFk39xgf2vxUzva31pPkVsQEAQEAUFAEHgPAlPnSSFk70FZ0mBqRxPIBAFBQBAQBASB34TA1HlSCNlv6h2fWNepHe0Ti5asBAFBQBAQBASBH4/A1HlSCNmPb9KfKeDUjvYzayFSCQKCgCAgCAgCX4PA1HlSCNnXtMM/n+vUjvbPAyIVFAQEAUFAEBAELASmzpNCyCzw5HI8AlM72vicJaYgIAgIAoKAIPD3IzB1nhRC9ve3+bfUYGpH+xYhpVBBQBAQBAQBQeCbEJg6Twoh+6aG+tuLndrR/vb6ivyCgCAgCAgCgsAUBKbOk0LIpqArcWsEpna0OqFcCAKCgCAgCAgCvwCBqfOkELJf0Cm+oopTO9pXyCB5CgKCgCAgCAgCPxWBqfOkELKf2pI/XK6pHe2HV0fEEwQEAUFAEBAEPhWBqfOkELJPhf/3ZDa1o/0eZKSmgoAgIAgIAoIAJp9oI4RMes27EBBC9i7YJJEgIAgIAoLAL0Fg6jwphOyXdIzPrubUjvbZ5Ut+goAgIAgIAoLAT0Zg6jwphOwnt+YPlm1qR/vBVRHRBAFBQBAQBASBT0dg6jwphOzTm+B3ZDi1o/0OVKSWgoAgIAgIAoKAQmDqPDmakFHG8k8wkD4gfUD6gPQB6QPSB6QPjOsDU8jpaEI2JVOJ++8jQC+j/AkCgoAgIAgIAoJAGIGp86QQsjCOEjqAwNSONpCdPBYEBAFBQBAQBP4pBKbOk0LI/qnm/3OVmdrR/pxkUpIgIAgIAoKAIPD9CEydJ4WQfX+b/ZUSTO1of2UlRWhBQBAQBAQBQeCdCEydJ4WQvRPo355sakf77XhJ/QUBQUAQEAR+FwJT50khZL+rf3xabad2tE8rWDISBAQBQUAQEAT+AgSmzpNCyP6CRv2JIk7taO+pQ1WWqFDhWeQontV7spiWpixw2u+xPxUoB1MWOKYp0vSIwo9bZIiiCFFmPQmF+elC99UTRZ4jL+64nfbY708ohoUL5WSFKUzzvIANK+Fd9v4b2QbFESlhc7Tqb5U+dKnkGFnWUGZ9z59X7Km9r8++WJ/4rGJ8R9eM5buiLV2BLIpgdy+8rtimW1xfY8Ut8chzPD7cl8aWJ/EEgd+HwNR5UgjZ7+sjn1Lj3o7GkwORla5/9sTxwnXrxeOJ3Ew6L5xXEVZnd6Z55Sfs1l66NMV6d0LuRrXqqyahnAiO/694onqdsSIitTqDszAkisK8f1mh5IuiTBMyc9+OS2mzLEDSLMkANVm3CNHDyJQhW1HeK3hQOLk0Nx35MRtQmLp59cvP9Te4cCEVHucdFjOSaYbF7oyHYRoGN4cxNJJ11rWkDIwcBldKF5LXzq/7+nVetdqO6rK5lkCXnNUDl/0GS7//LjfYXx4w1ewuteMJ9y+r/Qxx9csxJJ/ls3Ew+SqMHHj9vClqRQS7S9pAHib7wG+ReR8YgTgSJAgIAi4CvfOkG5XvhJAFQJGgYQR6O1pocjBZtp6FCZeZmLMi8PxxRBKtcGqrDvA8rRAlRzxMec6vmewDpInIBstmEbKaJN2wY0K2w01rkKoAcag1TK8nHo8HHs8XXiZ+rgnZ+ghfM8UimrI94rfKMk0SJxIyQza8/JTWboDgPE5YmHSLUxDL8rpBHEWI0w12m1Rdb65Ks2jKdhiD1RDmuSmj/iXyYdrIJiID8lpZty6ZlBjN3wMnJrVvONyJ+4XapMR1G+Mty9ta0jJH9hZjS2Qu9Gd/iMxjRPG8+Sihj4xW3zeZBPo4PWL5DA5GI9v+CElJM1YQcbfIHtFYIqMOiTblceaelk3hPku8/LWWUwiZjZ1cCwLjEOidJwNZCCELgCJBwwj0drTOiYdnCW/i8CajWmuQYMbLMt5zEu11wSpOL21BqgAAIABJREFUsLs9YSsAqvKJ2y5BvLooDVdPNXiCaS37GG2U1pCZ9I8TUiYNKU410zPEQZE7pcF7sraPiEqtUZstcbxXzeTPz9yJk4sxhIxIjCYKnKcJX00kZEZ2/UsEimR6YybSRXBKPG4HLFnrlWC7XSmitdjj9nhZmqEHTinVcY0Lc5MSlzXda3wM0RkiZMHnBldDRKgCRt4lTndDrrzfLkWQhUNVZEgIf0MyjZxOm1Qo9tSHTri/7EwrvO4n7nf7wg63CrAug2So672ocuziCNHuxhgXR02KkhkaDazK/HnZYZkkSNIU83mC9VEvr3PeMeasbVPL6EEZahkVzk0T+Pd1RL4QQubiIXeCwBgEeufJQAZCyAKgSNAwAr0drWvioWxbzwKEiy3HzAQRek753HE57Z1ly/Vuj9PlPkjGaMnstlOkaXergPKhljAvmSJejlahQp691QQrXp21TY8hDmscc23jVhyYREZbNbHicVSapvUFLzP5NzOgCzLjopeFvoCQFVnMdeD61gSnIYZVnjH5ItIWp1ucef2RliW3SIksaNJyobXc8ooN3S8NFsDzvOQ4vUuBpsa9WBhcQ4TMyBH47cKVyyxxr+uRoCZUnXLQ0vYZx8xatlxukB3PyEcaXQXJUKvvK0CepMkizVScILPJHstn4fAkzeUK51ozXGD/FmOXV4H36n0asi4YhZCZziu/gsB4BHrnyUA2QsgCoEjQMAK9Ha1j4uFcW88U4Yrn9lLJAsd7mJC9rttmGahle2PnYdup+fW54/CmJvWYZiAzMTPpsJcsS9xPa0WyFgccdorUxIsM5+KslzGtCRMjNGRdM94QITOyectSfs2C99VNaWDiLdRqm9E4NYSMtFD3PMfdtvKvM6vwehTIyc6OwoysFnFlAhJpWz+DZ1ddzfO6ToZgkTwhQlYLMvGCNKZrJKzxixAZbaXJxchRy9m3LGj3reENC0yAY7tvGNxszIEyJ62dIolEzOJ4gb0xgmT5rDy+m5DNkg9t1jCwy68g8FsQ6J0nAyAIIQuAIkHDCPR2NNuWJk3VhGgGcyZRfWTJlG12gZUoTnucPr610GQMWrp6M2Qg3miSYiZMQ8gq3A+p0gzN1riwVsIQrjdkxY1tcPwlJVWIMai3lraMFq5Lw2JIjkUQnSXL5Q67JREXd0KvK9V5UaHIEq5HwsuVFNEnZIYEGWLU88skLFd1fzuATLHo735QWsQsN7ZPfUbgGp/7CUtqh+UJd2Nr17IhmyqbFkj/kFzxfIXsXLQ1p2Z5/J27Qd2SrDtegowRx7HaPGAeeR8jr+sGsyjGqlF54XnZIp3pPukTMgDdS5btXZZBLZ2RReNcc9HWfR2RL0RD5uIhd4LAGAR658lABkLIAqBI0DACUzraqMHcTI4Brdc89nZZeoSvtZtze21PvqZKVYEsIcKxxvm8UzZSZhnSkCKj+aloWfTWuAbQBuKv5xOv6oEruUzYu24JjKaotiEzxM/8NjOgkUj9mrKzHNVtxwSKJurqcXqfUT/n+sIt0wb36bEmT21CRhvyXJus+0ktQS5Pd88VBpFMZfgeRW/I8pLUPMhI42g0cC3Nk1vV+s7U2eDNDwwBazRDvmytnai17BYBrguxXXzoXbas6VNEP+RGpLbhCvRF6mv9/E0RYLJjfNx2iGNridEjZKieuHcRdJI/6PaCyGy4noSL/SRIyKgPPx940nvg2FD69wbAiu00R73DJon8CgKCACMwZZ6kBELIpOO8C4EpHe1jg3mHDVmH1MFJyMR93ZClatlRaYuIWJDhNNlN7XDjnWpGQ2YSAWVxxJoNrF3N0SxZ4+hr7pxdfRbJ0STL8U3WFNEsAxriZn7jWNl2hYz6uSx7Cm4yLO9nbHVd48UBtK+g+fM1ZM0Tc2WIpe9uxDzH84xVbVtGuFianl5CZrkeMTZ76Ub5A9tn2JwOAc2jcY0yRrNaS6gvTF1pQ4DeLcsE0IRP0Th2kRZTZoXHiZYdDQmjDQIp4hn1k1fQzotTlndcsnVrKX6dXXD3N3QGtGach0/2WOG7cnd6MsFcYpMdcHv6dVH3zS7LBAnvuFxic3niY++wwUd+BYHfhcCUeZKQEUL2u/rHp9V2SkcbNZjzRDPj3WMtjVeaYmt7vOTJx+wo82x7aNLp0pCxdizG4mA7fn0hP6yxPj8aUmRrbMwOuLcMN2vXXUXkjrVCO5BNtfkzRGayhsw4gM0LPB537SetwKPQy3o+ITNyRTNktgBaELZHimZYHvKAtjBERtyw6lmwDL0OectH7az2Zmt6eglZY79nMKpJwHqH/WmvNgzU/t2oQq5sButxvyatRaapfY1dXaSN4q3MuL86S+xuH+vSkFXFAclsjVPtkE1l+rxl2NX9yyeAT5wWEVYn37+ZIne0I7S24afsOt+TOeJJy9k+IbMACFz67zBtApl19L1AcgkSBH4lAlPmSQJICNmv7CYfr3RnR+tZevSJljOxdX35h0QNaANC0YJhlcWe/Aicr6chIxLHxGuFQ/7QfsVeeOQHpSGy7KgoO0PIFodc+SIjf2T2v6ev8vCF8O5rmXy3F3ccjQZsH/KIX6G7qoak2OTAhH3MtQRL30vIvPq1bhVRcG3zRsgWXMajzO20WkOW7rBbkaaU7LzI2H/N7jSMKD75MOEf/g322/cQsmY5t5YpmHf9NHDxMUKGxwnrlMhnIGsJEgQEAUagc57swEcIWQcwEtyPwNSO1p8bffnr43Y67HYcrdeQDdmkI2QsyWry4/khe+U4bpZIydmnXkqkXaHLzbF1KoAhZCZe+zcwmVoitC7r45wuOPie+qsbtmZnYythX4AhKSFCZmmSzLKp+e2yf/OL+jJC1iObrdV05DF1tdLSMnBMWtcnSiIWswjJvqjtr4ZsyN57JFTb5YsW1CxZJnoXI22ESVKElyz73pMpS7oTCRn5RnO+oByQ5UYQEAQCCEydJ4WQBUCUoGEEpna04Rx/QAyzbGjcO/wAkWwRjHE76/heOfaLGFGSwXZdZcfvu3by6ov4rmd6F2WPMrI7W2Nj9mh7yu9ONPmJqz2kE1P/xJ/C5c+U9SfqI2UIAoJAHwJT50khZH1oyrNOBKZ2tM6M5MH7EHiesQsZfb8vN0klCAgCgoAg8MkITJ0nhZB9cgP8luymdjRylWD8ifGyXucS03ci2Mj4dVKIlmQ8tk92LXJ1rNqt1D27TK1YX3NJZb9X1TVR7ud1j30nCAPV4yXvE/zNwAOppj82dWItc4Ggf+ERuZLmdhysPX0jVOdQ2Ah5hqMYf4luTF72/rIlXrXr2NnoZBWvzCYmmkZY6c2l2dgz6nQKs7qQf6Fmm00hbDMLI+nn/5bFCfuTvfnrfWVMnSeFkL0P51+fqrejVQ+c1uROIuYdj2o+VbY85EahRcj6bMLIbmuAvHF+9kHOnh2aPy4+b3tslinS5QZH4xWdW7SR8csa2BtUHkdyi7DFrcPW/yODa3nbYhanOP5ww+vyccNpp9w+LDd7nMlFBP/12zl9BJuPtu9HDP/H939lEzZUVqfNG3X8DmP/6nHBnuwivXeF8L94u0THYFXXqaO8MXnAc07beSoH76Lu6RshGUJh44QaiBWWo7PNeGdypsaflOxQM5xuPokxbl7c3b2NHW3/OMVtEe8R2uozUBnnsXq/IsTBTUNO1GaH+ieU6+Xc3Oqxk49v49CekzUC58A2GamrV37AWp8Lyzab1o70uj/7iSbe986TgbyEkAVAkaBhBPo62v2QMIl6luSENcaWz+tpBpFWZ/8MQjZA2kyN6JDtOMlA/kyVLy3rbEO9I6/T95bJ5CO/HiGjifF4CrmmUIWoQXHc1y57y7cN7185TscL3jG/fqSGnWn5gHO/ne4HPjqIHcwCqB7k38x4uA9PdqaAKdiYNON/25MiH8HERxGQ94m+kwhoj4o3mdKHhT77s9X/LaFC+YbCrCTNZYh0hMLKK7axdurbpOYrOsrpzTj49Z7RLcvCLkG22NAmE3192K/Uh1OovDofawIlPKyPKKXt6WrvULgfZuWdkgsQ2y3OEUVIrt6NRCPdiOhzXfkM17qeHf2Dffcl2J7vMB50qtcd5y0dZt+cC2t2BtvjEPcZC6+Ws2yrbHaN4r9n9vOR1+r9+kGEjNvQHgvtNvfetyFC9jgiiVY41dr3J06rCIn+eu17R0fCx9H65slQPkLIQqhI2CAC3R1N+ZlSh1gDj1OqB+0UZhBpdfbQYDkoQRPBH6z8r/5GQ/bAKY2wvjTqKCYxO30Y+DcQsqYW4Ss1KNqDUDgehY6euLuz+NInrXY3Ry7V+Kviuc9srrTI7XmTd8Wbgo2b8j136kD6N3381FSsmzpp1yjBCVN9tPi+x7gs7Reta5mKalTl6uQJ8+5xLfndMuTkqLQmVYF9EmN1aogBx61euJNj22TfuVHErrd9XbftyHe5ju80RUd7c54RFs3sSb2d+4byYTdid+lIuWpxRsbng+FpF/LiBFsRbWOj8ixx3VAd7FimtAf7omtIXfPxamK4eLWfm3j8Syc88HKb7k/OiQzaD2C8AmmaXsUZ2TpR5/USiV0dayfS6v36ACGjZeLNQuc9w2JzqvMGKjzOOyz0znVysp1dlS8+pbnSDrvnC2yOeumQl53tk1E6+osDRvjGxVPFscPs63AO40K758lweiFkYVwkdACBzo7GX4xL1Mfz5Rki9tXVDCKtzs6Dn5k0vC8dXlLRE8mATIOPWbbU8Z3ExxTVvsQaGYfyYiLnOO1UaWnCrp435SaDz/GcIVk3g5xy7NnYQTAWthPUV44DDZA8AS+wWtAZkYaQvVCcM6wJk2SO2XyBHdsWvXDdKcKrNBbaka6njUP1YI/wCanp53MsHLcdanDbnW7IVqQdiDBb7NBpuvS88UDOeSXLZtAkxeMtU0sBVM76wK5B7seFOtPUfOVrluwSYoV6Q176B9wWdr31q3A/bbAgmci9xDrDlQ2dusL9HlAgixsnsu0J149v36u+YQhFq/+bqKQtjBMkbzFsUjWuLDWpr1ZEqKydt13Egs5WPR+RWcuWtHx2POfNUWFGLuvXlsW+ruvUVZ6VB13W8Z3wcHuTxj1OErzFO9xqA7NwXJBWuF7+PuJmjNlGylWLMyJ+me+RxqRhf+C8ipFsz/XJCjY2Ks8OeXWBbvz2OOTi1X5ey+1dPE8LdY6tsVt4ntX5sTx2EUmMERPpyfbY0q5tOrVEL1Fymda9l7V7y3jR8WlmqfSOAx9RFyPdZDxWsAug5KiIa5GpY+vSLU6XM7LlHOmhQGXymS1xuFxw3KSYry8B59ZUvMK0diztLb+nfe6PREPmtp/c/d0IdBIyfyBjUkCEohlE3MHlIzj0qKz9l7PLnobla04IMFq8QanuB7xFi0blXV6wjt7ACpTHDZf6zBs1URrNSi8hY81FhCTLtcuHko/eaQhZieKS1wbTRCbjyByOrvB1jmZyCJkefOulkRI5HTqemAPC1eAWJVttQ/TAcdEMzi4e6ms/2mrNYvnAXU98vCQcb/jrm9I8TyvWHtDKAE86vmbo05YsB+rH7dXY073udzXId4W7FVYE4q0hOlyXEVorykYdZu+RcI+Y0rmV24TObH2iLDJext1elMbAnaw9wei2vLPNplr20ofJJ1ucqQ+23sc+P2bux1CjWW7KtGWxr/md5jqNW+rLack3UgSrsRVTmhp71f153fLS0vlZouD+avpngOAYzZ/+Gnzddki0JkjhYD76Aho1/9zQxwmplq+pvbkqkdORWPEC+9oGlU78WGI227E9qI2NSqXemQ9pyEyfSZvVBiNR5y/Xo3F2zWYDkaVtrKxNFCauJk0fIWTmPN5Gc/jAMVW+AHd5hddlpXw6+mSr2Kuj4tJuLW1nXSc+oA/H1XzOp8OIDdlE8CT6z0Kgk5DRkTTRBA1Zrx3H8AQxCRWWTZMmnZAHqNQsNzSkcThfpfo3dh7lZa01gVbKqsTrUeC4bgbEPkKmBrE1rBVV7fnfaMiavKvyibs+C1JNYAOEjAdbt+5gEhlpbYya4Ex9qCSeVHwCpUVgzVayazQQHK5kSO1lGf4iV6SxKz9bo0iaq3cZ9Q/Vj+uqlumaBWsiM0SkA+EN1NrW0Ni1qQftCddOYF3rw+zV2akq3P8goSOX0jhCsrvVmgBatlnOFqzN7S/rhctqhnR7sY5YKnE/77A+3tuEzBLtPZe2LPZ1XSefAIYK4WOrZpjNXPMBo/FQ/blCcUgRRwl2N7PJQ5MeXh4MELL6488Uar3PQ3L5aUmzX2t7TH7Wr01krGBzGdwZ+zhhEXfYkDlLnm37RTbDqI+Es+plCuz8NUeVbUGmvIoIJ9ZGnxfy4wZL+6xeXe+PELJi72rbSDzuL5G20ypv2LEGjewQaVnS2NHSB6x2wB3Psco6bGD77I69j/G+Zf4u2HiH6Sf4o+ycJzsKliXLDmAkuB+B7o5W4rKOsGAVeYnbjlTiilgZ7VM9eJsiBl+u/iXLlgG190I2X/oVcpJnd9MaKKUFMktJIWNaI2Lol5fWmLCoOtdaMFp2XM0xX2yQnS6jCZkaAF3y5YYZu4s5VrsjzucMaW0fMkDIHG2ZqY1Nwuxr9ZwH0A5CRm5MaNJfzNTS5pl3Dqg8THs3tnzWbsHO/IxM9m9g0rUeO9gM1o/WyVS7xDTQWzuqOsOprOcZ61mM1PLkT8E2GbFEci/LHPs0xmxtk6XQct0LD/ssUDcXBCd3L07nbdANxZBmuft9s+ttX9fv9BDxgdLgxZsrnryBwBzETjXw2vv16Fk+Dbi9YHK9aMiGNqJnrfWQXB4he17WmA/21QEcawJltc6oXZZW/ODlFEIG8McTjxOanM38ZUU68/aCe67Pzf0ThIzqRVgcVmxbzEultbbsheKkxha1zHnosGkcwL9vyZJx7cax7s9B/McHds+T4TyEkIVxkdABBHo72vOK3UK5vWiMRJvOP6mzewPlgFjeY2+Ap6fVA+fdAvN5ynZUzsRsLat6GYVvnycs6EDn+xWbyNimqbMJ37K89qfkEBuPONikQmnIzBKkKtJ5zkbb1gTGk0yE79CQNYDopc+YDll/4ryMUBPTJhJfOTh4z8ytOkHghUeRI39cxhv1D2nITAH0yxO1tWxjnjnhNCnQctkM69bB30OErMTjtscijpFmjdbLFNPZ/8s7zrsl0voIJbJ3oyOUbpb2y+Ti/jbLfq5WOeUdh81yqZsqcDfwvnEbzpQdHu861dfLhdpZ/eolPqXaNGCWEYmc0dJffZZo4H3lFdkzdkt1nBSTfHZVsEZ2q7fI1RVpXBkkmJON5UXH6ZWLGbZlq1lnN+rCac+hckblqCPxh2pgeXXqOKXttZLtFiuyDTVuLMzyoCGexr7sEwhZvWRZrz6YJUtjh2mdjkG7fvl4tjWbOjSnaNB5vbTMOcNhqg+PUe3QzEl+szht6j+ccN87TwbyEUIWAEWChhGY2tFs7VOrs/dpyIa2LxtthbbnabQyzcTUaMiG6uW9oOUN21kE+pp3lrnqbPTB0KvGTgpQX6GpMaItCxwW45YsQZsO4girejfZE5dNY9Sv7D+2tWHz87LBW60hU7sAa7suktEhf7S0FVvb640NmbHVUJPhuCXLCvfLpTZgrm5bRNqWTdmQWaQRLzy0fRlrFFNt1Evytdp9ycbF+/0ex/NtGiHDQP2eN5xrex81OTBx7AqvChw2akNC3dzWha0dsoL15QuX3U7ZcLUfdhi0m0PGfZJB7fSGeNcQ/ECW3UHBiUm1dbcxdLeGrKug+p0OlqdTEeFNtzAcSYUqrW/GrnEChEx/9NSvhBGgzJHR5oe8tvI3T8K/vly+qQSPM40tqT2ODI0fdd2pZL8clmZIk9OMVc55ocG8wtXrD6UNKeYsVwszxpbCE6yzDJvULBUqDZqxN4uSTffmHlOwdv9BeW14J5Ax6o+QrBuj/jg94s7fRCtEcYrN8YIbf/hEbM9akNY0mmGZnXG70EcNbRRwP1JNkaRRPXqrIU27jbFlVON9W6OfqnOLDVFtCpx8NXWeFEI2GWJJQAhM7Wj9hOyMFWmajKnIRIj7J8cpmXmEDGSzssJbvcuxnRfbjtlGsvRFn2dYJnPM0wUW6wyHXdo4ty1vyGgZa3NleyEezK38q/sJmwXtoCTHtTvsKa15Tto92nU0T7BYrLA5ZljXhAyo7kes5rFKS/ZDDiFT2sFLtla7DJMkuMtyHCEr2X5tnZKmZI55ssahJjtmlyXVn54vsTndlbbwpTSn8Txhh8HDzR2YoO0meJywnMVIqK70Z+2yJC2Ts4tUHxBf7zDdnZV/tq5wu5zA9Uf6nDOB13n/eUJmG8/XYrzzoq7Th0hEoL2/ipC9s56hZHXd6eGH6u/l/ml5KVMNXv5zfMxVuB/1kuEswfpwxn5p7ZSkjxJe6Zhhcx16W8nmT7m4MGMbbLcXMe3qtk6MIPOBtd4ZTpsVljuw2YPWErP2NSKC7I4tLkKB/uJGGLjzx/smutOmTfDkq6nzpBCyyRBLAkJgakezUWsdS9HSlFhfjPoLqO8rdciGzPnqtAVpXbePTiI/Q7Ntl4aslYEEfCoCHx1wP1UYJ7PPJ2S8NvfuJUtHOPsmuOzVp1lQ717f+2Znb65rI+gPkYhwe5f38UuWRp7W71ccnTQ4bk3XNDpyD+U/tZGczKfeqCPfyrJE699IJeXUEofjD/fjfoP+js0TRusWsgEcFsqJMXWeFELmwCc3YxGY2tHG5vuT4lXFHunS8iH2k4T7FbKEzwn8FVX/pZUcf5blLwXou6rN2naz7On9rrr8hH2XsD+n3KnzpBCyn9N2f5UkUzvaX1U5EVYQEAQEAUFAEPggAlPnSSFkHwT8tyaf2tEanCo8ixyF8aDdPAhu7//Qln8rb7kUBAQBQUAQEAT+JAJT50khZH+ydf6hsro7WnhdvjF36DakDNnlhML+IRilKoKAICAICAL/KALd82S4wkLIwrhI6AAC4zuaNtS9XLHVxpLGQSwVYftPMn6NzNZlMsgMETI7jYlb/45wkzFQNXksCAgCgoAgIAh8GIHx86QqSgjZhyH/nRmM72j+zqluDVkIyRAhC8WrwwYcW9bx5EIQEAQEAUFAEPhCBMbPk0oIIWRf2Bj/ctbdHU0RLt/Z3vEa1pApjCo8LntslrTlfo3sTEeRqz8hZP9yL5K6CQKCgCDw7yLQPU+G6yyELIyLhA4g0N3RhjRg7eevywpxkiEnd/jlHadVjESfHciEzHjhbwzRaul8n2Z8ZEd9XEcdTS4EAUFAEBAEBIE/ikD3PBkWQwhZGBcJHUCgu6OFNWRpmuHG5w/5hMy/N0f+qEO2hzRkvkfl53mJSPziDLSePBYEBAFBQBD4agS658lwyULIwrhI6AACUztak13bG/79QGf13fR5keqMRXN+5DRCRocVJ5h95pkwjeByJQgIAoKAICAIjEZg6jz59xGy1wVrOvA5PaDgIxseONGJ8PqE+tFIScQPITDU0YaOM3KOtKAzGncLJAkd6jpHsj6i0Kd5TyNkH6qSJBYEBAFBQBAQBD4NgaF50i/oryVks4UhZGoXnxAyv2m/9n5qR2ukCSxRNg9bVz4ha7m8mMeI6HBac/5Y/bvF4Hm4rdIkQBAQBAQBQUAQ+BwEps6THyRkmgwtj7iYU+OjGRb7HM/iiBVNllGM+eYM2zE7GWJv+BT5CNFsgc3pDnM+KdsERRF2pwt2izniKEI8X+H00DH48FqtETPXkXW2lrEfsk+aJ5k2J9ybQrCiNOsTrsc1yP9Vcnx8Tgv8klymdrQGlo8RsiYfuRIEBAFBQBAQBH4uAlPnyc8hZJo07bIN0liTo3iO1W6H5Uzdb696Dep+QEJkKE6xyTKsE/XcECJDyCImURl2yxkiir+9KhsjQ8JoiZJI126FN85vge1+j/2pQIk7DpxvjHSTIVsnKo/kCKZdJg9KR4Rwt8FeWZz/3Jb9YZJN7WiN+NMImRyd1CAnV4KAICAICAJ/DwJT58lPImQLnLSCKc8UwVrogMcpZTIU7wsAFW479zkeR6REjKId8gowhIyMvFmh9TxjyYRrD8oBhkwZmzH/nkq57RQBW5wUAcMDR7IzI82bKkRpyKIEmTJE+3ta+IdIOrWj/RCxRQxBQBAQBAQBQeCPIDB1nvwkQrbCRXvyZJufKMJKB9QEiwlZgT1r0GLwLUOilz2jBLRq6MYfQcAChKzY01JpBEUCFe5GLtbEBdL8kdb5hwqZ2tH+oapLVQQBQUAQEAQEgUEEps6TQsgGIZUIIQSmdjSTBxvlb6+1J34T3vUrS5aA7/zWx6oqS5TGPtJ/+KX3FcqyrO0/JxdVTZH7iet+j+tzcimc4E/1I9MW1bNAXjzfj82IapqyykeO/KFNQkak+1AUarORna2v34baIxT2IVlN4vKBPH9otzomsMAxTRHwNW0ifOy3OCJNj2pVp5WTUkKszkqLwX0l/8o2VGYi0R9xBzT1PS3xyHO0u2/ItGV6m/3Rd6PVzsDUefIPE7JmyTI1a5xmyTL2liyNCs3XZnXdR8qRKGFSL1nWHtvNkmXsLlmaZc8AkBLUj0B3R3vhuqUjkLx/euRjDejq7BKy8o7LfoMlpVlusL/c68HT32XZL9V7nlLfiDHbGj9o78njM9K8UJyb46N2p1s9SAUxq4v8k4NtXai64HdxBT2veA+Hb/22be2gNX2ICbyaxDrnlFdzNJfb99RuW78sXQHkpx3Wppz6d43dKXf76HB1SJ2P8yoCTbRDbRZ8R+ryU9huYVouZBiPpqxQ3arHBfvNsvUeLjd7XMwGqVF1ciO59VITpIu3eu/pdXfjuvmEZA6FuaneeRc837a7P72KM467tcZujd3xjMKc5WZEYMLljXE2wQuWWSdG1rmKZOJ85q/qK/aKETrfF3I9FCPyx2hbHNs7IZilAAAgAElEQVRNUZIgWe5wrvuUj2uoj9g74P34pqCmf5sQIBS3/6NwSp/q66+NDNOuuufJcD5/mJABMEb9UYJ1bdQfIz3eWUIGxV5u7CJgNZm64/BG9mFkwL/D5ligqo36IyTrxqg/To/gUvw8w9hIaA8C3R2t/SLZL0W706sNGKujJmHlHcdVhOSg+gOn1UcnBb9myys20ftJAdk1Pi5HnHJ/xO2p/AcfkSNc92u1xHUTI16docY15Rw3Sg7cX9uY2QIovN387OcfvG5NPLTzeYkzaaoGCVmbnNOO5ijLWSi7X7hShgbeUJibytyF8g2FPY4JotUJbaXbEyfqg507r5UsM/Kbt92wPaq6PmA/kpAZ4mZkbn77359QvFbdyiu28RsyPousSUFXZZ7hLd7C7LFynwINKU4wiyJwvZgoKm1PV18MhfthTd4p72yPzJFoqSKgrXqg3X9s8qf6ElsW+9Vw7svrBlG08eoc6k8ViixBvNgjr90CVHjmeyxiz964RbisPkF4JbQZrVESOAJp4m4+Lhgne85zI3/KHWFrNHKc4bsJWYnrNsZbltcfzW6fCuFqVcGMGRfSIBKhVf1MYdHf3scikLfJr2P4dvuUerf8c5ZTvWLj91dL6ndfds+T4Sz/PCGjQcFyexHPye0F7YxUf63O6ZMn/540YsUBC97NGWO+1Ib8ttuLeM5uL4yz0dbGgDA2EtqDQHdH659QWp3+fsBbtMPNWnJjDeebIiPuCxUQaOCFDKT49qBWnZhUpvXGGBawumEXvYF4aQszpwZfTMicsgA8TkjN5DYZe6Uhf7PJtpmV7HL0O7442VTJnvDsL2w7IV0/cVpEiGrtuHrOmGsCYLRP6gzVHW5Pe9m1Qvm8YZfEtR2sX4L7pW5PEk3fH9NmzgRZF9LkYYKc/mJNpPNYTbLOc0pUFdiT/Kc7XtZ7heqF+4nOjd1rp9qmhNCvXa/meVe9ioz8AapVDhOb4xofgcGvKRNT/bbq4T5u3Y2L/2StJdkUm41mujTWUjndj8ai2B2LTKE0JsV6TOKwDkJW59d6bnKiX3VaiVl+Z5w8QmbPkb5rKGgNFbU/eSMgpcOVv+QqPM47LNjdFJHpNbLrg5fNaTn4VE+AYz6mbHnt61C/sMPsazudvm6NGe34ZXHEOk2QpCmS+RyL3UV/NKm46iNBjwGt/Nwy3T7Sfrfs2F19244z9bp7ngzn9EFCFs5UQv99BLo7WrvT2y+FPUjzxPgRQnY/YsFfojHm9Ve8Lv94wXFltDm0HJippalkjtl8gZ0ZDfXXaq1h4oF0h9Mt0370ZljsrgEtCrVxhftpgwWp7ZMEKQ2M5stafxBQeJIssDvTwPjCdZeCB1KbHHwyIXvlR2wWVO4ccxqUL2pQ5l75vLEbGCXXkjXK/DHUFe51ZW6/hdYqDQyGXlI6pBRZrM0G+MhS0pa1NRz3Q4I4SbzJsT1wt/NX5grx2wqrRYwVq/FULLsP2ule9wtOe3vZco3d/oTLveOTmxPbstjXTd/vH9ybeLYs6rr9LCR7VSmiQaQu9BxkN3U+IrOWLZebDMdzyF6nLYVLOpvnwXpR/42pv7mkJxiXNdLN0nx2Luql4WA9mqJbV8PxS+T7VBHQxxmrOMH2bMwh7HZTWYfl1cX6fb1FuLz8Ws9b4tcBXK5NyMwqUodrKCa/tCK0PeFyzrCcpziQt4AiU3470y1OlzOy5RzpgVaMAn9cHzNutpdeu+3fpmrIFC6NppXK2uJ6DWnIaEjNsWPtrpH6hcua+hV9nHkYU7X8dvGq6vaR9rtlR+9tfzvihOvueTKciRCyMC4SOoBAd0drd3r7pWh3+gdrNJolywKHRTOwc9q+JUse+OwlS1V+FCXY3WhSrVDRF+klr50T89eu0fIECVmEZHtRy4ePIxaRvSvYAobJZMq7gyn0db/ryUUtwy4O2uFxmSN7M0REy+cQkU9csuTBPGmWq540EcXY8BoVlUM+/bRLmfKBOxPIrnCrrnRZFcjerKUPZ1DvMmBu8uC2f8tq7YzdtkZr9bxukdAS9LPk5aMo2Wqbp8Bg3GTNV69bhtQsLel6s5Nqnqts8te/NGIvi5nlDLcoWxb7WrUtL4n02uG035Em//YzGycj2+5a1PZq9vuF1hJzaLJVYb1KK+1uSNn6WnZAfr2qOw4LvYRlMD+oFY/2uw4orWQGXk0t7zitYiR7RRqceraE88/AVdrW2ha5AVBdlTn2aayWHw23fuU4LGeY8bm5drvpxJ+gIauJR++SpSss41QTssbOutboGTtrdg31wmWlXDitjWsDnR1hyz4715ea5LolfdJddcdps8B8TkuzwzZkZC/nDHeOGF47fDchMxrdCRvPnOp4N93zpBdR3wohC+MioQMIdHe0jslOD7ChQZpU8K5Rf6PRcSabkExdhGynSYeXpiqfuF8y9n2nBgmPILXyUwNGcHmpvGAdqaUhs+TOxfGXqrv0Qf751FKdV14tn6XFS0lz9R6jfj2Yry+1CQBlzzZregmPrxNapjNfoEqArvBaPCj7GmPXxuEDX6dNWlpJtImheuK2bYXikCKuiTTFeSE/LDFjf4LewO1kTlxxj2S2xNFZlslxWO/Ydsgty0s8+daWxb5uyFSwn9fleO8IT94zXqIxhMsQ1DpJ66LCs8hRPCt8xe7Ex2mBeDZzl+lYIbFqDL5fV2zoXOHVSds+AtXjjG06w+5WBZbaG3zq6liapP428tM+cV427pXq/KyLqnc3aGg3YInbrtuGbGc7D+8gvjWPtOpliRS8dAnZsGuo8rZTztXJxm+xwdHYv5a01K7IGi9zHjs2pnTIbvqe/VvXJyh5KNDfNanej5qo8kpGipT6DCe33x+VX9+SZWtn7MAY5PYpvw+58ve/s27csXfd82Q4ByFkYVwkdACBqR3NZDe10w9uW24RKPXSuUthxraCTo844nz+JEJGlaKvbloapZMpDmoAVAOsO8HyIMejW0g+g074tx8zOz/7uslLyWMMjEvcyc5kRoO5vTuqK5zy0Us/szUutlnXwGBYS/A8Yz2LkWpNiAl3B0vC8lHvLDVxmt/QBNo8HboK9SPbyNyehMx1NymyJxH7WuFP5L2/zTxpx0zelu2Ykc/8to3bLY2WmQCd3wFtJpPnBIe7WhZNsmbZy61XhefDqJ+8OmlbYXVySvOMiD85/VYfMGrzSrxRp7C0+kOTjMm5sxGC7eRmPZoXnXiAfITaeNQuS0e2wM2YNtXJ1Ptp/GYOEzJKVj5uatxhp+oxam1Z+cDtsFJmEaR169KW9fQn1a8G+khn/ez3IYBLKyj0XnfvnCRXL86nZHAMovRPPJ70Eelqxp0+VMtC8Ss8z9bHRv3sYxdT50khZB/D+9emHupoPNkF1L7ugK7h6xkc2pONB/kIQlblO8S8DGbKO/POuA9ryGxReBLTtg55FtjVZSKHSZN5yr/s66lE+bwjzwvkp76Bws5vWEPWlKN3cnqG2Ey+sqQ20KaBf7+IEacZeAW4yWDQfoM0XMWJliBnWJ8arafJwh0sTShQ/j97V+ukPOyt/6iqKhxqVV0VDoXCoVB1VfwDKBxzBQrHzJ1h5ic6V1ThuAbFVai6585JmjZJk7bZZfeF3bMz70s/8nHy5LTnaXJycjkgm6dIaBUjEQmaFkmXyM86G2zTt0d+IjKoR20hXeNv3KMTaXQmte+gXI1IPlRzzJIxYS+sAr3GzUrnOfXh6Ew+VNddjrKoaUTcjpJM52cxDeZ8fsVI5rLuJ9ln1Hfz1R76gKWQRw+ZMJ0iWe6aNP3taMmus129F8287jb0FuC9KUKSuIaRhnDWSjQJWTtl2UzHqilL9axWLSV5nNbaNCW5Z9R/tNJWkLVlE7Rd3Rr8HSO7N42LkFkjwsbHgdQXA0InySKpHWWLtF1fOPKX3J5vTkKmr7JU75jltsSFCdmganCCF0VgkJA9Sbn7X9Jq1V/rx6ViQekjZHLZ+7pZyXk7rsT+p18mZLczDmq6oN6eS05LSh+yRF8afr3Wfh31C1f5cdFrZlcTD/WiWmbY0L6s5FweRMjqqTvaEkyFPKinCuXOGRUuxyMu9fxqdaaXOYUD8F0nn58MWeMIbSmj98VZp6tKbFdbNBBZ2Z19e9tjFi1gLLCkfIYfnlXQiFNXXcIQKp8Rhb326xo96a+qNfxBRt9r3LTaPIZHjZKlhkVThNHSq6ZtvtGPCkWeYr5rV70LCe4FtisZMqjbrgpFZoZBUFLf6ENCLQBRF3t+zT7qGnFaDKMb06btjg8/s5q2X+h6tw16uA8fZuq6ucLXlNmsdeyZfD9FiJKVDHysnPqdoaHkqGWcrrA7nrFfy32aKUyQGOGhhQC7I87iQ4jKlKvVO7L0fAT3h+yoSxI665gFMMJYdGp1XBhPsswQGY6iHJdC+qerFzQtHiOeHzyLuhwVWpeG7KSVHDxCZiPC56MQGFK0rnKPKraTaPiBopWLM0ziKRJavXOXL1+dkIll4qsU02mC2WyB1S7HsnE0tdKLF42+SEC+MJw+ZPcCu9VcrLCUy7NVHDExX1M7viZi5IBWYKoRpuqyEys4J+QUW8ff6zRcuzCE5XU/xyROoIpqVlnS0nFjleVD+M+JJeX1CsytYEu+65oQrsMhQubKo11z9u1PE7K+AJiarOMOW8Pv7zP/KF5DMBRx0klWENYOIzeuAYOpuu36LkI2KEpAgrZfKFO3DQFFWUmdOmylGTylD5cZxS2bYHWSU8B62ItOaKhii2VKK8glQZ1n9XuHiPOyXsVNHxpGwFZLiiB9svLSqfcjIlT3HOm9sjnSOkTTL4X0T1cvHjjnc8xzNcWulzzueMhO2qUwIbMR4fNRCAwp2pB/jm5r+ioMeaD6ynnne90XxYu0xvviHCefr28/P2Xpr9c1tTSko+4Vlv46aEWvcrR/ep/1jWgIAqePeg2TvrHPn91ad7to8cXIKUu7QO38OxYnDPbxZ4Go5e6MbisyLX7N0TStqf/+cFCfBraVGvDNGw+rg2QNyDa+bOKNug9ZP+w+N5v+XP13h+yknZsJmY0In49CIFTRRhXKiZwI/MS+iM6K+SIjYCMQsJelnZXPGYG/hkConWRC9tc05EntDVW0J1XLxTACjAAjwAgwAm+BQKidZEL2Ft36ekKGKlrbAtcy5/bu0BH5VthL6V15mnRfnFZzlf2Ua9UNZVE2wWrNMuUybOPaV0Ymeusyavk3J9S2ZnnYvxHhGbU2OveMwr65DFcYkOdVaceiel7JXBIj8E4IhNpJJmTv1LsvJOuQolXXI/IlrUyiIKd6gEKHz0CnXXcUe7WdzRLZvi9IaoXLbi42Qo7Jqb+OjND4unQI2ZB/je6L0xHMf6FTj0x6L/bIBA4plvkBpQrb5EkvcjkcZpv2+CRw+nT07fcmHZ2dq9Y6PkndtugO6Mts711J6RNXv97x87DaIraaslZDuvxIxsaPeoZfkas/XNeonboOfAU3gVPAIoTH5YhNvXUS9ZEeDLiDud4hdOzQQSNJ7wfCmGe8LW1QljYpHzECb4XAkJ20G8OEzEaEz0ch0Kto96PYN06GXqBl2u32KM5YMkaNchuhj1W9dVF1E9Gz1fLtjtET251oW+3UUeqbdB3iE2YsdNFEmRYxEPuyEcnq1KP2Vczq/S0rXCnchtq8WKSv4+fQsn2LhOiGm5zLy8AwImIpfbyB2CnSIZsKD+JcPao3Wh1fd2JLo044CgrCT+ENkl0deVtl0H47bUsEgVbbqfQa5Iq2nYoQfeQovKNocheBeLZB0exAUOFWbDBTWylp4vQd9sqiZRS6YC2Hb3ROS4ev4KaXA7nFGG3gffbioGUQoRNm2IpAYBVuIrr7rNnAvr+dFJolRhTFMKLTa8U729rct58xed6J1l6z6n5ZmkL5gBF4OwR67aSjNUzIHKDwpWEE+hRNbMOTaVsXXfdIm70j7Ze1VReRuShtDIe4W52R1ftJ2oZA1JUXshCtniZdh4wM1G+Jo582ZeoX1XGnHrk/ZBPcUaS7Yp9Gcl/JTnpZkD6aoo+o9datZGh+ZawiGRNNkUWN/Il0gSNkNcnOzjdjepG2ohLbzSwC9s8Tm6nPofb+9hrkeq9D2prnkCdiix4VQ61pKh2E7EFoZOyeeGUxkj7EhsdRtMRR2zNL9JEi7GoI7ym4PVBuZ4iTNdaLGPFiOC5S5xmEjH+ndMLbzscFhzVtH7RFUWwxiyeY1ztQ6BD066P9jNnneklhK+HMnHzGCLw2An120iU5EzIXKnxtEAG/osnNb82RF3ohqw26+1/OEMa6S8jW0Qe2FzuGkCQ9bV1UtpWuQ3wG6u9pea8R6tQTTsgo6nb8oUbUHrjs2pEn3dj3ByytI/An7SbertG7nmb6b90vOO43zRQsjeItsw32R7Wpuj+rcYf2+lSjd2J2zFyaTr5Y+Ypiy1GQSzXdrbZ2mmC2yrE51hu3D8WV6vRLrUOKOBlhCurgn4pMGUK3J1WZi9HC1erDIEde/fgsbtUd12KPdRpjMlcR7W84rlPEkzm2x4vHB7Hev1T/KBoiZCLUwBST6QL58dLuhfq44JjTVjwTJM3+g/Zz2GIjj+xnzD4303vJoZmMzxiBt0PAbyfdTWFC5saFrw4g4Fc0MwijLEZ/IevHrkrkdImcfnrgcZdf7BTxmzajNY2eXZcsW0yNTGO5CXLHIA/V75JJXjPrttKJetQWHtJ3i6YNW4JFm0Bn+IgpMr571KpjmDTZe+sWolAMLGm8O9scaeWIpJ0pRBWFvPtL3GTQ56ohNWPiLsn+VSM1JE+n3Y8ryuu93QbGgLoSW0rRptrNX+AI2TCWTcmdg8dlj0UcY0HDe7SfYhpD6Ord1s2v4/Y4ZRC+X1dtGK6WiEKhHDYrLFU0YFtSMWW5wK4eUryPmLLs34wb0HbssZ5Du3L7GbPPzfSi/yeJ3CZrgAybOfmMEXhtBPx20i03EzI3Lnx1AIE+RSs3MWLlIETl1ITgKBza+1/Osto7it1KbIAdTRIs89qfrEPI5DRMMy0opjbl6FpjdG0yAtOpX+xxqIzBQDDHpkwXNp166pboTv2687sj/eAImdehW0ZLn8xW2J2v7eiGktNRl7r1079VkeMjXhj76xmELJAsSvkfctrU40Nm+0H19mMfIGI63dqXk3wct7RJe5eQ9RX1E/fGO/Wbz4Thv9iQbXP0sB9D+xm3z83WG/1v3uIzRuCtEeizk66GMSFzocLXBhHoVTT6Oo/J0Z6KqR2uV6eaKPS/nPWKXS9q2xBIEpOjeFS1c7ncu61JN0BGXHXoMujHTZn6RWrh44GK9oykDczVKkorTef0UWK/6W6+/BUfsrYt5sghXW+ndZUkQ0bYWm06ED17TFT76iJ9ktZiiFDJ4Rgha2+JIx/uVjJ8bpUlbUGjRjZr0tGzN6I+SmTX7wzg+1ncHMS0+/Eg5d2KlRu2NOqcQqg8jNFG0tdbeUZhjby1+qPyqt/uM+vsE1p5ebviVtnp7XNVbiX8Ef31qnT8ywi8JwK9dtLRJCZkDlD40jACQ4pG03Oz6RQp7Zm42OHSzDD5Xs7dOsWLOjsLg/K4X0XcroJW9BkjRZVweJ6Ifd1WjaN4YzCeTch036OknmZZblFe/ITsLvaeo30la4NP+WifOYeHutOod0YGu1jRldaw6YRMjiJ2CZm7DHF1KOSBlbXB2rrenj5QitAkCdYqLkl7U5NbXTRj1Q2Xr/J94ndAP1wlCpyNUdW6X9U0uSuT41pou9r+dRTmu9Rpn64bZiZ/+d1nVsiuPwtiJG2OVb7F+Wanl+ftKkv1LMyxOt4c/W/KxWeMwLsiMGQn7XYxIbMR4fNRCIQqWluo/bJu79BRZ2+4ZYbNZoPNZo9jUaJLyMz86qwxdh2DpFLIX78RMtMNnnnqqYoMMYVssNyAqusei2gGO4xEI7ddYW/cJ60tNVGg2F2ChNEiiThCnGmrXkVy20jWpKKZorJHyIhwWiNJTdoU/hEyIswJJvOtN1ZZtw9MHekLuPpl/zZPv9nw6+ddeeVdZ9+J8j+Dm16jPPbV203ZXqnOaxm+QsUMqQpkQh8KY9SMcojyXUQzNcOUtKX7jsz+86VS1+120bT2JJogVzKrhPzLCLwZAqF2kgnZm3Xwq4gbqmit3GEv6zafPHIaPTuRPqKkG1zHNJDPXybYt1ivR5NHEbLzVwmZVqbvsDVs9SjI7ow9hUnIDjisUqyPddRcUUBgP3ja55Ml5Hort8oVKJvKNvh7x2ltE0/fuUVItbK78sqbTt18Im6+ejXRzEOaRo8TrNcLxAl9FMgVuPFqjVWcIDub8+v+8kP7Iyx9p97rHst0aYa+MVvGZ4zAWyAQaieZkL1Ft76ekKGK1rbAnI5qr487chq9vqweX62+LJ+6J3yF3KsMnVOWyxxHx5Tl0IhPH1G0RxfnMxq5OuBK08XVBbt5qo06DPmQpTDqGvKFol0STPs+GsaOQbYWXnRJ8+frGi1UT0IbZ0M+2//sibh1ceoR8nbAPE7rKeJ2NwsKn0HuAzRCu5pOsNI6rbddqaUPPVUPB382M4t6DWUz7/MZI/CuCITaSSZk79rT/1juUEV7mrgjpu6eVhcX9DMIUJ82PoY/U+WfrKVvRcKfBIQbzQh8LwKhdpIJ2ff2x68tPVTRfi0Q3DBGgBFgBBgBRsCBQKidZELmAJEvDSMQqmjDJXIKRoARYAQYAUbg9yAQaieZkP2evv/RloQq2o8Kx5UxAowAI8AIMAL/GIFQO8mE7B932LtWH6po79pOlpsRYAQYAUaAEfgMAqF2kgnZZ1DmPAhVNIaMEWAEGAFGgBH4SwiE2kkmZH9JO57Y1lBFe2LVXBQjwAgwAowAI/DyCITaSSZkL9+lrylgqKK9ZitYKkaAEWAEGAFG4HsQCLWTTMi+px9+famhivbrAeEGMgKMACPACDACGgKhdpIJmQYeH45HIFTRxpfMKRkBRoARYAQYgfdHINROMiF7/z7/Jy0IVbR/IiRXyggwAowAI8AI/CMEQu0kE7J/1FHvXm2oor17e1l+RoARYAQYAUYgBIFQO8mELARdTtsgEKpoTUY+YAQYAUaAEWAE/gACoXaSCdkfUIrvaGKoon2HDFwmI8AIMAKMACPwqgiE2kkmZK/aky8uV6iivXhzWDxGgBFgBBgBRuCpCITaSSZkT4X/7xQWqmh/BxluKSPACDACjAAjgOAdbZiQsdZ8CgEmZJ+CjTMxAowAI8AI/BEEQu0kE7I/ohjPbmaooj27fi6PEWAEGAFGgBF4ZQRC7SQTslfuzReWLVTRXrgpLBojwAgwAowAI/B0BELtJBOyp3fB3ygwVNH+BircSkaAEWAEGAFGQCIQaidHEzIqmP8xBqwDrAOsA6wDrAOsA6wD43QghJyOJmQhhXLa348APYz8xwgwAowAI8AIMAJuBELtJBMyN458dQCBUEUbKI5vMwKMACPACDACvwqBUDvJhOxXdf/PNSZU0X5OMq6JEWAEGAFGgBH49wiE2kkmZP++z95SglBFe8tGstCMACPACDACjMAnEQi1k0zIPgn0X88Wqmh/HS9uPyPACDACjMDfQiDUTjIh+1v68bTWhira0yrmghgBRoARYAQYgTdAINROMiF7g059RRFDFe0V28AyMQKMACPACDAC34VAqJ1kQvZdPfHLy+1VtOqKQzbDJIoQxVMstiUeAXjcL2cc8rnMvzjiHpCXkzICjAAjwAgwAq+AQK+ddAjIhMwBCl8aRsCvaBXOWYwoipGucqxndBxhtr8NFypSlNilKdJkIvJFTMhG4sbJGAFGgBFgBF4JAb+ddEvJhMyNC18dQMCraI8jljQylmxxoTLU+Ud9PlBuc7vMmZA1YPABI8AIMAKMwLsh4LWTnoYwIfMAw5f7EfAqWiGJVJwVqEQRV+zTCFG0wCFk7pEJWX8H8F1GgBFgBBiBl0bAayc9UjMh8wDDl/sR8Cna/bAQI1vxpqwLuOOwIEIWo7nUX7S8y4RsDEqchhFgBBgBRuBFEfDZSZ+4TMh8yPD1XgR8isaErBc2vskIMAKMACPwRxDw2Ulf85mQ+ZDh670IeBWtnrKMeMqyFz++yQgwAowAI/C7EfDaSU+zmZB5gOHL/Qh4Fc124lfnysmfiq0eeDykh5m3Fp6y9ELDNxgBRoARYAReHwGvnfSIzoTMAwxf7kfAr2gq7EWEZKnCXsSY76+ywKpAFpNP2QR54SBltxM2mw02q5lcZfmxQLbZYHseGzajX26+ywgwAowAI8AI/AQCfjvprp0JmRsXvjqAQK+i6YFhJwmW20IL7nrBLpWxyVrHf60yNTJGoTO0f860WjY+ZAQYAUaAEWAEXgmBXjvpEJQJmQMUvjSMQKiiGSVWZ6yjCIugOBhGCXzCCDACjAAjwAi8NAKhdpIJ2Ut35+sKF6poTUvuBTYUvT/JUTpmLJt0fMAIMAKMACPACLwxAqF2kgnZG3f2vxQ9VNEaWW8HZPkRl5DNLZvMfMAIMAKMACPACLwHAqF2kgnZe/Try0kZqmgv1wAWiBFgBBgBRoAR+EYEQu0kE7Jv7IzfXHSoov1mLLhtjAAjwAgwAoyAjUConWRCZiPI56MQCFW0UYVyIkaAEWAEGAFG4JcgEGonmZD9ko7/6WaEKtpPy8f1MQKMACPACDAC/xKBUDvJhOxf9tYb1x2qaG/cVBadEWAEGAFGgBEIRiDUTjIhC4aYMxACoYrGqDECjAAjwAgwAn8JgVA7yYTsL2nHE9saqmhPrJqLYgQYAUaAEWAEXh6BUDvJhOzlu/Q1BQxVtNdsBUvFCDACjAAjwAh8DwKhdpIJ2ff0w68vNVTRfj0g3EBGgBFgBBgBRkBDINROMiHTwOPD8QiEKtr4kjklI8AIMAKMACPw/giE2kkmZO/f5/+kBaGK9k+E5EoZAZyXucUAACAASURBVEaAEWAEGIF/hEConWRC9o866t2rDVW0d28vy88IMAKMACPACIQgEGonmZCFoMtpGwRCFa3JyAeMACPACDACjMAfQCDUTjIh+wNK8R1NDFW075CBy2QEGAFGgBFgBF4VgVA7+XKE7H5aI01TzHaXV8WY5frGwLCPa4Hi+vgcxrcTNpsTbp/L/U25KjweFVDdUBYlbtW4am6nDTanz7akrnNUVTecNht8vqqwdimRqscDEpYSRXnDMCx3HBYRorxURQz8fi8G5S5Fuhsvy60sUI7t/IGW2ber21gM7Zx0HoqrVsb9hHW6xumuXXvSIemHqRPUn/a1vsruOK1TrAeFe6Dcb7AvH7gfFogWB3xDcyxBA5+5b8TZEoxPn4zA+xMyeiiiCPFm7MvuyQhycaMQ6FM0MhDH3QqzicuAVrjsV5hNp0jTKabJEtuifQWWuSuPEqnELk0FYSfSrv41L90yRxTlMDWnwu28w2qeIElkniRJMF/tcB4ykOWuqUPVZf7u2rrES7OVidIJe61kuh+wiBY4tE0FqiuO+VLUQfJoMKAfB4WH51fVWd9WHzmm7CnS9Ql3lMijCL08R5Q3QaJh3rTP1S6PWO1lSQIWh3uAEZR5Rr8XvoyBW9fSmoB0++eOYp9hKTBaIj+UmmFv29tioB8N3Qe6BKXNH0QkOv3Vj6tbd2q975TVyvS1I4dOduqShMut01S7A9Pqgt18giiKhe7Lz502XS+OjufbX7fWeudHYrd9bpxr0t9pu1Y+H740An120iX4642QMSFz9dPLXetTNPHFXhyRp11y9TitEX/kKNQg2G2PRZRgd5VN7Bq6nqZXZ2REJoo6jWWE6eptP0OUZDjrRIhe1+cMSTTDvm8QylFeI419z/fSVOk69+84LmIkeQGC4nZYIE42KOthgV4cNKI4jSPEU0UE69EKVWcjrDroGgKMJmQ20a3L7LRL1UW/sr4JEeH1CosogjzeYrOIEEbIIEgq5RF/346BC6u2bWb/1H2ZneUIaHXFYREjzop6lKc1+m0J2tF1j1kUIZrtUD8G2k067JfFJhLiPKYPHqUX9FvrhqO/qC0NrlbNnVNdtxxlQesXs36SYYrY/ihpKvAQLNGGNU6l/UHTxdRuNz0bersu2w/EiwNujxJ5EmF5pCevLcfGsRFNP9DbT9ftcz2t9363P3vr7uDch1WKxPkhbAvG5z+BQJ+ddNX/JUJW5rEYzWqV/oHTKkIUTaAGuO7FFoupTDdJlthdaosjlCxCtNzjtFsKJUp2V/nFLF5OS6xm9DUTYTLLcLy2A9iPct/ciyYzrPaXdnibXobZDPQwkhzJMsepyXtHsV1o93ZQ4rjA4Wt+BIYVTb7o7Ckm05BR+e0Lkc669/0yVOcMMfWxMjwJ6YtOHMyyzZL67tUp+1629j3rpSmmtEguJZN1H5ctPqIM50atr9inEVYnyVTH4eBpgy2banj9zM0MFqqRJmW0VXr1K8rTcK7xFiOTdrtUHvGrGx79uJW71xAZZQE0jUtTS+ZfW5Zx/csY6PIaJYsTo38EBnMcdHJvYWaTA1lihVuxwSxOsD4WOK4TxOkah4vdxn5ZbAztc0N6R3+5cTVytSc6ro6y2oSOozHp7wX2mRw1TpcZ9mrYuJO32+9mu+37F2w/2o+36z5FtDrhob1/zPwO+emS3n7XuZ3NTi/ud/uzt+5O2+1KzHNDN81bfPbDCAzbSVOgLxGyqiCDGCFak2LTXyGmP6I4Q0FuM0WOD7o/mSPLV0iJJMVrCJsjlIxIE92fYZWtsDnX8/h0LZpgtsqRiSHmCNFHPXpw2SKh+3GKVZ5jmcgyiMzRnySJNCS9x/GQYz5NsRXDDhWK/EMSvHmGfJUK2eNGdpGd/xuJwLCiyReiTcieNkJ2PyNLIiSbsiXjjpffPx0hq6r2BW69VO/Hrr8KvUjVlNy4l6ptdOrOc+BAdy7bBHGS4CPWiWDXOHhVwGqDSOe61hSgl60ft3L3GqKmnL6Dtiwj1Zcx0OU1ShYnRv+IkdoPbDW318dxiSjd1yNetowVyt0cyYTeceZU9b3YiY9NGvXMz4qY9ctiY2ifG9L39peR0n2i4xpa1mD6C7ZJjMXhKp7p6nrAIk4krp28Nqawpr+t+48TVvroHLXjY4vLNxOyx2mFKFpJm9cg2u3PZ/aZoZtNnXzwLxAYtpOmVF8iZKinjJpRCfHVH9VD9fKBiKIEm3oeRhjHKMKaGJl4wIhMJcjVPA2Nl6gpSzUPVRXI6tEuIlbnTBKw2b4e3L/ukAoCRySQpg7k/eXRnqOiIe8IUTMtdMN+RmlrgmjiwmcDCAwrWt3/Heck3YeM/LpCfMhIKBpV2GI+mWC+K/HQp0jUaJQh+1d8yFw+aXXhumGiS0KfY0zVaB39zna4qHSWQXG9gPUXqX5sNMc4qT+AsjMqaNMYDhxupzUSMki3B8o8QZSs61HnrnHQq6jKjTb1NUUcz5Dt9tht1si3exSFPZWk59bL1o+lboipVho9/5Ij9XdhIOUVU6x6nzp9yCqUmwTRbAsxgPcoxJRYOxJpkQOCqBpwUK+q9kPjM1OWPkwNPR12yBd62pn+1H3IlM47yrL9p657pMaosK4r9AwdsYj0kcYbDvMIC3qXG3JT/V1MbVmNUUnr+ZMjXXLUV6VzPZOWhO0Hlrqhnm91bvxWKDKaHYqR0QhF86c/C/KiLXsz3UuOqLbsTTl00C5KkJeljUyVfTTS8slPIzBsJ02JvkbIoKYopQ/Q7TAXyicIV0PWMpzuD7FC5nHO5AjVVikZjXRtWsdonZCpOc/6waORtMXxv7ER5CxupkSVfwUROxokozrECJqY6lxhVw950/SWGI3LTrjTCq/HoyZ3E5A4/BeGwLCi+QhZfz19qyyrYoNkOu2MKjQl9r4cm1TjD3Syl6Ziqrv11yKfGM2p31fq44qiuOLxKLHf7KXBprTlBnHsmF6tPyTGEDIxZTuZYBItIdxhlAwGDhXKLY0GJ8gaRzqaup9jMqMRnK5xUMWM+u1dPaqXrR+3xnSUEewR5CcwcPVF99od5SFvnfqPl3rWgIRv2yvO6pXkjdE1CJ/D72ugj2wM7XMDvl7jbqQUJ18qy9BDmkDJO+97s0ayJzHi2QbHosBxM0Mc16NLHbm1DxAdP7FQhUo1MZeDBykUTxH2QIxgtul624p6kYf42NGm7/Xzpu66VTf6WJkhyxaIxLOmWqs/C+pa/dtppzCK3QVBTbZWfnlJI7FNGj74VwgM20lTsi8SMkAOyZLz5EX6j8ULCJsiFEuOVgkiJEax5PnkU4Qsxvr0n0FCRs17XM/YLsiBlOqLQaNl4mHTZGhlYkJmqsS4s2FFky8Ke8pSlu5bwTbkkNqOGtzLA3b5CvP6ZTxf5difLrjd1Zeoow795am/xEeFL7BffG6cGt8xvXxxbDs0q+kZ6XhUlTkSZXzG+NJV5JgcY3W6ial44aysRLIN4f0KfySRcUvw/e3SHMZV/c2vbnj04xbLfiPYFOQ++CEMuuSLBkq6C1akkHa4DRmugT4AKcxH8+cyvPXNLiY6dk0JqB53XG/dcA3d/DKPWKkpSIK12rctsnPkKws0gjdUlqWHt+MSU9/IXVPzA1fyO6YP8G3R6m0PXk1W46DVMXn5gdM6xgctoqlu2C8iJGKOuU3nbateriWHP89NLOyg+ipcxWxMkp3rlbfu/hTVWOV7rzUytfKLS1WJTTLpXzXd5OWD70Zg2E6aEnyZkDVTj6s11rTia3FslU6MZulD0FrlQvFGjJBVZ6wFkaIRsHbKshmSVVOWtd8avSjUH/krCeK1POJe5pKgzQ8vFqdKSftev8OKJl8UbkLmb6vf0LV5yP8qTmihx72Z1qGVnVtataj7lLVZ5JFlIOzb/efWi68/cfeu60V7OyGbTTFNE0yni3bByxAhe1ywp1V86lmjl3AaY7LcQ/iDe9r5uByQzdMm/EeaJEjSJfKz7o3eFb33iqtdTQZpeCZUT5JgIlZZ0vEcsyR8lWVTLB38IAZCJ7OzHOW/XcSI59FHyGzsffj4rqtZAoO4KBz10bMU5PS+OZS4WvGzBEnQpxmTRE47L7coL31TzAbC4qRTlvrQWG5QdFY+WvltLKzb/lPZ3uwsZzJulwJlQauxHUTSG6PL8bxWJbZioViM6UrZgTadn1xpklr95szzoHro+dyjWU/2KMQzGpMbA31I+ELNiPLn2F+o7XdcKX6dr+1CrFZ+TUo+fBEEhu2kKejXCZk2pUjkp/WbqJfyE5mazLGlVUQUp2deO7oKxfMTMnLqn2cbrGf1Ck0iVSS7cuqPEiwbp/4YqQgkS18lEeJ0hd3xjP06EYRMfgnJLxaScTLf4lgcxWqeuRrDNnHhswEEhhVNvijchMxjYNTLvnfEqucFNGQAhu73trmnXi2fMN6T2gCq9jS/Dj8bLa9+2EdMKURGuj6aHxZidXEunYdd7bxRaIVFN8wH+Tt92D4uuiTy+FntkqW1WJoG7YpdGiNKyNm6/+97MahQbEzyQyOwm80Gm80OhzPFj3OPkFFohUgLp6IWPq3b5bQ1BESMlP+VWVca6FdnYtiPW78/0kBe+7ZFTjohL3wj0ipGn1GePaI9xyonvDfYHc5+UmLLYJQ55sSni568Vn1u7G84HwotDp0qi0Y0acDAImS3Qz3VrfRgiUzo2gab/dFqe3eqlnzgTFeKupx6CvW6ky4L+qITJRH/fi8Cw3bSrP8JhKyO9dSMYukVVLgeaSVkHb4iSbHMj/KroZeQxVhtVLgMCl1hhqfQw17EUwp7UTb+GhRmYyni3UglnWeH9iulDsQp4rSIcAlL5Ee5okeXmo+HEfArWvvCoBdFVJOTJnirKNp6IQ1XZ6RoR8jUKjQKhP/cETJvoMaGXKmXp0my+oiU0YiBky+V812ErLNAY6AR3ts+I0jvi0wQRxVuzFvE0I1vwECv0tU/IrZdvMB6ncipMeHcH2OVZ0hiWlChlWAZdu2OtVpQv+M+dpMCd9pvJWSeKp9y2YeX7/roSn26qBdgk0X17Dt+ez8mVZmB778vtrG6HpHNzJhsShL+/V4E/HbSXe9TCJm7aL76mxEIVTQTi6EX3LCzvNOH7HxtiLlZX30mnPSHy3bmHXmx39dqzFYusqIvbZ3kIiNilu/zU5bPapdsXQW1lZBNJoQvXeILkDqyEyjZN2Cg124Tsut+hsl0hT3NUdFo5ZI+QidY7uUH3/2cYzaZIlMRkYciv9sO4nrl1rGNoXXbPPVO8ZnJRp09s6yhCn2kZAjHUQRJVk4f+hvt435IpM/f/1lCBuFr2Qbf/rzcnDMUgVA7yYQsFGFOLxAIVTSG7ScRsB3Lf7LuL9R1O2I5W39+X02j6m/GoLKc9I265Ynmzuq4y5cYgbEIyIUhrXf02HyU7objcob1pzerDamL09oIhNpJJmQ2gnw+CoFQRRtVKCdiBBgBRoARYAR+CQKhdpIJ2S/p+J9uRqiijZWvLw7ZYBl2IMrBDCMT0GiIEbNABva0L/WWNnK6tJk28U3R9FbyuZtNnSOzi+nLgKmgkcU6k4XKZhZCIRS00Anmzc7Z1+rqFNdzYVyoEV8BYq/Y8tasMPal++nrcgP0djp6TP1fxZzqHPschroBhMr2pXfXGLA4zdshEGonmZC9XRe/hsC9ilbvJ5ok004kfoq0f9mvMJvS5sdTTIMi9bt9z5oFA06/oS9E6q+hdvno2D5EnRVmtfN/w1s6slW47OYiFATtX6hmFJq6egmZGwcz0KjLV86O6i0b2NTZqJavfLmAodN2OwhnU86IA6cP0Axi0bQzBIRZJm2vI0KHJKkMGqwv4rFXs4msdxS7On7dMjf2yO3iYNYVdNbrXzXkQyQXxjR6TThQMNnar8wppxNH3encpQ91iwbz6gtX2kU7jb4JJVdtap3kW7y+C3OrLm87fHpbx5PrrIqWWHVx7p867D4XLQL2UbdsOwWf/wYEeu2ko4FMyByg8KVhBPyKVqGkPUMXMs6P3M+0jR/0tL0sScR6Nwi1y5bLkfvTe1laUfob40NEa1d2A4N2CJc0UM3WO2KlsRaZn7YZEyvv6q2MlkexIKF5UfcSsrp/7DT2eacbLQPWFGPvq6mMa6cAcaFreNzlunMPXL0dMNfiTTV4uLKJbdU+kCtHeTxkkNxmf1q7HXKLIxVE91FQMN46kPUI8ucj3UI37FAVvX2h64aLKHXx1HHQj12wdK51dNNK0Sdr515XNlmawtq+/xXMNfJnh9AQJNCuq21XV0fdwXxd6VQpHZw7WKiUNQJGKBQpWyccRR+pNovjs1+AgN9OuhvHhMyNC18dQMCraDVJypq4S1fs0wgqkG/3BWi+VLv3/YKIbXNE+JJ6JMAmPb0jN2a97looDMMGqzmVP8dqU4dscQVu7Rg9ZaDqkq37Il6VYpJijz+5RUxjBAZe/qJUO4193mmUu81NnU16S/bmujzo9pG7XCvbqFOxKbfY9Fkm78qmFWNhKu6Ia2prGxmMtonWITaYbrfPodFa2hv3ow7Q1FuXVq3rsJO3ty/68e1s+2ORxU5dLoH0ay6c9Pt9snbuWX3dfLgorK37z8Lc2QarrqZN8jptV6RHGhF6a4Xh6epyU0g3/EgHizYtHZll+WSTeYL70KyKz94EAa+d9MjPhMwDDF/uR8CraIJcdI1elNEG2LTV1hrxR45mUONGEbjbJdnmS61HhvsZWRKZkfkdL+1Pj5AJWVeIEyUrjb4kiFcnMZLVkbNTt2V0jftyD9hFE2yL0n6AeEHzoh54+Qtk7DT2eQe+C7YfLQFRt5s61QXnVF9z0zI8dF0byejEaSMyq095teV0juqtkBbHI3aqHHvkSc9Exj5OrBGyBFE92tgJwEn7h0baKGVDdGTQ6S4OemX9x528zr5QU16WbnSKlsbcGF3RcOjURflFfZ5AswJL10hcXbFTVt89F9GgLc1Um6z7z8LceH4UYFZd6jIFD48TJBTwuPkwtAmTTNyEcyF8m90NPFOWfTgxIVPo86+GgNdOamn0QyZkOhp8PBoBr6KJF2c7RUkFCvLSbAWj+5DRNjpLbOsN4Ju0zZCGS5wKt2KL+WSC+a7Eo/lCT5F2Rsgo/+d9yMpNjEiNYlFR2ubIok3613bHYEgD5Z6ytA2JllYZ3oGXv0DGTmOf2/CJaP2R2OhYHznoGnhNHkWMxK/fF8euKvi83m4mWZ+MUY2ubGbJ0oeMtoCi/k8wW2kbuDckoc7T6SONADfk7OCIsG7W6Tq7HeZQHx3ivuiLLkFaZhucbjq+LqJk64dDTkUeVLyyob53Ca2u9eXt3JOyGWQxJZ8/2aa8tGR/EuYC36j+0NOeeQo+3X7YUJSHE9ZiW64bHrQ/bJRgXQf/7nxEqfarvp/T1mNqy6IrLtaWVENBdc3yLRy0uuhQ6LXdh1YaPn1/BLx20tM0JmQeYPhyPwJeRRskZP3l9q1UqoqNdNxe7aBxuLZAx8u/vfmJI7FNV4L1/ozivBcverkNl+NrWzMSur+Z26lfTpWpaVzpCyeNTUNAOoZQl1+NSNGG5brR189tQ19J/6pVhswaOWjq1Kuoj00jIy8a1zzt1jFQxw0Wej2PK867FdK4Jtj6PWW4GjJv3Rw8tVZZ3o+d/RDLPEa8KUVJfTgMVWVgMpTYJoqd9F1jrsumHzdZe/WlSeU+sJzhxU4mhqP7mBFOhbW1cOQpmNPm3DEmk7iZXpYNMXGqyi3SOEK7iTdAO7fMJzPQDnl2H133c7nHZ/3RQWRZbI+1O+Bc/AAh+7Reu7uRr74eAl476RGVCZkHGL7cj4BX0XxTlvpIE3yr+FIIY+AdIaOpEfnnjNR/uuB2VykcddiOwWr0x8kUVEUXHLIPRB8ZDhexm6q4Yb/c69T+H4sstlO3FW77RbN/Y2NsRxlYNSpRV2vVoQsjHNjJWf5GayEyxPFCRpUfID2udrqu6XWFHFfFFqvdGdd2Fyzo4QkaPHyF6jjpx07SQ1PFMZK8lHp0O2ARJ2KqmIofrMsnQ+2rODf2RqoTVzcU+w2yZe3nOF8h3x1QtqrkKNUkGrZsTjktUtXd33AMqZKijOrfHiJujlp9HXPatzSm/U2pv6IEeamecRunO666IlnI6nrV3vKH6aiuZxzP2tZ6hn6pEmga+ib2qDRxs2XT01egNtHCp141UFn4920R8NpJT4uYkHmA4cv9CHgVTax8izTfDenUr28631ey+VJzp2z3srw3BO3Ze1nqNQsDqKYz7leUhX9zaW+Mrg5ZqlBuZzLsxXTV7HPYGFvny1+Xio7HELIKt+NaTN1k5/b1fzssMYkmWJ3uvURE9Ed2xoOmcm4XFM62S/Lbx2ttyf3ncvRQTf8NxtzScdKPbWxUhdUFuwWFW0kxnc6QqXgjXyJkQFUbZVWN+K1KbJIYs00Bsac0XazuuBzWxupOI484kcbcmBZUU9mj5PSRgW5NritjnkFXPnnNUfcXMBf7g0YJNjUJux2XmMQpcqHLjrpIiAd9RM3FFLYcnSXXCNpH+WxMh3vlrRvXPIv1uc9Pjzae355v1ghctw9JBpJnuS2706GqDv79VQh47aSnlUzIPMDw5X4E/IrWH/ZClqr70NQjB2q0in57LbvnJUwFd0iP1Yah+1ryzubiy0xOZ2z2ODpJSd0yY+m7VuDIw8YIGOTCl3kMIQOqyxHHizYEVRf3uF7FF3pTp7heodiYfUIGR0zlbHY4nF1k1JLDJ+6Y62IUJEIULbDXHd18eQVOatqWpmxr/8XrHrMowtgPASrexMGssKMPur4ax9pIVK1vhVmU2M7mMLd8nzpp/Bf65JS5ep4Rf7HNnVGETLRNrWY19YVIhx5DrSnYcdDblqpAns6xK03dpWnIlQhU52rnDXvaSLujPHVIlKxoPuKkOF3ipKbYU40EO0TvXBqFW52r025q6yTCJLfl61TDF94IAb+ddDeCCZkbF746gECvovUGhqWCv2bA2xGy9kX9nSNkLih8L1/fdVcZrmvNi9pLyJT/WNcINoZEEQTl8O2qSLvW1Kld6zvstvFr/SnrosUaG8ziCZbHG+RIyAybYiAivY6TOr6U2M4mWOS58B/aWgbd17ZQHHzlNNfFKtAPZKd2ZJLuPS47MfVWR9toko89sOUcTxZTdL51eqYebX0y8gZ83PS1y25LX9ruvecRMmNhQF1RqGzd56IrsbrSLfuK/TIVm9GrNPz7/gj02klH85iQOUDhS8MIhCqaWaLDv0uRCPFrO6SbuenM6UN2voqQFN3U9RVhfIbL9ubXbvhevs0yeqM9ijxpoydaWc7DR4n9Rl8x6Ez1lItd49BfbLftQ/3Z327hjD2JMV1scdZ9gO4FtssEkwmt4lN+Q5Zslu8UxYubz+fIDtL3R6zCnK9wNDmRVYg8Dd0qx1mIdbG6HpEvUyRJIh3IkwTJfIWdc1WKldlzqkft9yT5/stDRG7kx8DXMHcRsvApy9NaPZ+O35HtIMC7z4W/G16iD/3i8Z0nIRBqJ5mQPQn4v1ZMqKL9NXx+dXtpb08PP/pcu9vFGs781VMrc1bBF98TgZC9LN+zhSz1OyMQaieZkL1zb/9D2UMV7R+KylUzAowAI8AIMAI/jkConWRC9uNd9DsqDFW039FqbgUjwAgwAowAIzAOgVA7yYRsHK6cykIgVNGs7HzKCDACjAAjwAj8agRC7SQTsl+tDt/XuFBF+z5JuGRGgBFgBBgBRuD1EAi1k0zIXq8P30KiUEV7i0axkIwAI8AIMAKMwJMQCLWTTMieBPxfKyZU0f4aPtxeRoARYAQYgb+NQKidZEL2t/Xl060PVbRPV8QZGQFGgBFgBBiBN0Qg1E4yIXvDTn4FkUMV7RVkZhkYAUaAEWAEGIGfQiDUTjIh+6me+WX1hCraL2s+N4cRYAQYAUaAEehFINROMiHrhZNv+hAIVTRfOXydEWAEGAFGgBH4jQiE2kkmZL9RC36gTaGK9gMicRWMACPACDACjMDLIBBqJ5mQvUzXvZcgoYr2Xq1jaRkBRoARYAQYga8hEGonmZB9De8/mztU0f4sUNxwRoARYAQYgT+JQKidZEL2J9Xk640OVbSv18glMAKMACPACDAC74NAqJ1kQvY+fftSkoYq2ksJz8IwAowAI8AIMALfjEConWRC9s0d8luLD1W034oDt4sRYAQYAUaAEXAhEGonmZC5UORrgwiEKtpggZyAEWAEGAFGgBH4RQiE2kkmZL+o83+yKaGK9pOycV2MACPACDACjMC/RiDUTjIh+9c99qb1hyramzaTxWYEGAFGgBFgBD6FQKidHE3IqGD+xxiwDrAOsA6wDrAOsA6wDozTgRAmN5qQhRTKaX8/AvQw8h8jwAgwAowAI8AIuBEItZNMyNw48tUBBEIVbaA4vs0IMAKMACPACPwqBELtJBOyX9X9P9eYUEX7Ocm4JkaAEWAEGAFG4N8jEGonmZD9+z57SwlCFe0tG8lCMwKMACPACDACn0Qg1E4yIfsk0H89W6ii/XW8uP2MACPACDACfwuBUDvJhOxv6cfTWhuqaE+rmAtiBBgBRoARYATeAIFQO8mE7A069RVFDFW0V2zDsEwPXIsCRXHBvRpOzSkYAUaAEWAEGAGFQKidZEKmkOPfIAR8ilbmEaLI8y/eoAyq5UmJy9wt0+KIe28VJXLRlgWO/Ql7S+GbjAAjwAgwAn8PAZ+d9CHBhMyHDF/vRcCnaLfTBpsN/VthJshMjNlaXTvh1lvqN91UhCyeYS1kq+XZl3j0VsmErBcevskIMAKMACPgRcBnJ30ZmJD5kOHrvQgMK5oiMzE22rDY/bAQo1XL/Qm7ZYJJlGB3VWnbkSg10rbQhqbuxRaLaSzyT5IldpeR84gNIXOM0N1LHPIlkokc1YunC63crlwkwzKZCBni6QyrEkOk1QAAIABJREFUXUvqPi1fL9J8kxFgBBgBRuAdERi2k2armJCZePDZSASGFU2RGTcho2nNyWyFbLXB+aHS+glZVeT4oBG3yRxZvkIaR4jiNU79Q1yyNT2E7HFaIY6nmK1ybNYzxFRHM7VqyXU/YFHLsD0esVulmC7ltOeX5BuJOSdjBBgBRoAReB8Ehu2k2RYmZCYefDYSgWFFU2TGQ8iSHGUzwKXS+gjZHYcFjWAl2NSZbvuZGKVaj2FkipBFM6yaKcstzmL+tMKjkeOKfSrr2V0JCEuuciMJW7rRZKd0X5RvJOacjBFgBBgBRuB9EBi2k2ZbmJCZePDZSASGFU2RGTchi/V5TJv4EBWqFweIKcvqjEz4o2U43R94PB54nDM5dbnV5kN9sjeETF9s0Mp1L3ZYzWn6VN1X91QbFFG8Yj+TU6ZRPMUiP+JKZO6r8vnk5uuMACPACDACb4vAsJ00m8aEzMSDz0YiMKxoiswociMLVj5kQYRMTRU2hEkRpwiTEELWTEW2jbxsE0ns5lscLwX2cypbyazaoAgZ5buj3GeY1T5nUbJFeaunMj8rXysOHzECjAAjwAj8EgSG7aTZUCZkJh58NhKBYUVTZEaRG1nwECE7iPASFc5rSbqkU3+JnHzGojkOvmWaFY2cNXOPZivUCFmHkJXYiHIXkPXecBggZFVTxQU7Mb05wbYcIZ8pEZ8xAowAI8AI/HIEhu2kCQATMhMPPhuJwLCihRCyO47CRyzCZJ4hX6XSVyuKoFZZ3g4LeW0yx/ZY4LjPsJzvIVy9qgKZIFYT5EXDmNqWeAnZDfuZJH7JMtfqVSTygu2HHDGbbUv8n1hYMME8P+B83GAmFhasxMKCXvlaSfiIEWAEGAFG4I8gMGwnTSCYkJl48NlIBIYVLYSQAdVlV4e0iDGdZYJw0UpMRciACtdj3oScmCQplsqHCzRaJX27zKnQujFeQqbXO0Gy3OKwmWtTlsDtuMI0jhAnO1weFxyyeR0iY4IkXWJbqIixffKNBJWTMQKMACPACPwaBIbtpNlUJmQmHnw2EoFQRRtZ7OeTVWesicDJucfPl8M5GQFGgBFgBBiBJyAQaieZkD0B9L9YRKiifStG9wIbWv1ohNL41hq5cEaAEWAEGAFGoBeBUDvJhKwXTr7pQyBU0XzlPOX67YAsP+IyJkjsUyrkQhgBRoARYAQYgX4EQu0kE7J+PPmuB4FQRfMUw5cZAUaAEWAEGIFfiUConWRC9ivV4PsbFapo3y8R18AIMAKMACPACLwOAqF2kgnZ6/TdW0kSqmhv1TgWlhFgBBgBRoAR+CICoXaSCdkXAf+r2UMV7a/ixO1mBBgBRoAR+JsIhNpJJmR/U0++3OpQRftyhVwAI8AIMAKMACPwRgiE2kkmZG/Uua8kaqiivZLsLAsjwAgwAowAI/DdCITaSSZk390jv7T8UEX7pTBwsxgBRoARYAQYAScCoXaSCZkTRr44hECoog2Vx/cZAUaAEWAEGIHfhEConWRC9pt6/wfbEqpoPygaV8UIMAKMACPACPxzBELtJBOyf95l7ylAqKK9ZytZakaAEWAEGAFG4HMIhNpJJmSfw/nP5wpVtD8PGAPACDACjAAj8KcQCLWTTMj+lHo8r7Ghiva8mrkkRoARYAQYAUbg9REItZNMyF6/T19SwlBFe8lGsFCMACPACDACjMA3IRBqJ5mQfVNH/PZiQxXtt+PB7WMEGAFGgBFgBHQEQu0kEzIdPT4ejUCooo0u+BsTPso9NvsSj0/VccNps8Hp9qnMnElD4H5aI12fcNeufddhdStRlDdU31WBVu7jWqC4fk67tGJGHlZ4PB4/0q6RAn06WRXSjkeJ/WaP8qdg/nSrgJ/Vhy8I2puV9KwCqhvKosRt5IN0O22w+cLLcrxOhL+Xw2R74FoU+OxjHWonmZD1KiPf9CHgUzRhbNMUaZoimUSI4qk4pvN0V/qKe971e4F9thR1zlcbHMrW7N8PC0SLg5cIlDspt5C1bkNLHErkUYTcbsK9xCFfIk2Sup0JknSJ/FB663leYz9T0hW7NMZkff4kMR1T5x2ntQPLNAWpwFA/jKnBSBPa52WOKMphd6VRpvPE065ar8s8QtRVEBT7DEulT83vEtm++LyO3A9YRAscWvWG/uylaYJJFCGaKL2k/ljjpKWXTbyjPOyQLev+WmbYeXTX+Xw07Umx7hbuRNG82H2uvPVI5em02ywPI3CQeqjnuxc7rOaWzs5X2BUdwLRsn9EHyj4ec7NPLfkI+753arlr371aP7Xvt137DNxPWFtpRNHqWXHoG6orjvTuS1PMVzvoULmfBQ263kNTJ7wYiI86M60s9o7DIkI8tfCqPwI7slUPSTqdMrnKdyZ0XvTZSWdiAEzIfMjw9V4ExiiaUPweAtRbwWduPk5YxR/IasPwuOyxiGNkZ/lZF0YEKpwzMrBFLYnjwawKZHGMxf5ikpvHBftFjDgrXnAEo8L1uMO+eXtesP1wEM3P4N/kkS/Ehc4WmnsuQlZiZxmD1mikmMYRfGWh7vPVUQ5djupzZWQ0mT53KLFbHKXR7rzoAVx3CaLFHt2B1Rv2iwjJ7uqp2mHsp3H7QeEykHpJNyJskUjfrbtOWJXIkxizTdGOfFQ3FJsZ4iRHOXI0hAgGGUBvH3WMvSSL8tFyPFeqHa42uq6p9I7f+ynDRxwjjhc4eIB4nFaIP7IuWb2fkH3EWJ3GDscN6wOehjng0jcDgj49t+/5cFXpOvfvOC5iJHkh3n23wwJxsml0pl827XknndY+2iWp9+mE67rrWr8+2rL12wVX+QbKvSdj7KReABMyHQ0+Ho3AGEUTiv+DhIxerFG6h27irvsU0eokXhr9D57V9OqMLI4wSdRXVm1E9GEV9bKysorTvnuu9P/s2tdeOG6x+1+IQf0wYOw/1edP6puqyPERLXGs7bXQ93pESg1c3I9kqDKcb/r0YoXH7YwsiaHInBtH8+pl+9F+IHQMpJb2TmVHSPKD+DBIsrNzJI7Ki7Oz46OBPkZifGwvWqF9h/393clJz1b0AVl8j/6VOeIoRlZozFC0O8ZUEHhthKeppML9csR+k2ExjRGnazFKfisPyGZTTGcr5LsDztoclDTQ6sOrKUgcFM5RTzONOhujD8/D/HsJWTNKmUzkaLKtb5ctPqIM9bcufXpgn0YNeZWY6i9LhZL5634XeHRCyBBhtteZtUwr39NqBLhfH23Z3DIoOT2yqNsDv2PspF4EEzIdDT4ejcAYRROKbxCy+kHZHbFbTBFHcxxu8po+1SMeEH1KifxGVjMkSYIkmSE7XB1GBPiUcXa2+I5zliDSvvgAx4P5rBEyQRDW2B0yzOpp3sWuxO1CRoReiDGmix0uul2iKZYZ4THFNFkiP7aYCPzmOxz3K628S42ZhjeNAKTUD4p4qhcajaLlWAq8pRFrp25cfWiDWKcZPUJm59fP+8v6VJ8PTeUoNqWLYR/TCBSNjmrDLvaLXmVRBKGdtlwi2+xxvPRNh6nc6rfGoR6Ng20gUeF22iGj52oyR35Uo7Z3FNsFppMpFtlGm4Lrx9VlpET7jClQ9bEif0dPWRLRijNInuV4rkSTHzitYvHMRzPtI6vTboVP+3s/77A/FrgIhyerndUd17KADn1VbpDEM2wK3dewwq3YYBYn2IwZKhylD5YsrcjiyIW5uKaNIOkjx+K4T1f7Pjzsey5cq4pYn5OQ0YeG7f5B+hFvJAnzPQtWkz3uC26duGwTxEmCj1gngq607eiycJtROuuZsnTh3srpKr+9O3Q0xk7qZTAh09Hg49EIjFE08VA6CFkUJcjOZIwqVPUIiJ+QXbBNIsy2NaF4FMg/rK9mJbVzynLSfGH3P3iykOpWYDufYDLfoXxoQ+u1T07XRegJPmTipRdBjWTcjytBkuLZVjov3/Zi+qkZTblskUQJ8qIemqmNgZpaEe2MYsy2cgHD45whjhaQtlwahQbv+qtTbxeRHH2K51Hkoj45olHnN/pQdYD6lWk6PhyjfMhsJ1355a2mnVUNza+zzz87Td2U6j94XHHezjGJJljuWxJMGUwj1BqFjhG1p2fHLHC47jHTRuO6hIz8rktcjJE4vRkV7tfScE4OHa0x26eXHXIsdeMjL+sPBGnwzBGOCpftDPFHjuJxw4Gm/9Wz4CIOevUOsk1T3l1dVB8fdWbyBd3pvn5LZLsDNBdUvZb2eLQ+yCyhmI95Z7XCWEeKTFmXxal9T+CqRh5roj3b4aLSWbi75NL1Qz92Va+uiRHIeqSt9RXrzkbcTmsk5DN5e6DM6WN5jeOVvlD7CZOQIzuLBTCPxw2X8oazNerpaouSb6j8Np37aIyd1HMyIdPR4OPRCIxRNPEwuAhZpk+TWASBXF7J+V6NkIlpC/2LCKCH2DudYjl4Hy+t/0fvg1cV2NBo08znyNv/4I8GzpVQvPQ0B+0OSdIxqn3blkfDb41e9Gq61sCP6jPK08uy71FiSYBMfB84LiNEot/q/EYfuholr7kwd11rS7BxLpBHMeoP7zaZfmT1uX8hxxdJ0m2PWTzFIncbaqHvOrPVZfzSMZGSCMlGkRjVb63OtMbMHLXqkkGNiDzktKnbhyzDuX10hPTPaJ/wNYpXaN2y7P6+47SaIIoX2AuDS99tVxzWKSakcxYxGIa1wq0sUDqXB/4DfQjEvP9ZGWi9RU67xNQ15WuV+biiKK542Ktbyw3iWF8YI98L6qNxlK4It5AJJpMISzXvLzVNW0BVodymiJsPQEpAo75zTMTIqa0/D5wz/RmYY5VvsNlssNkfxYprJmRWH/Pp+yPwJUJmGC2LIAhb0xIySS4mSOxRhb6heg+8Qy83GqEXf+JrOW9XXc1XyPcnXG73ZqrUZQC7Lzx6MWgG0COXnBZojatJoCiTjpF+3BYocZIvSP1Ytkc6eEvYrfwGWaPU8gVnO2iLF6wg11b+VgTnkQtz17U2s/WCvR2xnGrYtAk7R52l8tUDt+uTw17ohECNHpAk4rpuoKR4Lj3RSdLwNN8Np3WCON3B8OjS5eggIS+MMooBK/4avyJ6FoVvkfVcDjyTt8MSkzi1pgCt/ib+dbs6fd5Eq2xi4Gl7e1nqq63P7f0vHun9MEIfZG2fXWVJ7gXWKNaY0VVR6TgcjD423rlUt/4c0sxFO2VflTkSjWgP614lRrri1Qk38sU0Fl1YOnG/GiO7Zo/ZI+rqrj9cxb084qgNfTrfR7Ty8nbFjRZhuFbXq2oGfsfYSb0IHiHT0eDj0QiMUbTWiKtiXca8e80gFAX5MOhf1Kos36//xSOM49AL7H7EIk6QHa+4K4JGMXi25iqibu3+ertprSvvOEJmkGqrPdqp6Mv5HpfHA4/7VcQyKvZd/5M2y2dfgN18PqMwSg9agcwjnwG+7pGqUV0zh+dsWF+q6xHrVE7X6f6DokBdDlWDRVZ87VfJv/Srk4+hgu4l9usE0WTZjno1ebr9pm7diy2WKflJ1iMeFFpmvsJ+IAjZEAkmQqxzx9fQB9Xqnl9Xn/ckN28N65uZ3jpz1X07yYUSaYLpdIGdpqT9uvfAZb8QbhHSjaJCuUkRk36IGQ23TjzIp3aeGvogQgyddSd/Jbe7DLpryybeUR0/PRpZ2+J885ejaur7HWMn9fxMyHQ0+Hg0AmMUTSi+a8rSMuZlHqNx3H0U2KRxO2UJ6UOmlleTgI9rzxf0wKq8oQY6v5ZEpqEHs/+F9zivMYk8S+iDCBkgnJAdPmRqusAgtCS7eJmq0BZSzsaHTKx4i7Bul0tBrAzUvliVD5l0brbyOwClkRB9FChdZnLKYLPB/ljCJGS6nx4ZXuk/0q5u1aYfdCtq13vdIY30FVg37GeR0Cv7de3vY7tQx7nAsh6lUCvQAJDDcRTNsNeX+KrR3s7Lvm2Td4TsfsRykmJ9UM75liwuA2ld8wfA/OJUHYkSQMiqcovV1hdzzf1cVUVW+5BZ7Rb+lDMYC+2sJOL0XmC3mpt6mKZY5udOCJKf0YcnYG71r6vZ/mv97yeVT7yzlRO8MUJGOjtitL8uyCY9qnzxSz6v6Rp1pJr6VoXrIUMu5rIdOkHuAtGi2+9en2JZhus9Ihz9LRtkyGecOGQx7vefjLGTeglMyHQ0+Hg0AmMUbSwhg/jSmgjH29kiw2G31ggZMTC5ynI6TWQA1mUOsSbAKe24F48zK13URsgaF5pnjJDdC2wXFLbAsRQ8kJAJMdUqSzGC4FhlqY/WiBe5h5ChwmW3wDSeiFACOzEvpq2ypPINv7phQubFtr7xJQPoKpxeyjR9sl4jEUSygvBVmq2xnlGcONP5/kv160axJiXHc4YkmmF32CJVzuff1VbVfl2Ovmvq3pd+bdLcEkqDeJMBHxqB7sjhNnhfJWTldoJorfuqUsVX7Gbkr2Subv1Jfeg033shgMClXV+wMaOEsu9MktVLpLyydm98rRyHTnySkLlft64Azt02yCsOWXxJHdfH2Ek9GxMyHQ0+Ho1AqKKNLvjLCYdeZOYLyFmd04fs3OPHQKXIer0jHiDH7AnWrTezs+q/cPFLBtAGiHw8JhPMd3IVLq3GSmlVHX3N09DY/Yx8NkGqVunSJUrT+frXSEbfSFwnyOkcs5RCJkgjL+pfHJpRmMG6gglMDYCTkHWjrZvt7BpuG86fP/cbvM9OWYo2eEfIjlBrBVRbB/voifqg6nzVX78PmXw+/O83s0X+0VkznfvMrROhU5Z9waZ7dzgwhHLLYiTpOQm1k0zIesDkW34EQhXNX9JfuCN9JBRp+Ast7m1j71YlvTnH3awonMpv//s9e1n+9p7i9oUj0FmgE17ES+QItZNMyF6i295PiFBFe78WssSMACPACDACjMDnEQi1k7+MkClfB8e+ZJ/H9J/kpCCPRVGg0Lb4+CeCeCoNVTRPMXyZEWAEGAFGgBH4lQiE2slfR8goZkjURCV/Xh+LPRGHAlQ+r7o6OGq7FcUTi35KUaGKpip9qv+QKpR/GQFGgBFgBBiBF0Mg1E4yIRvZgWLVCBOyBi2/oslVeJ2tSmrnZSZkDYR8wAgwAowAI/CLEfDbSXejv0zIxEoYEY+H9gubYbWT++eB9vjarTCbUkypCNFkhuwkVyKJuFNRhDZ6Mm0kSyNbk2aLFCp3UeedJEsj6Jy7KXRVroiIog8sVrSUP0JEW53Ue/qpfP6yZSwUJTPVm5/+B/+1INn0f2pfQFWi+/d2zps2xNMFci2AnSR4c+yOOyxr/CazrAnnIIhLVI+QPY5YivrVhrwi8BE+6P7i6I9q7RbrKVf9itYfdoIJ2VPg50IYAUaAEWAEXhwBv510C/41QlbHN4omc2yPR+xWKabLmiCIDZAnSCgoZL5Eok0lihgzdL4+1fvx0X51RJ4k4ahoKwU6n8yR5SuxjD2K19r+Z+7GtISMyOGizUvkr44901u22DdRLpnfHw/I51Ok2/+g2GdYfBAhizFb075Ye7nps08MCp1FGzSrNmzWmBE5jOIm7IEkZDVhzHKsRDDUCMlORpY0CBnqvQSjGHkh14+pKdT5wQ572SPUE2/5FY0J2RNh5qIYAUaAEWAE3hQBv510N+hrhIw2GCXSkW5QOtaZVw91sd4QOYokIamjgzcbSF+2crQnK1CpffuipNnz7LafiRGq4RhOaoSMdoWXDVZ5I0EU66CWnrIpQrkYCVOkssFM5RvY5LhJLzdoJgKW1RHQqzMFO20jhytCNlOhvcUWQRGiesTLJGSS4FH+WOyyfMdRjNqNiFjdyPTcA7+ijSBkKnL5Z2MwPbcpXBojwAgwAowAI/B0BPx20l3V1wgZrtjP6ilJmhrMzaB7j8sR+TKVU4dERpqRKjVFmYAGhG6HeTt61JA1Win5wIP2wDtnIu9k64hybrSrJWRNMGYRTZtGoog0npEJOTxlP87IknpqckLTr2q7j0BCRtHeRT3a1KYaTYzkSJ8iZGrkTm5F4idkULJ/bHFRx7N9E4DSgOEHTvyKNoKQGdsp/YCwXAUjwAgwAowAI/DDCPjtpFuQLxIyKvSOcp9hNqmJTLIVo2Vqyo4iZu+LK06ZTsjaEZ/F4SL9x+KavDTEpS6vJnJE5r5EyJIdrmPKJt+3be1/RiNSxsjayBGyPkJWT70GEzJUKDIivwl2OzkyqaY33V37vVf9iuYjZBTIUm5pEzEh+97O4dIZAUaAEWAE/jkCfjvpFu3LhKxSs5K4YJcSiZpgW6optRhqL6kiNwmZ2vA4Wq2xpq1OGuf0Ernwt5o3045u0V1XuyNk110qRtdi4a82VLYW4ftxwlqQwSWOdzVCpvYDdNWtX1NTltHglOXoETIAyvcujmtiZm1krEvw3cd+RZNY6assk0Ruu7HclrgcFmBC9t29w+UzAowAI8AI/GsE/HbSLdmXCJl0kJ9gnh9wPm6k43q8wumhRnMixLM1Ntkck3qkqyEgja+YJGqzfeucLjYGpvRisUCB4z7Dcr7HMP9QhIwc81fI1WKCOMW2dnLrK5vuRXGK1e6I834tFyIkW9B+y5fthyR26QrZauf0mdMhViOEUTzDunHqb/cyHBoho/zC5yxZyT35ROGKUEaIarn0On/yOFTRlGydVZZVgXwSYZKT/yD/MQKMACPACDACvwOBUDv5JUKGxwWHbI5ETFdOkKRLbOtNdnE7IZtNhG/YdJbhsJNO7S0hAxqHe5qGM9hWhesxb8NBJCmWln+au7tqQjbLsM1mggRSKI5M7DKscvSUfS+wVT5v5Hg+z3BQO9FWJbaiPbSCcw7li69Kdf3qYS9ECI1O2It29aftQ4amvglWdbgQqkMRw4lw7nfV+jPXQhVNSdUhZOSHuEyxHAOoKoR/GQFGgBFgBBiBF0cg1E5+jZD9AzBo01Hh6N/5/dnxFb8cj28c6VHhL9p4bf+gC0SVoYqm5Lyf1kh5daWCg38ZAUaAEWAEfikCoXbyzQhZOyUppvM0h//v2C7JryOtT1lXjpGO//7CnXce5R6b9UyEGYnnh3+2ulIJF6poKh//MgKMACPACDACfwGBUDv5ZoTsL3Shq40qjhsF2t1CzQq7Uv7UtVBF+ym5uB5GgBFgBBgBRuAVEAi1k0zIXqHX3lCGUEUb00QxDfyNM89ilHFfb+01RiAjzQPlfoN9+TCu/tTJ41qguIbU3crb9dt7ntTUZ9/YZZ8UVIZYQXVDWZS4jRTwdtpgY/ibBlRfkSvFyIoCin1G0ut+gek0QbobiuP4jNpCy7jjtE4xnaTIzyH6HVrPF9IH9G3wO+ZRYj9i5xclfXUrUZS3b3zmaPZn2gRlV/X+89+APvjnsmoChNpJJmQaeHw4HoE+RRMrSFU0/lSGvEjTHfrNQT0NrOKkjBfFTHkvsM+WSNMU89UGh1Lun0qJhoiJ8G9r5LXllvK1+6/W1d5LHPIl0iQRdaZpIha35Ify63uMiqDGucBNYOrDprpgN5cLaMg/T65XbuW1263aSXu96uFJ9PNxtlu6EPjEMjuGzkakL3c1jgp/+1fTo/sJa6u/hNwKNxF3cIFDqwJAdcWR+kvox84Ybe7FmMLOXHZYTCeYpglmuyN2aYzJ+iy2fxMYR7Kvuu3+zisS045eNlVesP1ow+80l3/0YPjZFsHBm9BHmnAUF3KfYzWXejBf5difr/WWe1o6lNhZukB93P5bQ1sbpWccPDafH389pHtm2rpooY+06E2XJ4XQVYeO9n3kOMv3tmDE82blFaGVfizg+JDutsJ1nq/qikM2Q5JMkSTaYsI2y0sc9dlJl4BMyFyo8LVBBPoUTRi24OCvwy/tQaEeJ6zij2YT+8dlj0Xcbl8V9jIDxH6hjb9eS3AaOaoCWRxjsb+YBuJxwX4RIxZbgTWp/QfipRxjWr+wxaplYjmKWBCVoTh+HuZDK2/jxQG3R4k8ibA80khDK6+73e19KZh9XovbMSZTxJEiOY4Xfie9NEKS4DnS26hobbZv6XiIew5jJq6rMjr3KT5ijCQvRH+JEDhJu+1bH8ZAvbtIdq5HJ2i19g772n+gYzA6wj/nglhlbejBkFEbgflzROspReqWT38powjzY78zbgcs4gTrwwX3evCxul9wWCdS33tqNG519MC4C/WBQh9TFJ5pUsdNVB+R7ufHTb6caZU+mtXKs45s/f3VKf+rz5shk1w0Jt8fxo1vOhnS3bZa8/mqUOYfIp4lfXzK8FvqndTmeYWjPjvpko8JmQsVvjaIQJ+i/StCJl7qqRmvTpCqldzEvvMy620lbQvWHUEyRiL6XrR99+x6rZdyQwy0Mpprdl7IEZC8kDfa9rYEy93u9r7MaZ/XFWkyiCuGrA7jYaevi5E/jvTGfcE80exxO3TPkAUod/UIREKjhTlK6z7EnrkZ6u1lAQq5kkZYneRUmR9jEqSfVJgGwxb8eeddGYeMmmyjweGeJ87Ikvqxo0IEfvVzKguVBLjZ69eoST6bqt+MW64TWw9cacQ1t366nx/6SIoRxRkKbaZapFWzA/IrxPiw6lTdkc0tg8rXkeWrz5sqmH7pgzb6TEB2vZCQ4yHdbcsS7VYj0PXWgWqfaPUcpy8YOqnPTrata4+YkLVY8FEAAn2KJoyG/bUryr6jPORY0khQMsXEiBFnvbSrC/YrGpJOQJH+l/mp8QUiP43VjK4nSCjGXR0r7nmErAL53cRqO69a9sMigkHInjpC1n7hNUZXe9k21+w+Ei/RNq8YRaL9TntGyNSIgD5FSVM77bk2vaPJIGE4YDE4Quabuus3NqJ8uz69vfa9jjGjz+WqNYDW/fuxu0sE4RrXMf28GOOOU1bvyTuhqWnCx9RXw2AImW84U2Bqob8JlltzX9w024p9ftNkAopRuGt8E33PiC1DirWYg5OYzvMd8gWNXkaYzDItmLSU09Bb8ltqnq0ZssO1HvWTZS3znZwipOnZWYZjG7MbuBfY1XmBwKeOAAAgAElEQVTJL82ODymfzSmmot1L5Cfyd6qxWm9FvTQKFU8X2F1aJiPwM1hjv650+0qmb0e3zOlB2We6MrmO3XV2SBBlFaPx9A6KoJNGZ9pmGp76px0Jd09ZumVQ0nbKt58JlVD89pdlJKWTcoOY9nxubsj8692hjSm62KG8UfxRFefT6sdiV7+bSQeWyI9Kt6jQCtdDhhnpFY1CLueY0f7Wmk8Bxe5c1s+MvoBNtFsRsuseaZRqsUDrRW+ZGr1uGvDPD/rspEs4JmQuVPjaIAJ9iiZelk5C9kB5LBpiVZ0zxBHt7EDVmQZOTM2ku3p3hjsul9oR6LJFEs2wrV/mjyLHh/pKdU5ZTpDVn7Cdl5mrlRTseJ1ikq5xvN2Ew7HyQyHCor88RPZn+JBZxKExNuJlK31PmmlMW2Yrr5zWk3mUvP3tdhhsvQ77hW/UJ1/YyggKA2On18uqfciM9Mb9z4yQtQZO9NNsh4uSwZDVPcXUYD0wLWzrp31uGAwxvRkjXh1rP8Ib9gsy3MRsaj2f5Y3/mtw9ZAkx04zxz4iETvZBlKxxFB8mV+xoZLcJHF1iY/iQXbAlErG9SBL2KJB/xPUzIsv6WB0gv3FoaihBJAg+1Sbzqilf4IYDTc2rkS3xbCbIi9o5/36BfGzrNsczbAXxfOBM+/JqPmP0LogWey2cT+gImZTd4HS2bg2d3w6YRxHkSIvmKzYlWQ+tT2h1wXYW44OmvsW0aozZVi4W6n3WhD7Osb/IWJr3a4nrRf/AIQH729EpX+m6s22yrN7nTcvX/WCR+aMkw5lev/cjVrStYdOPUq+bfrT7v8ZGjWRS+fSRe1AE/37EUiNk9EEdxysc61f9bb9AVPuzGc+XaLP2EaqeXb2PtHb9y8M+O+mSiwmZCxW+NohAn6IJI6eG7Wu/KDV6rwquHjdcjjnSSO0PahKyx3GJKF6Il5fKQ79imiDTv4QK5NEHtrS/Ff1ZTv3HS20c6FbvPpo3HJfqq87yCZMFozNCJmv8+v8WcWhIgvayba7ZtYnh+/ZrURg2MW3bEi293Wp0TJFMc2TMHFVwEixDVofx0GS2RR0yNiJ9M5ogZWlH7ZRsmlN/twJ55XFFUVzxsFewiREAffSuxqi2AF6MRammfjbEqmYAhsEQGLV9QtmF07ogLnY5NHBwRha1U6eqWUPPiEwn+0D/UBDtEMapxI6mb5MciiMRYY8jfdoWoH2GP8QD1C0LYjRCPl9CtyJFHGspxTQwtbUepXAaxW6bDbyoKCKGCflvaf173WPm8yGbma4Jo3RLAev5ve5niCcTxA0BlQn15wf3E1YT2nt5X5NWWidywDqdiH2LjbSU/brHXHfmX2bYbDbYbHY4nIsfIWRjSWpH9poctrrV1491/y+Phj+t/LCmvpJT51LPVAfo+ibLNqYdBUGWH+yGvjAh+1+FIP8yAgKBQULmfDGrIespFtkOh4OfkFEl92KLxTQW0xtyS676hSCmjZSBlr824XN1U/eFY6Wi6S76Eyu7NsiWqo4lst0B5+sdKrLBeGKjTf9Z1TWnBskh0lk78GvkprnWZFIHD5zW9dd6Jb9YE2FcJVb0Mh1styrK9avJIG4bsgYQsorCYzjSu+psrrVtaC45DhrfMd3wiWN9AQJlpBGeGIv6E70qcyRi711ZqB9jul/rXmPdzPOuwbBG7kgesUOFma+uGXkzUhD2jCgi0hrNWn+a508aQrVlnZSzu+JPhsTQDaSUDKK/5UeT0cb6dnvf1a4mkfiY0Z367bLE6Mxa/9Cq835ylaUYUbbeE73viHoBwfZCo34RkrxsQksIWRs8K9yu+rJd1Ub56wt74Q1XUV1xPp4bcqf6s1Gzuvjqccf19ug+y/bzqcT5xPPmGyFrdavbx20/du+RKO19h24ZhE/e11d9y49G+f5sy5Ek1zllqRxpFQYv8NtnJ13i8QiZCxW+NohAn6K1X+hmMWJJNfkfNUPWNFzvHiHTc8opnRloxoe+5iM1RaInao7li6F9iTQ35GqqoW2bqhIbMtpbPX7VA9djhsTwKWvLlUf+eu2UnXNH+AZhILWXbS9ZaPY9jTFdqV0cWnlMg0LDjAGhJYQMugHXSY6DYNnpyR+EyMh8hePNkb4Dhn6hbYN+dfSxQR7rXGKP3akIXTG1/Jh6MQ4hZOLLXhu1NQSWbdLJSUNqCloxRtP4Ic9I19CZz5+FYZEjatwEDMGa6TLj2REjatLRe9QImTVCImvottkwsMpwP9Gg9vel1e77GVkSIdnUJOx2xHISI83PYpqy8/yI7HcUWxnuRoWzIF/X+WqPxh1Qq8Zdhmi45pNJGWR/qmnGZiSbRtYOJa72KP8znzePD1mrD339+NURshsOczVSqwFXHxr6Inx39VAu8qNDugR08/7LK3120iUXEzIXKnxtEIE+RTMNQluUcLqP1s0qt9txhQ8PIbudD42PDa47pGpa0vZTwANX44vVMkBt9eOONBJkZhgqt//+47zGJIqbFX1m2Z4zTZYgAyOKa+XxGgNXtVqdrtvmtVCCFZq+bYNZr3kmsLFGQxpDJhzwzfS+s36MpSwtkTLPDYOhfMgoFImq7H6tfSfrfEvlX1b7adXhN/qfkdroGSNJQ4Ssxrxehdv1A6MB4WvtHyXTphsVPLn2EVO+XuJjhUaPZNiQxoesvl+VGySR5kP2KHGUzkfjRsiaUSjNf6sz6qlGretfz7BXf1+qTqHfCkWeYr5Tba7v3QtsVzuQJ0T3+alQZPWotF4UTU1rfk/6LVGG5cYhdVT/wNFzuI+7srjTyavd542mzuPYs5Kys8rS1i1T56kOXe87/V/7kKnR2cuW/BHb6fNHuTWc+qUPmfYxAhoVlLMWej3UZ/6wF1cRHzBKaGHTv//rs5Mu6ZiQuVDha4MI9CmaeBk2L1etKArmt0pF1PDZbIHVLhdOnWp4/rqfYxIn2F1oupJWesmVlFNajdmsBAPalVxpvQJTfsnKmuRLo/2q0+ofc/hdI2T0gl98eGOJOUXTyNF4A6NKanEIeolrdaqS/L/dF74/Ld3pT++aBm6JlW6IzWngcGzcUvaXYxsj65x8hSYxElJe8VevspxORdDgZL7CXixEqfPN5mK1cTKdIlnmOCvmNvCMyOC0MSbTFKmoyzaa9pRliU0cwRh8qldZiuj9FNCY6hezcLKsj9kCszQFyTZb7aEthtRWWdKz111lSW4Gy1RbZXkkf0wLK8uQE1xyukyR1BrCsT+DI76t7siVqWMLlum6z88nCZnrnegaxe0RrytLT2LH8/Y455jPc7g3RbDjkNm6NaIf1SpLCpDdWWVZPxMTOeK+3Mr3v/6ulqss5Qh2ksyx2teLT+znyxsYluIDZpg1K8H78Pn+e3120lU7EzIXKnxtEIFQRRss8GkJ5FYsbkNOL2bTmDur9fiQaUH/Hdlkvf4XPo02TLCuY145CnBcqrcAErOMZIDbBemOxN5LPr8WZ4bbCZuNivbvTKFd7CdYWsL6UI58fLIZ3eJUqSr+mGc0xd8nZpFf2jrJLKrnrGvUehI/4VaFy26OCY0gDgJvG+AnVD9YhNo6aYqVWl43mOdnE7hJUOCU5Wmt7RrQEkT5nhrxTqqb7JbFh0f48/azkfp9cn/tuvANTdQK/a+V9dXcoXaSCdlXEf+j+UMV7Y/CVDe7QrmhaZH6a+9vg/HHW//ThCwE7n9ByELk+0dp33Qfxc+hRfr5gntZjm0M+f/N1locvrEZvyddqJ1kQvY9/fDrSw1VtF8PCDeQERiFABOyUTBxIkbgFyAQaieZkP2CTv8XTQhVtH8hI9VJG/WqUBX/Soan1vvdX+vkQzedaVGwnyo9F8YIMAKMwJ9BINROMiH7M6rx3IaGKtozaxd+FGobjd6C7SmYB67nQ73FjBnpubeYb79py+mv0Gw7ObC2W1FNZ8px3J9/zJ0bBcgcuzH6mAI5DSPACDACfxCBUDvJhOwPKskzmhyqaM+oU5VhkhJ11fVrE50HrkWBYrdE9CKrcKTUtpyutshreturcotEi8IultyrbaT8RQzfedCWJlZE9uFcnIIRYAQYAUZAQyDUTjIh08Djw/EIhCra+JKHU+qkpD+1h+iI0A6/YYTMar0WVd26E3gqgzSqPegCM3NyRoARYAQYAQChdpIJGavNpxDwK5p0Wp5vDtguKOhhhMksw+l6Q7FdgPYmjCYzZCcVeIkcva445ksR10jGPtq1QWFJunuB7TKRS/fTGRazD0T6lGUdV4niIiWJHrPsWYRMlrPeHZDNJoiiGNPFDuXtgkM2wySKxPZOOy1gk4ijNiN51P6Y12YrFgpseD1kmE0nmCYp0uXcCJBITZbxeGQctmS5bfDoJaPavoOf6lQtE8Xkajeo1m7wISPACDACjMAoBPx20p2dCZkbF746gIBf0epVZM3G4LR/YIQoSrA+Eil5oMg/EMUbyKhaD5xWMeImqjndT9BEWu5EB3+IEBItIZPlz7Z1SAnapPgjRlZQhOfnErIoyWQAzfsRKyKW8QxbsU+K3EMyUhHN7d0E6ojVasSJgmDGsRaR+k5ThBFUgEQZsXoFFZZJj/7dR8jI9yuK9c2zBzqx57aoR7WnJx3fYgQYAUaAEXAj4LeT7vRMyNy48NUBBPyK1l3WLyKg61Gq9SlD16iO8GGSe5W59s8zSInYay9rtmMisWm/yw+xwfZzCZkiTN2NpvUtRL66p5vEL91f2x4QeyOuQDFljba3KWgTROQfEWZ6Pv1+4LEkZId6S53AzJycEWAEGAFGgKcsWQd+BoGnETKdnDWit0TKRUD0a/JY3/y6joItopK35TRF04GzTiOFdWKX0yWdrUzde1RYe98uSwiEvBkhk/dj2hrHiDwvo3m35egi0n58H+2oon7rk8eiHh4h+yR6nI0RYAQYAfYhYx34IQSeRshGjZDJ0SHVNIOUFDmiyLyv0j17yvJnRsikQ70c4Wtboo6MttcXbwdrClQl/sIv+5B9ATzOyggwAowAO/WzDvwUAk8jZLjjuHD5kG1QkhvY4yT8tRZ7tQjghuNKd+qXPmRJXuBRN/5xvdZTba7RKPcI2eO8xiSKofy8TBztcrqjYDpRqsoNkihBXtQS1T5ki9op7LJNEH3kULcf5dZw6pc+ZJqPGe643ggMfaRNSijIGNUlwNKlJmIXI54fQMhVRY5JNEEufOuA22GOOJ7jIG8in0SY5EW98IBXWepI8jEjwAgwAp9BwG8n3aWxD5kbF746gIBf0bpkpdeHjOrRVlmmSYLZylxlWV32WM2mmNA03jzDJkudqyyn0wSUP13m0vkeD5zzFPFkhdNd7NAtpwGTeqWkmBLcycUFtJJz8YEod23gHUbIqEnNKsuUVkoukYsFDQrUG860qnQip1qX29xw6qdUcpXlFFORf47Vvl60cN1jPomR7C54nNZiFWsUT83pzfUJd9H2Oeb5WRLV6x7LdNlE4H+cc8znOc6CM16xX6ZYKv8zjkOmOop/GQFGgBH4NAJ+O+kukgmZGxe+OoBAqKINFPcCt284LCZYk+f8H//jSP1/XAG4+YwAI/AUBELtJBOyp8D+9woJVbTXRqgSoTTmu3oU6rWF/V7peC/L78WXS2cEGIE/g0ConWRC9mdU47kNDVW059bOpTECjAAjwAgwAq+NQKidZEL22v35stKFKtrLNoQFYwQYAUaAEWAEvgGBUDvJhOwbOuEvFBmqaH8BE24jI8AIMAKMACOgEAi1k0zIFHL8G4RAqKIFFc6JGQFGgBFgBBiBN0cg1E4yIXvzDv9X4ocq2r+Sk+tlBBgBRoARYAT+BQKhdpIJ2b/opV9QZ6ii/YImcxMYAUaAEWAEGIHRCITaSSZko6HlhDoCoYqm5+VjRoARYAQYAUbgtyMQaieZkP12jfim9oUq2jeJwcUyAowAI8AIMAIviUConWRC9pLd+PpChSra67eIJWQEGAFGgBFgBJ6HQKidZEL2POz/VEmhivanwOHGMgKMACPACPx5BELtJBOyP68ynwMgVNE+VwvnYgQYAUaAEWAE3hOBUDvJhOw9+/mfSx2qaP9cYBaAEWAEGAFGgBH4QQRC7SQTsh/snN9UVaii/aa2c1sYAUaAEWAEGIEhBELtJBOyIUT5vhOBUEVzFsIXGQFGgBFgBBiBX4pAqJ1kQvZLFeG7mxWqaN8tD5fPCDACjAAjwAi8EgKhdpIJ2Sv13hvJEqpob9Q0FpURYAQYAUaAEfgyAqF2kgnZlyH/mwWEKtrfRIlbzQgwAowAI/BXEQi1k0zI/qqmfLHdoYr2xeo4OyPACDACjAAj8FYIhNpJJmRv1b2vI2yoor2O5CwJI8AIMAKMACPw/QiE2snRhIwK5n+MAesA6wDrAOsA6wDrAOvAOB0IoX2jCVlIoZz29yNADyP/MQKMACPACDACjIAbgVA7yYTMjSNfHUAgVNEGiuPbjAAjwAgwAozAr0Ig1E4yIftV3f9zjQlVtJ+TjGtiBBgBRoARYAT+PQKhdpIJ2b/vs7eUIFTR3rKRLDQjwAgwAowAI/BJBELtJBOyTwL917OFKtpfx4vbzwgwAowAI/C3EAi1k0zI/pZ+PK21oYr2tIq5IEaAEWAEGAFG4A0QCLWTTMjeoFNfUcRQRXvFNgzL9MC1KFAUF9yr4dScghFgBBgBRoARUAiE2kkmZAo5/g1CwKdoZR4hijz/4g3KoFqelLjM3TItjrj3VlEiF21Z4NifsLcUvskIMAKMACPw9xDw2UkfEkzIfMjw9V4EfIp2O22w2dC/FWaCzMSYrdW1E269pX7TTUXI4hnWQrZann2JR2+VTMh64eGbjAAjwAgwAl4EfHbSl4EJmQ8Zvt6LwLCiKTITY6MNi90PCzFatdyfsFsmmEQJdleVth2JUiNtC21o6l5ssZjGIv8kWWJ3GTmP2BAyxwjdvcQhXyKZyFG9eLrQyu3KRTIsk4mQIZ7OsNq1pO7T8vUizTcZAUaAEWAE3hGBYTtptooJmYkHn41EYFjRFJlxEzKa1pzMVshWG5wfKq2fkFVFjg8acZvMkeUrpHGEKF7j1D/EJVvTQ8gepxXieIrZKsdmPUNMdTRTq5Zc9wMWtQzb4xG7VYrpUk57fkm+kZhzMkaAEWAEGIH3QWDYTpptYUJm4sFnIxEYVjRFZjyELMlRNgNcKq2PkN1xWNAIVoJNnem2n4lRqvUYRqYI2f+zd63czsJK9EehUHVVVThUXVVVXVUVDtU/UIWrqsJV1aFQvaqq1/QqFG7fNQkJCYRHzus7jzlrnVUeeUx2BmYzmSTBGns9ZHnCTYyf1qi0HA+cY1lP9iAgOnKVR0nY4qMhO6V7p3wzMedkjAAjwAgwAj8HgWk7abeFCZmNB5/NRGBa0RSZcROy0BzH7BIfokLN5AAxZFnfkIh4tATXV4WqqlDdEjl0eTLGQ4dk14TMnGzQyvUqMuw3NHyq7qt7qg2KKD5wXssh0yBcYpvmeBCZe698Q3LzdUaAEWAEGIEfi8C0nbSbxoTMxoPPZiIwrWiKzChyIwtWMWRehEwNFWrCpIhTgIUPIdNDkW0j76dIErvNCfm9wHlDZSuZVRsUIaN8L5TnBOsm5iyITiifzVDmW+VrxeEjRoARYAQYgV+CwLSdtBvKhMzGg89mIjCtaIrMKHIjC54iZBexvESN20GSLhnUXyKlmLFgg8vQNM2aPGd67NFuhfKQ9QhZiaModwtZ7xOXCUJW6yruyMTw5gKncoZ8tkR8xggwAowAI/DLEZi2kzYATMhsPPhsJgLTiuZDyF7IRYxYgMUmQbqPZaxWEEDNsnxetvLaYoNTXiA/J9htzhChXnWBRBCrBdJCM6a2JYOE7InzWhK/aJca9SoSecdpJT1m61OJ/4mJBQts0gtu+RFrMbFgLyYWjMrXSsJHjAAjwAgwAn8EgWk7aQPBhMzGg89mIjCtaD6EDKjvWbOkRYjlOhGEi2ZiKkIG1HjkqV5yYhHF2KkYLpC3SsZ22UOhTWMGCZlZ7wLR7oTLcWMMWQLPfI9lGCCMMtyrOy7JplkiY4Eo3uFUqBVjx+SbCSonYwQYAUaAEfg1CEzbSbupTMhsPPhsJgK+ijaz2Lcnq284EIGTY49vL4dzMgKMACPACDACH4CAr51kQvYBoP/FInwV7VMxehU40uxHaymNT62RC2cEGAFGgBFgBEYR8LWTTMhG4eSbQwj4KtpQOR9y/XlBkua4z1kk9kMq5EIYAUaAEWAEGIFxBHztJBOycTz57gACvoo2UAxfZgQYAUaAEWAEfiUCvnaSCdmvVIPPb5Svon2+RFwDI8AIMAKMACPwfRDwtZNMyL5P3/0oSXwV7Uc1joVlBBgBRoARYATeiYCvnWRC9k7A/2p2X0X7qzhxuxkBRoARYAT+JgK+dpIJ2d/Uk3e32lfR3l0hF8AIMAKMACPACPwgBHztJBOyH9S530lUX0X7TrKzLIwAI8AIMAKMwGcj4GsnmZB9do/80vJ9Fe2XwsDNYgQYAUaAEWAEnAj42kkmZE4Y+eIUAr6KNlUe32cEGAFGgBFgBH4TAr52kgnZb+r9L2yLr6J9oWhcFSPACDACjAAj8M8R8LWTTMj+eZf9TAF8Fe1ntpKlZgQYAUaAEWAE3oaAr51kQvY2nP98Ll9F+/OAMQCMACPACDACfwoBXzvJhOxPqcfHNdZX0T6uZi6JEWAEGAFGgBH4/gj42kkmZN+/T7+lhL6K9i0bwUIxAowAI8AIMAKfhICvnWRC9kkd8duL9VW0344Ht48RYAQYAUaAETAR8LWTTMhM9N56/LriEMeI1xnuby3jy/JVeBQFiuKOV/32Sn0V7e01vSfnE9fjEdfn28qoyjOO5xLV27J/Sa66qlCjxrMsUD7ndehPaNfHg1cii2Nk5dtLlljPye+vd8/rEcfZiiqf4cd3Vsw5MHEaRuCXI+BrJ5mQORWiRnmKEQYL7K8vZwrr4uuCbRAgCI8Yet/XtwRBEGCbzyjPKvyjT0qkJGuwxXtEGVK01/WAmMhpHCNaECZLfR6/xxoOwlDhnh+x31CdOyTnG1peItuaDnWKItKNvEpuJebrskWwvcDqsfqJW7bHJooQiXwRomiDfWbWOyhse2OgbiVDHB/Qqt4L14PEVN8XQqr2vXDZBtheTElfKC82LsqAO9vVSvYhR48sRrg44DZJGqTswWAneYhTZq2u6T5VOCqsPMqzktr5TT3XfUJ1Hq54wU4ri5HtDJedfhTpgTINYGFQV6iqIYLtKt8S1jrplW3d5RNGgBH4LASG7ORQfUzInMg0hCxcYj+HtcwgZMII/gFCZsIpDEGX0JgJPuD4fooQbDPcyfATWUoiBOszHqJsP8OFxxlxsMGl8aj1icsT53WAKLnZJA0vXe9sZ5zQmS0sDqXw6N1zES5KrNrXvV/hug8Rbi94CJteoUgjBNFJeHD77VIVf9xv/ciRnYsOTq7ypwnZ/bSyyYqrGLpWpgiCtP0osnBUWA1mFh40Qa6WofUhcRDMeCi/67rrWrePbDm6pGm8j7rly/NF1CF7zZdFt2y7Zj5jBBiBz0LgywnZqzhhFy2E9ydcrrHPmiGe6iE8CWt6uZFHZrFG0nzyl6m81n7RkwEhr80Cx8abQeVum7yLaIfsPvS12EL5OMeirsVJFqK8UsE2l4bhecGGZFmfIQ3nC8Vpi2Uo6452GVQ14iVmEqj6gUuybtJS+uafvGKKkAVbHFSacInt+QGSWpWl8wShbOerwGkXYUFlhUus9xnKSY8CtfeJW6rkDrHcprhpJiBfzsEmQ57tpJcqWGCtSURzX3nI7iesLEyA6rqXOKrOaCHWR3MUTbT7UwnZHadVgORm6EZ9QxKscBJjx13DpcV3HjzOa8sQx6R/pvyWge8UMXavk1ScjqXv3esYc+0JkrqTlp371RX7IMZZslJZu4HLuLF3CfuZ16YJ2WxC8S5C1rbRjc+ALom+CrA+6wdQE2VJkJSHrtNHbXXiqNtGtwwqU1eW7rlKJ3+7Zdt3+YwRYAQ+C4E5dtKs+30esuZlFCw2OOU5sn2M5a4hP/cTomCBaJfgmO4QCQIjh8nqIkFI54drE59TyGG0MEFRA3WRSpKw2CBJ94iJMIUHXKfISnmU5TZGVHxZi3qbcpthw4UgGjWKdCWJxyZBuqchygBhI5N4iWlCVqMUaUPExwLPIpXtWSW4FA9UCocgwGK9R5psJMkKpMwUH7JfSwIX7VIcRVxTY4iCBTanHHm2R7zczRhGlN4PIneLTYLjYS3brPGRL2e6Hy63LX5BhEwYaHVfDVlKUhNoz1CNWyIJakqdMfA3R9EEhiahQWOUshzZdolQ1Nk3yMIYmZ6OqsR5v0ZEw4TRGslFEl3g4whZ/ThjG4bWkHLfKH4nDxlFjilD3DH27yBkos3G0Lv8yFG6AsiPnBSF0Av6MNg1/RJhd2o9Yt0+rO9n7NdLLBfy400OZRNZafr/cBJ9TB8npLfyA+yFaxLLj6BFJIYjhbeqvhv6EGOXXuUw9QxCJklS1nrRHPrd73dKpLC2M5CHNowirMIE7XeBK2077CyG8pv2yCHO/pClWwZVd7f87rlKJ3+ZkNl48Bkj8FUIzLGTpizvI2SKAMVHlA7bXesYCGXkAxyIVYmvdTL6zfBC46UJkwJ1Y7SDIMKxKfRJnougyWtK3z0WhojKJQL2xGUTIAzJGyfJiCRooZRBkahIyS6NbdCQKJuQlTgKL5oaYpJlB0FDoFRZ+qWs7jeeMMNL1saQqTJj3c5uc5znYliNCKoyADVuB0n25Fe6fDkHwVp7SAqKT9HkUt1vjawirlK2Rq6GHDtlADBH0YYIGfVHcqN4JyIVU4TsjlMUYH26C28jqgLpKkTSkEXXkGW48RmyrHC/HBAvYhzyJ55GDFzPQ0ZgfFgMGcUdKn3qoCz0ybwnMbLjj9bI7j3n5yQAACAASURBVMoQdwgZ3jFkKfRLedeo3BBh2Hohy2OIYE8fUk0d++YDDE+ct62nyCJk9FyGoR7+f+V7hPTsiA+spv/DNU7CPVzhlpBnUpXb3DdizIS+xlkzLP3C/d7Ezs0gZEYxHdDbU/m8yOerjRVT3sg2HelKRH34rFCKIeEDcjFGrPqlTWseieciuaGqKE7siXtJxNaOIftwQqYIoAqQNAXiY0aAEfgUBObYSbPi9xEyPHBeN0OSNESX5k3MiqyiuudId80XrvBUqaB2NUQpidLzskEQNERJk7UE1xe9sCpUyrPVDEWaDbCPFRGKkJU0dBVgvd8jJjJyKZFviZisQaMLejgzueIlXoxkCOj+AlSNTciUF0kZyaaesEvIVFB/Y0TU0KSTkAE0RCY8hYEcdpQvc7tF3bNXvhXkqjVYgDR+yuPYJ1x2W/r3ZexUIA1tQ/gkOe7W3p7PUbRBQpbcJLkSxfUNrmXMyxRhoMinrJ8M5kqOSQLoBvW3Xpohr4ZqxTPfYbmMsEtzGYOmbjS/40axk9j3tBPUb3lNREC6GuoaK1jNtqtQno84W+PdFNSfYifKkpMO5gX1S10X5Fx84OyRJCuEgsk8cI6byQOCNCriJmUUz7Ega0onmw8u8eGmnh3QTWz1szHR/w7CXuU7BOEWZxE4aOCjh3LNWCqF4zhJ0qXQ+ydcYLEIsJOMsbll5leTftSHBSWh8IcNFiJ+0UxL9+jdYsq0wT49Ck/58Zyj+ApCNoeJahD4gBFgBD4CgTl20qznnYSMinqhPCdY04w6Il3RSXjLKA5JDAHGB5yLB66C7ChC1sYpbS93GT8WNh4b5W1qCJwoszlWsWFmA7rHyttzSGnYM8QhvwhiFiQpUvJyLSRp0iSmV4+LkAGq3ChthyzDzUXGoimZ9VDPPEIm0CvPSNbNME4Q4eRyNRqNHCNkcri1T7gmCRl5N8SQ6gFZRoSvIcdGvd3DOYo2SMgs4zBukGU/LZoZjYZRm/Wl3zWM3VaQh47+KjxuZxyTnZ6lt0syXG4PvJSX12Hs+yRKyjdLtI4oAisLl04COnXIoGb4URxkG5PpyGtcmiKawkNEpJnIMOl4kSJYnXA3h0KFNyrEUs9mbPqmmTUo+60hZHWJYxRgm9FQc40H6Zj2TI/3P71faAapNQOR3jpNjCkNb54Kc3ap0VDrcEoXKDGFJkQI91fR5lVI3i9VSCf/6wFFcFWK9ndo2QtFoNuU6uhV5sjLth3OPqKZl88HnnVHloHhVGoPqe8s3VKC8C8jwAh8GAJz7KRZ2bsJWa2HKu/IYuVhejXeqBDKxtjDZuormbwyBxzCAKEeoiglcdIxTaa408fK87VaUXwYecMab9ZqJeLSVIwYzcgS3ilFqjpFi5eYHuYDubOwDuSXs4h/2Z2g7YAPITOn1bXg4Z7ZExI64rSnashSe42GhizbIUm7LfJl3l32Qg4Ly+GpOfF6cxRN1OuIIbONa9/gWsacyECwH48f7A5VabTmrD1VozxGCLcnax2v6pEjieyYMl1sc/CRhu59ZUkMBwmZWEKBjPkdRVGiODuW8zAaJyZ1rE44n1bSEymI2BaXS0PMKK2YIKMmTxiZm0OrDyHjNaP1RhDeXXJunx0H4bLz9vXDrO152SJsvN7y+lCfd0mMWQodV7iftwjVhyGRsyMt3bFrPHHu/NX9gmQTI1IzHMVyKDuk7SwboyJ3GZSg2/8CA3PJGOXpTE+4PbvlyPN2liXFWhJB3mCfP3tlGwLxISPACHwiAnPspFn9uwiZDL5fYJNecMuPWIvgezKeNQqKA6EA3fUBRx3k3nrI9Jdv46EyZynJlyx5s2iyQIH8nGCn44JM8R3Hr1yuCUblNt4wEffSq+cp4mNIxsXmhLzIcU522DRT08QLUhOyF/Idkc0tTjdaVFX+39WCVzMImZq5SG1KkgT5g2KhqO4Ul1uOoxj6DbGfnLnQBvULbFVQ/0JNepAvZ5Nw2W1Rw68h1idj0VPVBuqzZtjJga6+NEfRRL2ThIyMUdguVVEVOMakO2r5AhlDRp5JNaejejzs5RQGCZkWd+Sga9zapMIoWvK39+ioa0Stu9UNh8U8LCfLUgWLdjq8hY2XSi7PQI40w5NI92hizZGGyM7IZxAyOaRIHx/qg0qSIvLGBakM5ycCo5bW0E6kF3lvpLAWqRLPZLuciGpOk7LnAbPyook/PbTD3M/bpSV0jwyxnlVLJQ73p11v5+x5wTY+INeNofs1HpcEqXgmHeU+6SNtK0IgrNI6cY7tPVlGS5zafhLeVvX12mYYOHLIMpCSLnf1lN7bi2CBsUk7I8XxLUaAEZiJwBw7aRb1LkKG6o5LstFLK0Txrh0+eF6bobgQy3WCS3YwAsulCCpYXwXdt4LVeOSpXk6DXmAU5yPXVGpTuY8U4WhnTGoyFCgD0+SsH8hplpgYbiVDt0Oa20tVqCD8toxmaLYheCHNKlVkZmTIkpaqyPc0s5CIYoT09l/cLwk2zZIhoo3GLDV329RVc9kLmsnqWPZCLWuhXsiaXALPfC9mroWRubNA1ZDOca+QkmCOos0lZGh0hYLW11ulK4qQkfNCzrKkeK84ihBTe9vRHbI4BoFTEs79/SwPmVxSZaWJpSHPyNCjGoJUv9bw57va2dY/RTRBsaHC293G7qklZewPhmaW5XIp+iXa7HFu1o2xSRV90CztJUX0s9b3gNl5gfqeiSVwFrSoanbHq8iw35AXKMJyac66pTb6kZUWlakjR7lvJGQu3tUlTePSOGQZydAr+3HGLt7pST8jWfkWI8AIvAOBOXbSLP59hMws6YuOafsSOTup+6vHTj9BEumlCfdn5OcmGFd7/dqhwY+puB5oXyXiQT6mDlcpDZHVQzauNO01X0Vrc37C0RTBsViNq/6BGDIjpseVS3iiRsomT+9CL+3iKsHz2lQ7m/itqVKnCdlUCZ73aQmcVSqWtFE5qyt9oH30s0OlyyFLRWh7vzMxUnK2v24S5DtkSds39WRS10Z0qZVDtpF223AROzudPJvSU1cevsYIMALvR8DXTv4wQiZfimagf3v8GS/3pkPUMOg20166ir7aaXhvIAbtzV0pvCC2F063UcfZvbl0Z8bH9Yh0FwkP5iKlpUem/3wVbbrE35WiLo+IN+1Cw9+pdfWzFDP75vTzR8gtY9JMQmbvHPARdXxFGfP3svwKabgORoAR+O4I+NrJH0bI/hX8Ne6ZWuxVkSUaKpy7sv6/kntOvc3wFO0UkKitdqbz+SradImc4vciYAxt0h6nkVxqZF4Iwu9FhVvGCDACvxsBXzvJhMxLH9rhxK/yLniJ94WJfRVtjmhiOPoTga3KM45nYyLDHKF0GtdaX/rmNzkg/aStLp4oi1IH2HsLR7MyZ/aDN6bPK47Ha7N1madkFEt4PM/cXsyzbE7uRmB0k3M7i7cuePbnV3t27dbxGSPgj4CvnWRC5o8x55hYqV8EEfem7I9vV6Nn3c4NjBnqhVchZstSnM5mf8TFiAObip1qV2U343yU3ANLS7xKXNKdnGwgYoEiOTnkUtozQYfkVden4sNiJQctGXPFQcUdNb8i/EgF/YtJJsZCrKqO0d82RqoXBE6LnmZ7bKiuXdqsRi8Lc2Jalbg0a7r1JuMoGZ2ytNsLWXFWKu7rTe1yVjR4sbodsAjjZpuxwWTiRnfywXhqn7sDOBh9rmbUylIn0iv8fERo0tr9OxyfR/pnp20KEP3dnxks9NXRn2PDws7yB9vU6vNgEr7BCHwyAkzIPhlgLl4iMKZowqCPLBfhxrA/286dbuSq2KJnpTexr+5yf0q1+bjfC52WnosR6BhBByGrCyS0/+X5rpfkENJVd5xpyyGxFdiIvOatMaLSvecwZKIolW7oPiWyyJy5HVBrwGxCpmahykWQK9rH1Zj40ceUJsCE2Ir9RmWsWLvGoJjyOzIjto+xVf5Yu0ws33NMhD6bN6NbyOaaRdvUL2LnvJ8D0UliKRD3unJ9jNTHjDv9OBjtR4jUhXZJDvkBYOFvFOW67ro2OgO615+tDhpV6cNe+WNk79Nm22px+IARmERgzE66MrOHzIUKX5tEYEzR/hUhEwYwVvtYyiYIUmVu5zPbQNK2YLTRdest662GrwiQC62xe77pu2V1DJled0wsoZKi7Nx3VSevmQawPbYImbk6v8gk1wVTW1d1jaRYmJlW9VeVVjl2gbHFUrctKp347ZMNq/zZ7bIK/bQTIdsIIbNk95Kij0Ob3XXPda3NMe+o7X8z/VAbxNqBnf1uRVrlGVczRsf6u9efbhmUPD1ZxspmQqZg499/iMCYnXSJxYTMhQpfm0RgTNGGCZmxv2K0xILWkLqqlTilUdGr+Nd3nPdrEQBOq47v0quOiaJYlf1arkMV0Rp3TXT4xxGyGg9r1XaCw2H0vpOHjHZ9UAaqZ+iGutM0gO2xRcjEPpTGmnBikw1a6V9u/t01kmKLsaRdxFWtaabXL1MyOkXqY2yVP9IuSQbUXrKNd9NYVkPu4JFCLWv7vNE6h40OmbtuCPmM4d7nTcxAXi4Wzb6zcvhND9EFB5zOe7l1HO3nm93FLOV7tpbrG5oEZUSnbTj6OLT3Xfdc19oc847a/jfTW/irG8ITTdjRZvIPdXVgyDJrlvmgNRjbrbbcQ5ZuGVQFPVlGdWm8LFUm/zICn4nAmJ101cuEzIUKX5tEYEzRhglZhTIvNLEiIxnqbZGkUVGETBj2OIN83b9wvzcrwdKaVsEap2YBUhpCW6kvdeeQ5QJJISPUey90VytpseNDjIVYtf2J62HEQ0b5PyyGbGRx267hEcSkNW4i3mqd4a7SjRAXtbG3uQfl4XBAHLfDlzYh68tl4mgeExxWXoGv7FftaVRePBf2GIiFUjFQY+0SW4opTxzVKbcBU8PVYreOxlNKxD0M98gblXrSVlLrs5xoYBGyZpcItZG90L12uyjR9qDd8aIS+twuv9N9DgZ1uofFGMFy3RvATcecGfGHvbqaC2IrrACxIFhGrNgyRGB6les7TusQK9o5g3Y3CNv2d3XBqkr03UZsQ0XrSL4eJR532mTeIL8TXq1e+UrfrYrUiSRkaghWOezUXf5lBL4CgTE76aqfCZkLFb42icCYoglDpDwDZtC5UWpdPXHPU8R6gUtpaBQhq/IdgnDb7CPYZhRDJYnpgSmQmlvndIL687vacGkg6FgX/RSryS8j2q2hExMm0rgMoc78/oNOUD8Nj2oSIzCcYVSrB4rigWps9togqWk9ChapEmvwmUaTSFeI8FhKVC72vpjmELEERS44rIiR9uLNRkzObBaUelB2KkzWI3bWEMOseyTJCqGYJCKXdpExVrIfJfFohBBkpNkv1SRkzfqD7TwTiZHavUMSMsN7KORrF2ztErIhne5D0SFYgsTagfF2UH+/BN8rj/MaIXkBzeFm7Q29yAkqryv2tB3Y9qzXY6wfFxziBah/e4TpcZYTQRQx1Ft4Zbjcii8hZG3f+SLC6RmB9yMwZiddpTMhc6HC1yYRGFO0riFqC5N7A66XS2yTTGxWPUTIKM+rOIktc8LlttmSqyFti6i32vmcL+CewWgFk0dqs/fqgdv5iGSnvGM7JNkFt8dLLwfRBkOrNLHcjsqIOZMzBQ+4mts8det0ns8jfzp2TBk8/UvDQzaJ0tUMkpoBQtbsWRmlpVwwWHhFIiinUQ9TsZ3QWs9SFF4j5cEkIUa9GlrK9sCS1yBnbQp9VKQBAiLrZSoXbKaN6YlgWHFwsp022aU+bPrJJGR4Id+GWCU3QUhetwSr7oQGM4ZsgpCRoH2d1uIPH4xhZk3SaHXRmqXa6MXgM6L7lPb3DaD7ukvIUOP5GFbmoWUvBperqB+45TdN7oa2vaqrFx7Pqk/4hnCpK9oaHj67GQyDz3cYgbcjMGYnXaUyIXOhwtcmERhTtCFCVhc0RLnFRYeN0ZCF8ig0ZMvxSSs3m1+LTZyF0R3d/HyYzAgSpYa/hlpYlzjSLMGTuY5XhUeeWLML+9mH6+2nnbryzrIsEtOpS9zrDHfGMbKyNWCi/8x+qO/Itksso1juHanj/txeR0k6lojipdzH1lwBdsiIGmJaRFN4h2x5h7xDIoZwdcL5tIKYdCCI2FYQf0HMRB1PXDaBvG/UqQ8tQgYI3YvXwtNDy6jkRlt8PWS6DjTlBlKnzevO4xmYmfl6/Wfe7B6/bkiiANFREe4cu0WIOG1IaMcDKrPTPq1yqZeoIXsU57nZu9eI65F2JUNPT6UOqmFGTSrJs3Yp8ejKInAxPIcUE0jybPbIn60+q+r4lxH4agTG7KRLFiZkLlT42iQCY4o2RMiEwQwOuDWLjtIm56sBQva8XVCoj/FHhlgNS4o4nghpoYYiKzysr/Z3kplB4zdV7vh9sb5VEEIHt48iPF6WyipwdngL3+aZaw2Yj0EfNLZKyO7vIL7dhPKchtKCIBRDYu4UxlVh4BdYLMJmn0eJI+ERpCqcH5AxZMaHAcgD0yilkE95F0scwwBGVqOyhoyOeMjE8K2OgwQGddoq1XHiidn8/qtRpDE2WWex5FeB0z4TM2X7/VujSJoYso6oViyecU+U0QlhkDo64sk18qvDvizqjuu31Wd193nZIAw37QehusG/jMAnITBmJ11VMiFzocLXJhEYU7QhQob6gcuevCwR1ust9lmKnSZkNDNug0UYIbvT0E6G/UbOglvSbEyxrpUUS86ylB4bOQNTfs3Lu/PIzGADP8tDRkZuu4KKkTPrdw1/au+AHoY0htWazPMNr1nb0HFrwHzK9TOSHkOWNGx8pLimHc75CeswwuFsei1d7Wi2AQsSTfoFKQqCHhGWsyyXWMakYxvsz3J2pBxSVYSsxv0UIwgML0y8wT4r5BAmeWxGCBmt+ZasFwiXEeLDFf8Z0em2NUZAvdX3juHIgTFIn/5r63Uf9fv3jYTMnBigqup5yNQN929fFnc6ebXVZ5WquqXYbFLc1LecusG/jMAnITBmJ11VMiFzocLXJhHwVbTJAj8sQScgumfUZsR0DcSQGYv+O6SV9Q4NpwEUn7PA4fpx1sAa2uu1M8awLA7xAahV0kW5A8a+m3MobqibTp9Pbp1E678tsKDJFeZuB9Qn2R4UfxifmuE1XegnHYhZu6YnjeYOkLc2RDOn4ZMqfl+xn0vISDbPIcsrzeJ1EEpxbcbz2MDhS8gyMRT/Piw5NyPwHgR87SQTsveg/Yfz+iraH4aKqA7KIw0NNV6Yvw3Gz2n9/YSVGfOoYr/CZkbmz2nJ2yX12Mvy7ZVwTkbgdyLgayeZkP1OPfj0Vvkq2qcLxBUwAh+OQI07Lfq6pAkKsdivdL3P2tjGD6+PC2QEGIHfhICvnWRC9pt6/wvb4qtoXygaV8UIMAKMACPACPxzBHztJBOyf95lP1MAX0X7ma1kqRkBRoARYAQYgbch4GsnmZC9Dec/n8tX0f48YAwAI8AIMAKMwJ9CwNdOMiH7U+rxcY31VbSPq5lLYgQYAUaAEWAEvj8CvnaSCdn379NvKaGvon3LRrBQjAAjwAgwAozAJyHgayeZkH1SR/z2Yn0V7bfjwe1jBBgBRoARYARMBHztJBMyEz0+no2Ar6LNLpgTMgKMACPACDACvwABXzvJhOwXdPq/aIKvov0LGblORoARYAQYAUbgXyHgayeZkP2rnvrh9foq2g9vLovPCDACjAAjwAh4IeBrJ5mQecHLiRUCvoqm8vEvI8AIMAKMACPwFxDwtZNMyP6CVnxCG30V7RNE4CIZAUaAEWAEGIFvi4CvnWRC9m278nsL5qto37s1LB0jwAgwAowAI/CxCPjaSSZkH4v/nynNV9H+DDDcUEaAEWAEGAFGAICvnWRCxmrzJgR8Fe1NlXAmRoARYAQYAUbghyLgayeZkP3Qjv7XYvsq2r+Wl+tnBBgBRoARYAS+EgFfO8mE7Ct75xfV5atov6jp3BRGgBFgBBgBRmASAV87yYRsElJO4ELAV9FcZfA1RoARYAQYAUbgtyLgayeZkP1WTfjkdvkq2ieLw8UzAowAI8AIMALfCgFfOzmbkFHB/M8YsA6wDrAOsA6wDrAOsA7M0wEfhjibkPkUyml/PwL0MPIfI8AIMAKMACPACLgR8LWTTMjcOPLVCQR8FW2iOL7NCDACjAAjwAj8KgR87SQTsl/V/V/XGF9F+zrJuCZGgBFgBBgBRuDfI+BrJ5mQ/fs++5ES+Craj2wkC80IMAKMACPACLwRAV87yYTsjUD/9Wy+ivbX8eL2MwKMACPACPwtBHztJBOyv6UfH9ZaX0X7sIq5IEaAEWAEGAFG4Acg4GsnmZD9gE79jiIOK9oLl22AIAiQlv9O8tdlK2QIZgnxL2QukQaE0xaX1wycylS2Z3vBnOQzSvRI4imrR8mclBFgBBiB34rAsJ10t5gJmRsXvjqBwLCifTy5qW+JICPbfD4V+WpC9sp3WAQh4lOJegI7eduT5DAhm4UqJ2IEGAFG4LsgMGwn3RIyIXPjwlcnEBhTtDIlz88Gl+dEITNvK3LlQ8jQEJjNTCHeK7MkZAusZxMyRVxTFHNweF2wJY9aOiv1nBI90njK6lEyJ2UEGAFG4LciMGYnXW1mQuZCha9NIjCmaI9zjCBIcFOuoleJS7pDtJBDmeFyi+ze3FREIzxCjnAq4x/iWAKSKMl8NAwaBPI68MQt3WIZymvLbYqbSQAfZ8RBgEQLMd6krsyKoGV5hu0yFB66xfqI4lki2y4RBgHC5R6Xp2yHIo0hCQ1AnSfnHMlapd/i/FCg1LglAYL4jMe4aPJufUMSBIjPTWqFW3JGnqyxIGzCJfaXBx55grXAeoH1sUDVlP8qL0h3kUwbhFhuM6hueJzXoo3h4SrSV/lOtjkpUMNT1jnt4TSMACPACPxyBMbspKvpTMhcqPC1SQTGFK0qzzger1D8qLruEYZLrPcpjoe1IDOBImCKWKhz2ITseT1iv5aELNqlOB6PuD4rXPcNSdokRpkHXBX7qEqcRdrJpogEXZk1EQyX2CYp9rGsjwjhcpsg2SwEYQkaAqMIWJeQBcFCtLubniqlth3PpSZM45I+cT0ecS6bBircggCL9R5psmmIVoBgscY+2SMWZDVCJjicxCxcrrFPjzisZXuUvKhLpCvCmdI/cSbMww0U//OTdbwlfJcRYAQYgb+AwJiddLWfCZkLFb42iYCfotWolGMID5xjZfiFK0kOxQ0QMhJEkSM9ZNl4v4JQeeFq3A6StK3PigZONmE0gapzrRhJ0QTVrxuPlpZBevaGCFmY3GRM2fOCjfBiKU/gaPXTNxUh0xg8cdkQBmHjFWy8WkEAjVtdtfFtSv4o0x46QZzJ8xdFWBHRE96xaVE4BSPACDACjEAfAT87CTAh62PIV2Yg4KtoryLDfqOGyyRxEKN7mlgoomJ7yEgURY4UsXjlzQzKba5nHCpCpDxWM5owmqRbp4pJC1SdHblV/crj1D1HJ/1o5XNu9sqbxg14ocj22ESNd69HEB/IBFmW3rGZ4XdzpOU0jAAjwAj8OQR87SQTsj+nIh/TYB9Fu58iGY+0OSG/Fzg3npzPIGQqBuq9rfx9hOyOU0REeIHNKce9OPc9dtUVezHMSem28JjU+l64OT8jwAgwAr8OAR87SY1nQvbrVOBrGjRf0UochZFX6221Q2sWIQvSJqhfeWlU8L7hIVMLdqnhNj1xYN6QZV0ZQ3YTMP0TQlZXqNqx3XEJfT1k5VHG7ql1zBxDqPfTShC23U56IEMeshzvA77LCDACjMAIAvPtpCyECdkImHxrGIH5itYEiAcBKCg/3ceSGOjZkoqwhYj3aRssr+8DFNskZlguNkiSBLkR1B+uD21Q/8II6u+I/lSzCDcXPdmgk8Q6/XJCVhdIBHFdIC10wJ0lk3XiS8ieZ6zFLNUIu9SYpKBi91R50Ql3K8DfqpVPGAFGgBFgBGYiMN9OygKZkM0ElpPZCPgoWn1XS0csEO1OuBw3xvIVwOvaLtMQ7TLkmX2flrjI93LpiGARIb3RTENz2Qsqt7PshS0uquuhmYU4byjuywkZ7siamZwqDq3TBPtUEShFqDqzUymx3YYa96xZJmQRYXe64EhDxyI/TQCgWZch1LptKgYuUB41u3Y+YwQYAUaAEZhAwMdOUlFMyCYA5dtuBHwVzV3K1159ZDGCUA2Nfm3ds2qrbzjQrEg1NDsrEydiBBgBRoAR+I4I+NpJJmTfsRd/gEy+ivZvm1TjcTkgCkJsv+vUwVeBI60NFqUoZ4xY/ls8uXZGgBFgBBiBKQR87SQTsilE+b4TAV9FcxbyZRdrFKc9smL+XphfJpqq6HlBkua4q4Vt1XX+ZQQYAUaAEfiRCPjaSSZkP7Kb/73Qvor27yVmCRgBRoARYAQYga9DwNdOMiH7ur75VTX5Ktqvajw3hhFgBBgBRoARmEDA104yIZsAlG+7EfBVNHcpfJURYAQYAUaAEfidCPjaSSZkv1MPPr1Vvor26QJxBYwAI8AIMAKMwDdCwNdOMiH7Rp33k0TxVbSf1DaWlRFgBBgBRoAReC8CvnaSCdl7Ef+j+X0V7Y/CxM1mBBgBRoAR+KMI+NpJJmR/VFHe22xfRXtvfZyfEWAEGAFGgBH4SQj42kkmZD+pd7+RrL6K9o1EZ1EYAUaAEWAEGIFPR8DXTjIh+/Qu+Z0V+Cra70SBW8UIMAKMACPACLgR8LWTTMjcOPLVCQR8FW2iOL7NCDACjAAjwAj8KgR87SQTsl/V/V/XGF9F+zrJuCZGgBFgBBgBRuDfI+BrJ5mQ/fs++5ES+Craj2wkC80IMAKMACPACLwRAV87yYTsjUD/9Wy+ivbX8eL2MwKMACPACPwtBHztJBOyv6UfH9ZaX0X7sIq5IEaAEWAEGAFG4Acg4GsnmZD9gE79jiL6KtqcNtRVhaqek/JfEb+6rQAAIABJREFUpnniejzi+nyjDM8rjscr3pr9jbUCqFCejziXVa+I12WLYHvBy7hTZjHirDSufOxhVZ5xPJfoSzNQT/0W3ahRCYWq8CgKPGZXNiCDdbnF04WflZRPHAh8Rp84qnFdIl2a/aIpkcUxfB4Fb902ZJydV7WhfqIsSjy//XvTaOQnHj6vRxxnv5xrPMsC5RR4GuMXrocYh6v5phxvjK+dZEI2jiffHUDAV9EGijEul0iDANvLfGU3Mn/wYYV7fsR+EyOOd0jON+OFJ+VMB7mKfIHHMeVt//VDXKYIghR29hrPW4b9JkIUyTxRFGGzz3CbelnolsuXhVmnOpZ1v3DZGviWWSvfMkQQLtvzrESZBgi6jXxdcTDapMrXv4erReqAF4psjw3l2aXIH63V8CUxTnl02wcONNb9PhPlLaK2zbpdERZBAKvp9R3ZZoEgCBEfFJlu8Zxsi4m1rkf28zI0+sTVDNGGLYYfC7e+iT6JSOaurpmVdHRGpA+waHRQ9mvW0VUjv8bXuGYeKtJgXtPH/T7RtxwHb+p/Rzl0qddfzv45QNrdMTndH2e98ntyvP+50HW8LtgGpn60etmr9ttceI/O1nhcEqwjenbpfblBcnlAvVlsPenod/PsteR6AKsupvp8IP0Irr52kgnZCJh8axgBX0UbLkndkS++70DI7qcIwTbDnTwq9RO3JEKwPuMhRB17Qau2GL/1DQkZ+KK55jBiz/MaQZTg1uGir1uCKFjj7OtOc9RB5MgiZIaIrkP7xdak0C8mV47utRrlMUK4vQhvYFWkiMIt8qaN2qCobD2jaBMjpzxarK0mlPtde5ydFPnt99lwef2099NKtqMqkUYBdjkpRotnry2qTZO/bRlDSR+kG0GI5KZMzlBKx3WnHjjSiUsvXJMVwjDUfTaUUl6vcUvCRja363Ecly7O8twmg62Xdri/HFIKPQ2xFAY4w4k+LhT5PlxROjzCVimWnnflNFO674232++5EO0Wsh+w36p2HHA6Nl5tS1aSbVqnzBb8m+O3E7LqekC4SlEolasKpKsQh6u8MK4nTX/l7Yel+UH0uh6aD7QlwkDpT4zD6diQXn9sfe0kE7J/o5E/vlZfRZtusHxY/j0hu+O0CmwDKEjVCqc7tcL9Eh5qX31LEAYLRMoz0vNajD3kY/eGagQEwQtinIlBGl4t8+Ujcr8KnHatFy/e7JEVkjE5X2y9l/+wDKiu2CsZRDIy4AFWEsS+l6JblMi/waUho055mjymATSPoQlJv8+Gy+umlfqgCPXjHCPYX1EZhs+qs9sOAPcsxsLUAaELG+zTBNtVgH1jTLpZq/KEdRjhcNgiDLcaCzudlLdHZJS+xSMervqFe37GMdliGZL374Ly9URJHojlEut9iuxy6w/1VndcDhHC9QlFQTIusDkVHe+owxNlCd7FuXtuJXZ7bO0k7VlHT7t9PdVfsPKPyeW+N1q+53Nhym4e6zosWQmCt70zWvC+4siN25yaTQxUevOaeazut7/demdipTGemb6tEL52kgmZAR4fzkdgSNHIm0DepNapI5VYGOJXiUu6E18h0XKB5ToxYrHkwzKLkI2WQ+FSJc77NZZLcmlH2KXXZsjxiVu6Q7RciuvrfYZ+SNUHErLXDUkUIDqW2qXekoQW6w/1kNXyi5Hc+co7JWvqv0zK0wJBolx3AIoEweIkhqjEi63xKmgXv34xtbIPHpVHhJ3hMmlEcmG4tUEZKqBMEYZHPVw29qI1yzKPW6y7L2JIA6+8Jpq8EDm1PXOSWBpDQkTyVifcDcNn1TnUHuf1vlxAjdejwPkQI1xskDUK+swPiAXxyXG3hrFdZTgrc1x84ZadkRd3+Xx0+rd+PVAW1NLmT5D7JRbLLdL83sb/VXfkKZG6BaKt8iT/AUImyFVDqI0Pn5hCADoxmQpCeD4Xpt6bx1rnOn1GhIzinHQYgaXbdF0NxWqJ2gPxAXNARoR8EQiv8zYr8bxfkKzlkP2SRg4aZ239vCHbb+T7fLFAtFP3nsIbHyY3+d6rS6QrGppXVkHq7PBHxLCMfh4y+c4LlzYe2dXtISMg6ge1dYlltBT26UJhFhrj/ju0Bc99NGQn3akBJmRDyPD1UQQGFe1+wsocZqty7ILGu1SVyItnQ06aIQ/hbaCq5EM6i5CNlnPHiUhQWjQG44X7nUxKheveHI6p8LgrWeymuoYsw40yNHMMYI1nccJmscAmK1GZw3E9DxnV/RExZFQOvQipjTle9BIkHA4XOfRqEAjdWoqN2qm4tQjReo9z87Y1X/46vX4x6SvDB9o71SbRRoS+40eHjaRuKG8aleCUpynaLMs8loSMvJMdktWKNH3UbbNol/R4Ko+jVWenRDE5omcUlYEwhkZU/F11RbJLcL49WsKjyqTg4ssRezJ86hrs4Z+IDGmHaGpCrfPIg75shjymzEo20tSJYPjaGFkdw0U9722s3vhzJfpftWuoQap9nT7r6o6Qq4mZFPGVRp+2REaRAps8mFVX+Q5BECDY5VZfjbbb87kwZTeP2zZQnxkfDAqDt/wK2QJEyU1+NOV7hEEgPKHiu+B5xpbifFXcweOGXMR1UGUPnNetBxxPim1bIS0qiGF3HfLxFsHMPDXu573w4M6JIRsP0+gSLPoYD7Fv2vei9tPHl9AnOYSpnnlTorHjQTs5kIkJ2QAwfHkcgWFFkx4mRazES0t4FMzyalRP+qqOjaBjD0Kmi+qXQ0OEgesF9TgjDkI7WFuX0z3oBvWbwzHjhqMujsIDR963ZvTPLtzxQrYTvPGsLnGKQwSRGV9RimD0SAwTdl8+bT3i5b45406zXIVX5IF8RlA/vZzsr09jeOyVd4KNiVSFCI9yOsOY0aqLFCsj3owkNY1RK7k8Mssyj90eMpvAtAZYkSTjl6yvGK5uhn+JkJB+xUTOWzytOrvCffH5GE6ToujZZK6UnrhNku7uc9Q9t2XwatccQmZ6sEafySG56MMvxP5yQRIS8WiZ6Kg+eD4XZrvNY12H2Vbzw88k1I5jk1hqpAUOBrkTZZsTXKTO9yb71BVejxLZLrA8g4KI0QSRMGrCPewQitFnL/abzajaUD0KFHo6tZTXfkfR853iJkLO2mdY5DexpAvqvFQTJzrpVaUjv8N20p2JCZkbF746gcCYook4G/HCq5DvjK+m+oFLQkOJWyTZBZe3ErKRcsSLqjNUJprSfdlMtG/49tALWuWo9fDkq7wgS5tZhnGMzT7F+XrH86Ve3g4jJzxoRsyZepk636CqTvn7uhvDS/at3rIXbQBrQ0B2CY7HI45HGsIaIGRWmVMvJ+mRjNJmuJa+mI0XszYoVplAfZcxSSpIV902jZG6pn7NssxjNyFTuYxf9eLVY3PGPfKsHkKsyONaP3HeBugSXKtOM2tz3MNa9an61X3roQ+nEnivAe7KOoqDTDzcD/3nwokLzbx8PvAkD641m7V7roSrxVI4w/WqdMav0Q5aSufW+bjoyTVIyOhZdslVoUjbiT7VdY8wXOPUDC/3yjdEU576uc+F2W7zWNdhtNWq5i0n3XekKHuEkL0KnLY0tLdHes57hIy8ZBvyIJLH3pLHoefqWRC/yjtpZTJOXH0ib5sYGRkGDtula2SCMQ8ZEdWpd16/mjE72U/NQ5YuTPjaDARGFe15xpq8VHczsLtGQbOymll3VIV4qWjyJB8y5VkbFmG8HOkh20FMhDMLER4yFZhv3hg4HnxJy5eJtqED2V/5FmGUIH+8NEGrnyVO29COKevmH6y3m3D4/FXQEhqGp4deckbA/nBO4w4ZTsUbjcvt4YyXEw2J0gs7irFcrpEY6wNpg6ILrFBmGyyCCAcjnbo99qIVZYUyLjBqlvAQy4aslQe2/wK31iuaMmrkfVQxNHs5a9QMnu63RUk9/TvWLpF7Sh/M2CXLqKn+HzNuU/FGVIbh9Rz1VPYxVv1ie0JoMsMJt2c3vTxv44rUUPoG+/w56iHtoWxgstmfkOjZiaSLxgxFlVFg7PgI2uxxud86xPGF635he6IpWIBm6C12YhbxpD54PBdCPxYyFlYNR0vdppngF7x6ujvv/aSabv0KHOZ6yJ5yiDIt9PtNyKo9j/KDbLU/YNvxIFp1dk967ekmoPOu7rRpXM9Tf2hePRvyVy9JxDFkLZB89LMQGCVkkA/rdrs1AvzpAQ0QHJpATzyR71dvGLKcKKcucbRiyCqUOcVEvJCL+CplUIHn7aoDVHvoTxnCXgbzwghZmSp39P4DmRiWpKBy919dJGJauHTJG2mqm5genhhDK+YMTNtoxhAv/zbAxyhIHY60USUZ+bWNVo3yFGGxObmHeUeJwEglGsv+C9x6cdMkkOPZMcFjpGyPIUs3MTGMwhi7120Yk2Xg3izjBjgJfLxDelNB2G35iiB09aU3GaLNMnDU75OBhOKy1V9kOItUzFw1hwrH8pv3bN0z77iOHXLWlSYhrhx+5dslzM2r0/X62CGvXcXwmdC1uYRMhqbEmVwMiCZSndbtkKWYXR7uQROIxdAlLVWhP/DGPGRzYuJkG1vy3j5L0+8ts/kz32Ea45npjSrG7aSRsDnkIcs+JnxlBgJTiqYCXtfGIlo0g2Uf01fqGuvtHllKQbFq4UoaWqCZZftmQcZhIcbLIdebXM6hnWWZN2uK0ZDpxpplWejhw0597zGEJIL2kKkFc2hJs/d6yGo88kR6H+0xAC28FyHTufoHXQM4OeymvTNjHpm2Hm1Q2kujR115RhOrm7oP+0bqTeWpcsVv+3KeasvUfavY7oluQ/fGjHNtSMbSljgtAhy665w9MqwD6fExcw/j1sfYzNc/9kvfq/dxxi7eyaVd+oWPXvHrDz85qWK/8m1R5+bV6Xp97C+vlsCLkAG0tuCGZiPGa6x3KU5JLL12Yqa3ueCxmmilZpu/Q0Yh7HD+np7oxrkO2mfYdVdf0xjPTK8zgpe9MLDgw09EYIqQfWLVX1P0VHzOmFejkdAZQ+aaPWe2SNRrDxOZt+syRRRlzSK15p32WHo81JBPjDiKEHkOWfq92Nq65x7N3iKmKfBN8lQPFAXNVuy/wH2GMabaNNWWaTI73N9tHNyUFI77o0H6bfpBD1neroCuUk/hNuOxaIrq94mqw/Ur6p1fuKsIfU2TGX1l7MB/CFD0tzEzdaz07r0pXVLp6eOuKJ+oNVlQd8a8T+RJmvfBpEr7nN8pGacC+ifyz9YTX4LVjTmbRsfXTrKHbBpTTuFAwFfRHEUMXho1YLMftsHif+6NZ47d+mCs3faJTZmMIfvEul1Ffzd5XDLyNUaAEWAEDAR87SQTMgM8PpyPgK+izS+ZUzICjAAjwAgwAj8fAV87yYTs5/f5P2mBr6K1Qvq5fa3ZcG0h4sg19OA7tKFd/52y+ZQRYAQYAUaAEXgPAr52kgnZe9D+w3nHFG0wzkTEVbjG7Suxf96OAsN3KXLarqL5G4sdchKyzppDVAytRdSWqEqWv64y7BTGWS9ew7jHh4wAI8AIMAKMgIHAmJ00kulDJmQaCj7wQWCWoomFAdsNomX5XUJWozy2+y7SzJ3IWKW9S8im4su66cfWrCF5uoRMnDfbqrRT+5tAWCZkPirCaRkBRoAR+NMIzLKTBkJMyAww+HA+AnMUTaw/E9C+kmq6M5XfIWRiaxpzwVa5un98luvbCILV7GGnF/CraJuOGBFty9FsAps3Syb104/P5nISMr24YQcPJmQdQPiUEWAEGAFGYAiBOXbSzMuEzESDj2cjMKpo9RPFaYNFSKv1FzjGoVj08yZWre8QMrEZeYrCqPl52SBoNh3ve7zkSv20lY0ahnzRJr/rM4iT9dMzITOg5UNGgBFgBBiBL0Jg1E46ZGBC5gCFL00jMKxotJL+Evbm2s1m3ekVVddD5lj40vRa9QnWvyZkIZZiEdTvsJ7PdD9xCkaAEWAEGIF/g8CwnXTLw4TMjQtfnUDAR9EoqL6qXniUBYrHHZetsYqziDOTW2yoKu+nFYJU+sz6hIz2yR4ZssxixNZaZewhU7jyLyPACDACjMDXIeBjJ0mq70/I1Eax62xw/76vg5drUggMKtrjjI3eRof2JVthmxxxPB6RXW59Qqb2vbw0QWBi240Qas9FQci6MWTNzEnn5te9BUTdhKwmgvis3EH9jhgyMVPzecGWNk0f2LZIYcO/jAAjwAgwAozAoJ0cgOYHEDIyggGC8IhyoBF8+esRmKVoRhC8OTtyGRoeMhL9eUO6pT0uYyyXa+zPdx0f5vSQdYc9jeb300tC1tuIdpfgeCnxuGzl/mtNGb1ZllEEMdtyd0J5Z0JmQM2HjAAjwAgwAiMIzLKTRn4mZAYYfDgfgVmKZhCytuROUH97w3nUJ1iUTJYRLmlvNvs/WgQI0vnU3YxXcwpgXuy154nLJkS4uYgJBWZSPmYEGAFGgBH42wjMspMGRO8kZNL7EGwy5NkW5PkIggXWxwLPMsN2GSIIQiz3FzzVlDgRAnTGfr1AQJ6vhe0RIdme1wRrMqyUdx1j1fWQ1Q9ckjUW4voS6+QKHkUyevULDmcpWo/AkGAfR8i2jrFDN4EbBuR9hKzCLd1gk95QDVfBdxgBRoARYAT+IAKz7KSBy8cQsiBAuNwiSfeIBSmjIcYltkmCjSBWAQ7XxmTdT4gEkYqxT1PsIiJeAaJMrjtVl2lDwOT9fUykzhyyfIqgcCJr8T5FspHETuU32saHn4jALEWrnyiL0iLjRMiuhxh6TbEJGd1bJ8kyut4xfW4F9Y9X4EfIrjjEPLtyHFG+ywgwAowAI0AIzLKTBlQfRMjWaNbxREFb1wQB1s2FxzkW5+GRhpFq3BL7Ph4ZYiJcQYKipiUTOveFl8UgZGWKkAjgNpdesbpAQiQwyiApndE6Pvw0BHwV7dME4YIZAUaAEWAEGIFviICvnfwgQrZF3owZiiGjIMC2uSA8EESgBCErcRQetBDiVADYDHsGEbKH436HkCmCt87uqMRyCnecN0TidlqGb9gvv04kX0X7dQBwgxgBRoARYAQYgREEfO3ktyNkqfCWGYStQ8gU4RPDmCKt9KgFIROyEb348Fu+iqYEELMtxSbj6sr4r3vIcjzPnLtibTQjrnEsj0/asXLee68qzzieyzfGq1Uoz0ecy8+Jdvs2GD1orbv5bfTGtLesypxerVGJNVoqPIoCHuLNKLztV6/hd6tkf7ne/lw+cT0ecW1WubHE4BMbAWfIh53ks868nmexNJVPKEers2+Rf/YzS88qPXf/EEdfO/nFhKwdslR7Feohy5CGLF/It5JgbdS6VGLh0HbIUnvcjK1z3tKpnOd9CAwr2kB8VxPX5TQa1R35cS/XL9vscczvmnT4BunPa5X0yromBfTz+6Tt5/a+8ipwTnZi9uhmf8SlbKerOLEzKjCXFtHxdHHWLBczMJniVeKS7hCr5T3iCFG8Q3opPSbKfCVG4/rl1pcXiqzRr12K/NEy8SlMDXjFobv8bqrOud6NQuJkTgIW5TXr7LV9RjOHIzFpyUyL+o5MxMyGiA/XZmZv26+jbVHrOXZmJcvHsi+XaEH9QE66EcfY7DMUrSo6tihr2+zWw1jI/MJAXXihOCfYdeSL4x2SczGuiwLfsfUBS2S9cpvZ2RHFIKcjSyp18qqJamZ5Ix+Yk/oyRvCdk6JanO2jIVztVPPOWp1S6d192rxbxuR8XnE8Kl3VpdmLg6vL1u/7n1n9PPTk67fPqvoDT4btpLuSLyZkAFRQfxBhp4P6Q8TZXUhYXfciRiwIB4L66xKpmAhAL6UzbrcLjvsNktv8r2I3FHzVB4FhResru/lS0g+JruyOUxRgS0PQdK26I9sGiE5SH0yD5RGrr0t3H8iX17cjZNUV+3CF5CotX3U/YxuGSG6SQPSxc7dOXRXD+3pJjn6/QMRfhtieWwIs8lZ3nLchwqTdL1SV6f71wdNdwtuv3nFatSESpq7JMmuUxwjhVi5NUhUporANsehhWmadpVRsYtQvv5VclBUuRf79bismNhGZyU5pY/T7RnO4vH5a2sFCtKOS78BdTk9M26+9trSi9Y/EHrIKh35dVG6+DRGlhXgun5ctwuiIsuGyw3KbVbnKdV0DHlmEYCv3ozVLAC0cTe+DZtKXfU+ePc5rMSNfPSeuNIPXNFkeTNHeaN5NYdji0t50HNU3Gd8cJhgyT6M4domEkHWByCSDpF9ilR83rg6pxCVTV+PsBBqZUms1Hq7lNGEycevKaVZqptPXW53Vl6wDv2dWYCg+ag7Yk0OnOT4dmzUme/JN1W8J866TYTvpLvbrCVln2YtQLARqDsPUuKslNEJa0uKIJG49ZKIZrwLZfi2X2aAX4GaPzPx8c7eVr34gAsOK1ld286XTMxrCMCRoOIeQsL4lCFYnsTODmffjxJcvr+9GyOhjJIjP1uQUQaqajdZ72I0C8sB5TbOf23XaegvyOl+WTaFj93r1+uDZy/yuC3VBs7J3ENxEbS7feJyEoSKSG8R60pGaWLRqCP8kpiL/BsphP6aPZlnmMTSWfaM5XF43rSSezY5iaPWifd6sOkdRrVGkKwS7vPFEy7qkQW68Hr3n8oFzHGDfzJYfltuo+HnGOgigR0PELbOudpjrlRPhS3B7VnpRaOqr6nlDEoU6JtkoXRxW5QnrMMLhsEUYbnU/2enMOtvnofVIKi+ynUtW8MDtkiHd0zJLC2xONzxfD9xOWyyXEXbJEef8jlfrdJXZ7hccohDrU4HitEa42ODksFGjOHaJhNYjh5yDnkdXWnR2KOnqWqtT7tziQWs9i105zUxOmSfK93xmTQzNY/089OSbqN+U/53Hw3bSXfA7CZm7UL76+xEYVrS+svceksaLIJa+6L34gdmETORd42zEo4iHkMicGoqLY0TLBZbrxIhbkS+gtxGyGo+clmuJEEVLLNf2x8Dzpu5FiOhDoYnZGrpuasrHEbIaj7M0UGqyjelJ0XV+mofs8zDSstMBbWUVhtgqtqQImTnOVx4Rdoak5ItaztLWL22rYOOEZnUbu4SYumykEodmWebxJCGbM2QpjJQxLEeGTny0tM+bVWdXOONceLss8tI1yAARpKCzhRi1XU7OwuiQpayKwlNCrLZbrCfqUqK97jnOR3PYcofkeEZ+N8ZKReIar0eB8yEWREc/Y/kBcUikKcfdXPjSk6woeWRVd+TZBbfyIUmXRTCIMN5RlE9NIsXQHr1vtinyeztqU91zsRtJuIiwVUsSuPTVrLxLJKy6zYR03O/Dbgrz3NaVbt5Wp8w81rEpS1dOI6F4pwVyr2Jz2LP3cWjkgeczaz6T5rFuY0++gbAH7XlsPxRMsd5yPGwn3aUxIXPjwlcnEBhWtP7D7HxIdPnSk9MOWZY4rdtlU0Re0+Oh89GB9BqsNSOrkO8CCO9HVSIv1ItSGoeg8TSpl9dbCJkYUjcMjBgCCyIIh4swmgEOjbuvetzlGmxD1622kOvYNWS50Pt66hdMN595Xt1xOcRYxAfkz6dY8015ApwvQUVcPzCG7FMxorZW5KHYCI/F7vzQxpBumbomYDENR4OTiaN53Nw2fhpS0XjT6EavfCO1WZZ5LAkZDTXZw59G1unDrlER7ZLDV6pfrTodJVaPG04Ug7bY4WzE0annweSxrrLMtpvH/apeuKUxwigVQ5ySAK5xFM9j3/jTuoRKR0d/VaxWdUWyS3C+PXSsqZaBArgvR+x35t7HdhyY2M2jQ4Ld4RAOwy1izvrDhrEuoIZzj91WwHF91eloNLqzVZtDl9vkElc17KjFaRNYR3b/9vvkslXedUlORPrmQ7rtIzOGLMRSEBqTzMj3MU3Ak8PrSoS+jVB3xK+jnaa85jGlN3XRPG5lpj2VjY8Zq7LPPRm2k+56mZC5ceGrEwgMK5rjJUYPavOG6D5MohoKHraC+lsjaz5gLpEorkZ/yVc5dsFKkiOdWH7F5imth6eCd+ULyJ+QyWEbNdwlq2heOskNdUMQo+TWWQxXEsf+dS1ke9AJ6je/sp3Y6ZxP5DvaD3SH1JgUoW8bsUbttY86MvH8ZIxoGIwWnU4vMOY76Ib09OWV9zaEL9NQe3rGMBXDoUa8GVXSK1/XbA8DWeVqA2MaPpsktEbOQU7o2aF4JGPoVXiRxfB2a9ysOg256PB5XovFu92TNUy5mozkpQjV80LXmnoal+swDjL+Z7HJYE7ofRUn7JIrKk9PTqcZH3I6LPuM4qsHisJBBClrL/7Q0ZeNF0YRplFZ3kDITFI91hpbV7r93+qUKsNOr642v1051W2KFw/3uFwShKsUhR7W7Zevsohfz2fWxNA81jKb8r2hjyzZPE+G7aS7ICZkblz46gQCvoqmitMPibow8VtNLWPwOCNuvn6qfKdjz9Bsr7WkHSSyCy4fQshM4tEKLl4CaniHPFRiW68F1skF2hExdL0tZvJoEru6eeORB+l8RLJTBmEnMLg9Xvrr3Rw+UGSAPC1mzJm8bn7xDolo4mIet+k/FCPrBasC5pVHwSQRVD8tNk1B2KX0TIhhzsajKbL0h+YoV32n2KRFu8NI0xTzhd+2Th6Z/WMejw1ZWmWY7bJu0EmF6yHEioLs6ybQXXjuWuNm1dnLbxNGsx0iX8+S02SbdjiYdlAh46o2XDHzO6oaueReYsOlj0ov6bfd2cNBZIe8VqfyTSRpRHhi5MaHnStll9wYaRx5nTjWFV6PJ6quPjjyi9JrirsbqdcQQR22ukJLstxEUH+rAq1O9dOrK81vXaOmZ6rrgaoKMflOLhAvn8FwfWpIer98u1S/Z9bE0DzWbeziaFf2qWe+dpIJ2ad2x+8tfErRxAtWDTEYMOiHxLiGgSn59CKe3iz8KYLXt5e72OVBBhDXKJJQeM5UeJmo99M9ZEajmhdSb6bi0HWddfhlNYSpzkoHdYkjGdKTuWVVhUeeWLMLrTziZLgDkAjIAAAgAElEQVTeftruFZOETXnIjLxDWAxdV1nNF6xppAQ57xIywa6Qbcl7GGO5XCMxFsHq62OFMqPh0AgHI52q2nzhq2vqV5QVLhFRfCEtj9Acb9bKO9s3mtZ6Xma7VKHmb03D+bRMQ7M/sLjX9lu/LWbmYUImJgi01rjN9LwiWS+xjCPQh0121y6OUU+hKGDQE+E7bNu2rxWsc2TqQOeWOB15v0x/dDgIoI41aj54rPdcv4+1SA45hT51hk/FUh/HC8pHh+iI/MZwKekZybLZI3+O1KsFaA9aArzB/pRgq2ZZLpeIDicctwHMEQSl2yZJFse7I4qyI+friv0i0DN0Za0yfGKxo9jNGX1KS7zMfGYVhuK5o60aFxTfG2Gzppm7F7x6z5XsU+WlbFH5+KMpO9mtkQlZFxE+n4XAlKJNGYdZlUwMEakyaDgm2G6xDVSAf7MF14GGEenviXy/Mr5sTQKhShn6tdOK2WCOGLIjrQdQ35Hr4cIat0MAEbc2dN1Z5YyXlTNfc9Hx0pd3psodv1/dDlgEoZ5lZ4vwxRiJF2wTs2KsI3U/RQhIBzz2ULP1tEZ5irDYnKw1t8y2jhEyM511rPukbzSt8qoS5+PZGuqzynGetP1mt6Wf2DSq7YeOXHYmWNuze/u57SuW3PatibM+BpTBlK1n9C0PmaN4ja/j3tSlnrF2ZWgmqXSJ2GbvWGhZtk/FclltMXTVVUvv2izZVK4+rjTkTjND03asUCWe+G11aiKhvO2Qsx4NpPMsvyPElJ6r5DpdT74+VirPR/9O2clufUzIuojw+SwEphRNPwyzShtONOvFL2LHApBRUR6x+nHBPiavyBrr7R5ZujMIWSWDjhd7NEt+DQuAblpjBiEtomrOsqQFbptFViP60tw1hn3ourPW972sPs1D9ipw2q4QuDwpX42R+YJtjHF+SxAFa2SXE2I9NOIE2Lroq6ez9NGqQXxVNLrXNwRvKs8qv9WXqbaY92W9OW5JhGCd4XKKsT6Zyw9ZlfRO3i53HwMq3JStV9nUhc8mZC+KTV2juxRafTsgWJw6i8q62yea4CunqedTGLiGLB9n7GKawDGZuZOg1anODfepl5xUhGf5nVrn6opO15NvpI86db33dMpOdstnQtZFhM9nITClaK1LXMUx2b9z3cVvf/HPasY3TDQwKUJ/nc+I6RqIIXMFwbcAyHrbWJ32jjx64rLtx1R1U33JeW8IaoN1TLP45PIIz+sBcbMQ7JQ8s7dhaQp6kz7qQPC+ISgz+7mwPCpTnqFO46ba0nsmN2vE62PjDZRDSuYSIp3irVNrqNW6M3XiHi7qyab1vcHHGhbs1OFLdMzss7bVGfaQ9de/nBriHFnzzJSLjntEopvAPHfjaqaYf+xJmMTzOOO9pAWYetfohM6DKT1XmepnKZck6eE41Uc+bVG1uX+n7GQ3FxOyLiJ8PgsBX0WbVei/SDQY66K2evkXQn23OmnmXIxNdrem7H83Kb9EnrGtbr5EAK6EEWAEfgoCvnaSCdlP6dlvJqevon0z8VkcRoARYAQYAUbgUxHwtZNMyD61O35v4b6K9iOQ8A2q1kNR463TS3cIb9zcIYsK5fnoCBp21zU8hOQafvAf3tBtcFfPVxkBRoARYAQ6CPjaSSZkHQD5dB4Cvoo2XOoDWRxicbj1V90eymQuJGvGmmz2OObtorLO7K8Cpx1Ni44R0+r0KvCeEndjDVzDmWYsy0DsSjfOSJ930g/GD4k6XESqQnlptpbZpcj1ImdjC5a6yunHMol9AytzH0EbPd0G+7LzTAfTOu/yRUaAEWAE/gYCvnaSCdnf0IsPb6Wvog0LQAGzGc6OjXfdeYwFMrsJaP2qVdhbzLNN9kAWBdjqrZZoBfMtgiiTG3p3CVmbURwpopGrQGxrGnsbKGpvzZLhlAZydmKHkOninxdsgnYDa3m9S6TkCuhhE6wutmwyVpG3CdP4xICsdBCyibZ3y2+3VjGC0huyqnDq7kCo28sHjAAjwAj8AQR87SQTsj+gFJ/RRF9F+zgZFDE54/5qF6okD8/rfsY2jCDWBHNV6CId5jXz2JG/RzSGCFYnryYzA+kftI5aQAspNqvJi/wdQia2zjG3hZJbNsmFcBsPWbPApJopWZUZdrQ0h9hgfYl1kjfLgkhCJtdKamYUTbRdt8ElW6e9PZw69/mUEWAEGIG/gICvnWRC9he04hPaOKxokkhsjhectrSpa4DFOsH18URx2oK25wkW5mrpMr1e30qQFto4OMWWVjsPaAuiq15fTDaFtl+5IEv32Kghy80eaXZB8ahGWuvpIROyGCtjU12HK65ODxlV+5JtXNJK7UtsTwXIS6TJTJeQ1YTJBgtaaPZe4EhDt5sTaIujurtWz/2EVZCiMFr3vGzkwrNmHep+XSAJV8aikC+x8brciP1jPGTmSt6qWoHCxb0dkZmGjxkBRoAR+O0IDNtJd8uZkLlx4asTCAwrWkOwwi3OdyJHzUrgtBWNiO+qUKQrBOGxWVDRRcgCRIdc7gP5yLAOQhxLEqgdFuyu19Q7H1ro7HlD2mzJMR1DNrFvXYdgVdc9guiEu8BOtnt/rSQhI++VNcRJuwkssd5nxqrwFe75EfuUNmGWuGjS06mLqjA9UZr0qX7714QsXEL0iRlzp2TjX0aAEWAE/gACw3bS3XgmZG5c+OoEAsOK1iFYynujNt+mcgW52OIigow66a17IrHY+FYTkwm53nW7u0ikkKXjIYtjbNXS1x2S1CVF6lz9Tm1MXFcVquqFR1mgeNxBcVq63SLOrN3cmdp5P9HK+dJnpuswABgbssziGBZn5SFLAzk+ZAQYAUbg/QgM20l32UzI3Ljw1QkEhhWtQ7A+gZANzk5shi8touFoh+lZsm5PkBIrLZ08rzge2+HUUQ8ZbTlkErjHuR1ujWlod4VtcsTxeER2ufUJGdQm6s3mULWcwJA0+9QJQtaJIRubOUnkz4zA680wFY2tUVVPPJ51O+wqrne8dxoYSl/jyUOWGhE+YAQYgb+LwLCddGPChMyNC1+dQGBY0T6fkA2L5oiNciT2ImS9bXqMWYWCAJrrinnGkCnZDCJobiFD8XbaQ0ZpjeHW5XKN/bldOd/lIXOTLCrIgZOQodmwW8XlxTE2+xSn29NJyMJli4VYRiSOsTuVuDMhUz3Lv4wAI/CHERi2k25QmJC5ceGrEwgMK9rnEzLTG9SLHesOxTnaIQiZinEyyEcsPFVqKNWRsXvJ9Hh17xnnmiwNpTcIWZttyAvVpjCPdB3mxQGSFccRFkEA5z7hZn7j2Fm+cd887BFe8uYtAizSwvbKmZn4mBFgBBiBX4bAsJ10N5QJmRsXvjqBwLCifREh82ETnbb0CIO67yRG6qbjd4hgdZJqMjOU3lnvRxEyF8F0eMg6MndPdRu6NxznfXwfOO9i7FTsnSMPX2IEGAFG4LchMGwn3S1lQubGha9OIOCraBPFed2eiiGLJ4LIzGHBvoetWZdrjkRDBGsobyfmTCfrTiYQN+TirmpNMZ124MC5ddLEcOsETFZNXoTsehDLg/DCsBaEfMIIMAJ/DAFfO8mE7I8pyEc111fRPqpeLocRYAQYAUaAEfgJCPjaSSZkP6FXv6GMvor2DZvAIjECjAAjwAgwAp+GgK+dZEL2aV3xuwv2VbTvjkY/7gkQQ6M+43qdRlblGcdzOX/T9E7+rziVy1/UeJYFyqe1EMZg9e9pV/WgNdbGdlMwq33iejzi2qz0Yd4ZOn6PbENl8nVADPN7LPI7/Oy4+tR1bRz1L+nnmtYFrIHqgaJ4fNFzLJeOGW99c7cqcT6eUboeJ0c4hW8fwhlKMUsyTtQg4GsnmZCx6rwJAV9Fe1MlMzONx4TFGIops/LRNk3mzMus7Cz14BBmIEZLcTgXyaOX3C3bYxPJPSZpxmMUbbDPbpjJh6QgA3W3MXFmLJxjs3EhpArud00geKEgOWkW6i5F/mjJmrNdBjz144JkvcQyisUWUuvkInddUGvSdSdkvApk+41Y2X+XNjs0iPKUfEbh+tBtxKdk09lnHFS3AxZhjOwxI/E3SWLptDWDOBbbllnLqFgyD++CQXGMfVzd6ZXuD8ccuvrUdY22olB6Qc+H3IZMidyXR92Rv15xpmUmd5XQeMlnR9fhIDf1I8ex0dn2maNlYo7Ws2JLNeOsU9dgOwho52Sgpo5OOXRVt8cUg0hn+2ibd8bLt1O6l9Lppfl7F3ztJBOyv6cjH9JiX0WbW6lYXNVc1X9uxoF0w4ZhIINx2Tvv44w42ODSeHT6L0C5uGuU3MQel21VL9ySCMH63Nmzs03ROxp7GffuuQgXlagMYfe+2sD9IuSpihRRuEXeROn322VIV11xEHtoqs92uVVWeKDtoIx9PVWWusQxCrEVoFHaCOE2b/BR8qnE5q/73qhsZvY5x68C58wkiHMy/ds0bydkJLepBza+fVzt+91W28+O+UEgl1yRG9vTOnZEfhxlVVfswwhpUQH1A5dtiOgkNyUTkk6sdWfXb0vXu+cgL1Yd3fs9HW/Lp2dlFR5wVerf3pJHBvmjdQbbtfyaD6huXSp/75kWDAvbwJhFbZRtkkRxfLiidGDWw0LVR7/dOoVs/Z1LJAl39KFZ1h899rWTTMj+qKK8t9m+ija3vv6Lf25Od7rRFw5leRU47SIsxSKnEaJ1gkvjDRJ5m9Xv1Ze/uxZ59XFe21428rqZ5LL7gjMLG7tnplPHY+l790xDS6xIeQTUemSd+2QMgxjtKhU1bkmAVWMQR/vIZVCMa11MBQFfqf0/AVQ5dsEKsqqxl7z73qhsCrtf/Pu+9pt6YOPbL9e+34V08rmzMvTLetKztMvbYcL7CavAWLi4+2xZ5TmIv3G/J5uhn0RKr4dmwWNVh3UfgPqION/xMr1L9Qv38xZhdERpXjfqbg9NrNur1m4exmW6HgYh1M4c4pZ4zhUmaoHqJ/JkgyiKQOsqLqMdsmZMU/ShGgVoXmg9LMw6u++RLg5mWv1xZ1388ye+dpIJ2Z9XmbcBMKxoNZ63DPsNvdQiLBYRdlm7orx4KQRps7G4+gqTi5TeszWiRdCSGsWCxNDFWrxklssI9rBW44pXLxo97NC8VOlcldNrKm3wHWAldy4Xd6vrAUFDEEZfVp2y6scZ2zDEVrmRRNO2NiFrtj/6Hh6yGrV+iXaMQ3lEaPaRbov0XPWNswFGz3sw7iEz9+OUpTxx2QSgTdlbD55RvjoUpLFJZw7fKiOq0lm/0vAfMhpSXSAgA7/NUD7vuCRrsVhuuNwiuzfWVBgg5YGQGG2yHOe9I61VT/ekyXu84LSlbbICLNYJro8nitNWDCUGizUSHSw38gyJLbMCrBVbFkZzhbTZQssyul7PAsls6oFNksxy5VIs9v1ui8Wz093Kq34gP7bD4JdSLYzSLUs+l7FqIxVe35Booj4w/GYIMfbs9u65iEZtbAHmuk9xZZcMqTFsSbtaZJcC80IkTawtwRF0nj2A8AjF+4+86HoEvUuYaDMPIrJb6dkWpZZHrMIEpB6u57aHhSGK20NmvLvNtPpdYl388yfDdtINDRMyNy58dQKBMUV73HLclcv+cca6+yI1XzjipdKuGi9eEKZXCXecogBRWjRfy08xfBHu5RAYiel60UyI39yuUSQhwn3exG9VuGdbPXQ4+rLSFVS4Xw6IFzEO+RNPWoNLGUIXOfiwGLKLPVyh5RGAdO7Jl387PEJkdY3srgxhxzg4DJCJsXlsVquOfWLI+hibskj51PCWyaurfIcgCGwvyqQuyPKCKMGNuMArxz6kD4A1TsKL8MR5GyBQQ6YOQtamrXBLyAOqhldV612/sk1BuMVZPBhSp4MgwiF/oIYkrUF41B8qY89QTcNiNFT1lDKEye2DdkDoY09Ez70/qdKdtr00QeT1KMXkEFe/5jTs2DzH1Z0+YCIchSup28+mHKp8u74pHezXr8pxeM9EP3eG4hJjiM98HrR32fjgU89759fU17Z2dVQgJf0VfWd45SL6UDBJT437aY1wlaKomnff+iQD+b8hIXM9q6rFf/F3zE668GBC5kKFr00iMEfR6uqFR5lhF7R7MooXqfnCmSBk9S1BEOyQK4JHkonhi3ZIberlPN6YF4pzgp14mW6wP7ZkcnimmCzxme+gPHaagBqVvU8uoyDXoekVimPpWWw8EpIQmkH9rgLoWoVHQV/0FcrzEWc1XeuVdwgdGbEQYeNJfE+7urMs+2WVSMOw2dbJNsJtK4jQhNhfLkhEvFo7PtQvr82lPG5tYHtDlIxJBiK/0k8XIRtKa1bTO+7XIwiD+eFh1dUW4HqGaNN4IoMh9Xu41/FKgwHgHaIQx2p4q60Hzxz7WAb+C6IbtARll+S4nrve3n5Q/y454njMcH10N6N3fSSYxKjbz3KI/F0esswkTOSVVEN7zfVxtiSAqZ8livKJemhBZwM+30N6r4WLBRbdd5tJ/mj4dL+AIPJqUg3F0x1iLIjIOQgZMDJk6ViweYy4sofMt1f76efYSTMXEzITDT6ejcCYor2KE7bLJdb7FOf8fYTMMpBKug6JswOZHS/f0en6xtdpz3CNDXeSMDTsR38VHrczjslOe8d2SYbL7YGXmsLk+LIWw7MWiZLGYoatUEjo39EXq0rlkEF58+yNzJshkrSU7XtehDdDxVSPkx6qbBhT0WaD1JCXijwlaRN087xshTdAnnYNNZUtA//V0A3FoIXawzXlLZXl/QRCNvYMEQrkJVsGAZbGcLtA3vTQeuszleD+m+5zO19fH+84rUIkt+bLqiqQEqkeGJoW/RqlOhZL6IUZluDyPtsitGdO4tLelkfDOhv3vFZ9MqqeI/nrILxmdXXZtP2JIl0htIYYU8tDVj8fnQlARkFDy14MzZxUy3gYRfT7iZSLPJ1PVF3cLLJoFFJXRviDcZ0PMWYnXfAwIXOhwtcmERhUtKccolQxLV2vRI9gdciVeEEYnoM5HjJL2O5LxLrpd+J8WfWKUDMST9Y6XtUjR0KzB42Ysm7WeeV3c7nP31eWY4ioviPbyqUrlkszvmmK9LjlU1ddclZlJgh8REHI6wS5XnesS8gaj0FEwzeqRMhh4sVOzAIdJw4/hJBNPENohvG3hwMiMXTZYjF25MK+m34IP/HR4/iwGSrT5V2WQ9lyuRfhWc7UGn3dfiapatyzXROcHiHa+i29YrXrve+EISJiVdKcTKWt7jhvw3YmMU0QiEMsdmcZ5jGQnwj6TiyR03j4KGh/s2+92oYsQ33i6luRtvdRuENyvKB8dMIihGyt5zRWS/ds9sifrj40hPqjh4N2cgAPJmQDwPDlcQQGFa0ZTlRrN1XlCWtjyFLOFlo3M/gqFMdYBDkrp8njHCOIszZwVcxocsSQDcXueL98h7+Me94cJyTDLyLXC9AsYujFKdJUNxwWAcxYOTNv93i0LJW4+0LteFDm7ps51S4VHG7HrBlDSLNdgA5sxde4alD/d1w2Wd7neMgeyOIQQWTMGNXieQ5ZTjxDYjavqEcOXZpB3La32MBcDWurB03LZh+M42enpbNZetfP1rni6OdOiu6pl5yz3gmyjwZ11hrmlfKqeCnbO0aYD3vIyNMXH3J7eRsahkxSOfTsIGR1kTQxZB0UnmdsgzXO+gNG3neTrBixj1eRipqFm5Kp34fPywZh2C4DpFL+pd9BOzkAAhOyAWD48jgCw4pGQ0obsSBovF5jl56QxG0MGcU4XGlGGw0/rLdILhkOQRvUj9dVzIALl1G7QbWeZRmLL+Z2luUwmZr/knR4h8ab3rn7WR6yl5iBt1LxTGatI0OP3XZb3MfxsjeLnXs8bQzfi6mSpP+SV3eGfsdlk+V9DiGr8cgTrM11obSQnoRMDMsOPEM0fBys9BAvxHpdah2393kvSdxx/HSD9MH3I2Qe7wTL4+ejs/56qQGbOnA8o28iZA7i7du37yVk1S3FZpNCjVJPNf033h+2k+7WMiFz48JXJxDwVbSJ4v7h7akX+PDXbiv0QAyZntbfpjSPXMM65n36ml40C6qa1998PEXkLAM1XMv0tjUfgSnV716Nf1gy/y1+xsryvVeXKaLI8O76FvAB6cc8ZHPim6byWwRfLGlne+HsD4I5E0uo0f79PK2DvmBO6WyMtu1TMWRmWk85HISMSvAasrQmNHT6Z+YzLqT29JBl8Tva7QnTT0nuayeZkP2Unv1mcvoq2jcT/9uLU5dHxJsMakmsby/wXxfwmWO3Pnjtu/nXIeP2uxDw2MvSlZ2vfSsEfO0kE7Jv1X0/RxhfRfs5LWNJGQFGgBFgBBiB9yPgayeZkL0f8z9Zgq+i/UmQuNGMACPACDACfxYBXzvJhOzPqsr7Gu6raO+rjXMzAowAI8AIMAI/CwFfO8mE7Gf177eR1lfRvo3gLAgjwAgwAowAI/AFCPjaSSZkX9Apv7EKX0X7jRhwmxgBRoARYAQYgSEEfO0kE7IhJPn6KAK+ijZaGN9kBBgBRoARYAR+GQK+dpIJ2S9TgK9qjq+ifZVcXA8jwAgwAowAI/AdEPC1k0zIvkOv/UAZfBXtBzaRRWYEGAFGgBFgBN6MgK+dZEL2Zqj/dkZfRfvbaHHrGQFGgBFgBP4aAr52kgnZX9OQD2qvr6J9ULVcDCPACDACjAAj8CMQ8LWTTMh+RLd+PyF9Fe37tYAlYgQYAUaAEWAEPg8BXzvJhOzz+uJXl+yraL8aDG4cI8AIMAKMACPQQcDXTjIh6wDIp/MQ8FW0eaVyKkaAEWAEGAFG4Hcg4GsnmZD9jn7/8lb4KtqXC8gVMgKMACPACDAC/xABXzvJhOwfdtZPrtpX0X5yW1l2RoARYAQYAUbAFwFfO8mEzBdhTi8Q8FU0ho0RYAQYAUaAEfhLCPjaSSZkf0k7PrCtvor2gVVzUYwAI8AIMAKMwLdHwNdOMiH79l36PQX0VbTv2QqWihFgBBgBRoAR+BwEfO0kE7LP6YdfX6qvov16QLiBjAAjwAgwAoyAgYCvnZxNyKhg/mcMWAdYB1gHWAdYB1gHWAfm6YDBzyYPZxOyyZI4wZ9CgB5G/mMEGAFGgBFgBBgBNwK+dpIJmRtHvjqBgK+iTRTHtxkBRoARYAQYgV+FgK+dZEL2q7r/6xrjq2hfJxnXxAgwAowAI8AI/HsEfO0kE7J/32c/UgJfRfuRjWShGQFGgBFgBBiBNyLgayeZkL0R6L+ezVfR/jpe3H5GgBFgBBiBv4WAr51kQva39OPDWuuraB9WMRf0DRGoUdffUKwvF6kGw/DloHOFjMC3RcDXTjIh+7Zd+b0FG1a0Fy7bAEEQIC3/cRteVxziGPE6w90pyjeS1Snfz7j4umwRhGucH1PyVijPRxyPZ5SVO231KFAUBcrnGLWp8Cgo3QN2MTWqqkI1ltWstq780ht5q/KM4/GIs2pIdUOyCLBKSyZlBk58yAj8ZQSG7aQbFSZkblz46gQCw4r2jUjO64JtECAIj3BzwylZ2/tEMMV/miJVx+rXYJ5l+kYy+sqxWwQI4xPKmYSiviVCpm3+muitz7x9x2kVINjlghwJcqZwsX63uLwUnnTslqlo8Ntcnv0EikAR+RFlJ7gRAasqSYLKtOkjd293C3ycY5E+djJJRfokQSSS2P4/cCcSGgTYGg25n1YIgh1ymyV2q+VzRoAR+CMIDNtJNwBMyNy48NUJBMYUTZKSDVw2daLYj709SciAcVkVgdjgfFfelKIhZOa1Vuz3ErLFej4hU+TnXxKyukgQBgH214aFKNJ0P2NDpIm8k5o0KTwHCNnzgm3YEN8o7RFT1V5Njg3CJzixDyF73ZAQkaQywj36nLbsE29dX4qLg5DhfsLq/+y9y8szy3Uv/Ef1qAdGgw2CwAOGhjNoPGi8B8rAOmBkPJAnmrgHIY3B8iCTgL6A5YEjOEYfMSIEBMbii0F5N+5vG+s4ibVNIuOgcI4jckhjD36HVZeuS1f15bm9z6UeeF/1pS6rfr266terVq2KImQbB5lUKhKOAgIBgXeCQNs46YIgEDIXKuFaJwJtisYtDzkOmqXnelxhOo7ZADhKZlifxE1JmmYF1vMMIxr0Rhny3Vmb+rngUEwxZoN1jPG0wMEY8644rmZIRnyAHWU5ducKkGVHUyzyjOePx5huVNkuWVXjXQRCDtRuUiEJWX4QBE6QETaVxo41UFRFSlZpzZOy5xvsHLLLehQ5ibFkhiHCQmI1QjJbQ0KN6oytLKsmF5oFUdxnzyAeI8v3kIYsXt8Iy90eeTZGHC9AHKxc0jNtYnHbzznZERYjk0w50p+2WCT0/BIsFlNG8uJsiaMUQGJltIHat8LxeqmnyRkemsVSZmO/RBYvJ+zWc6SkS3GKYlXUx/P1DqdLc8pTyq5bw1zXgBIFlTvd1bgZ9YeTgEBA4F0h0DZOuoAIhMyFSrjWiUCbonH/mj0kZ6qOBbMcRKMJ8kIOhnxAV6Qpwiibo8gnnJRFd1iyubsb9nNB5CY5louMDdaRIASAuh+Pp8iXC2TjGbd4SFIT2WWLugHYspoNvz8hU0RJWGEkAZpu3YO1lNUmZB7ZL/sl5hkvO5kVzJ9pf6lwLGjaLMJokqOYp5zYLPagSb2S3YuRLo+4HAskJNNdji3zxZKkJkY6L5BPRqycZM0dwxQBJEKcI59vcIbAR8oswatKFIxccfni6RbnS4njcYcipWuKkF0POTJB1IkgLfakNRVOa6kHMcbpDBsi2KhwzEkX7pDvL7ie1nxKOttwK5yYwo18hExiHI2QzdeK7F2P6mMgymAbuC7bCcNCn9p0E7Irdsx/svBMkUuAwm9AICDwHhBoGydd7Q+EzIVKuNaJQH9Fk6QmEQQLuGwyNsAtyMQiB8m4wJHVKgfdCKNVCZw3SIk4xNLiVuGw4AM9mxqq70+b00512TLvBdsJ5ZXWpK5mSjObMr8AACAASURBVNlTFLsjduSQvllizsiVe8qyq0Tv/VpW4e9Wn/tllySpnrKUeZKlmO67YMNIGxHQEktmYZRkSGIhyGtZcPImrTvVETmlT9YgSibrInIliTazCBEWOsm8lVhlnEAnqz22U0mmC+wvkvRJGQBcd8jnBTa7Exp+/Lczjts18rlclEHkm55fijWt0rjtsWCkco65WEjSZiGrbldcLxdcrx7r5fWKy+WKq7EqQNap/OToGUpCxom3ag/HSZ17n3e4ERAICLx5BPqPkxyKQMjevEo8TQN7K1qlHLD3ciAUlgxGuCSJ0Kws9WA33eF/7rjztD4NVN9f7PFbOTUmiYTe3EbZkmANJWScAEajBOlizq0yjJSJ68wiI8sW1/T7jWOHBcWW1T6X1iiNTEqSJAmZdPKPaKpRTJUecpJnhFUpnO9r65QgZDEnZNLBPVuf+MrD2wkbRl75fbsuDrOYvq0JmWahm+0Ecbtgt0gRM0ItMZKEpc1xXneiP+JYXvgU9oVWznKSx8jQaIL16YaK2ttqIZNTzQOfj3wO7Bmqjwqpg2mxw/FY1mQyEDL9BQzHAYH3jUDvcVLAFAjZ+9aXe7e+t6IZA5o5GHYRsnixxxcthMx3v26UrLsme5IQ9CVkdUn8oLqgpNV2khzYtxv+Yh5LjGGBEYXYstrnPQiZJAncaqNjTYQM4KsAIySFmrKMJ9ziJQlXI68gbPK+JH9camF1qwkZmwPGznD84qEortcrrtXVCnsxgCTpddDkpcsfr1wjpTAn636rLCVeum+Y+UjJosenSFfbDTIiZckCNKvqyysJWXORgFlyOAsIBATePgK9x0kBRSBkb18nnqSF/RVNODpHnlWXDeKhpiSZ/5Kckozk1J26z6Ysxco28ktqDIKNsh9IyGR5FjnQAS7XKSMFPTmByirLluTRPm8jZDL0gph2jATJUoWLozORihFGtPghHiNlDvH8niQYcXHUFlOoEtyEzO8zdSvXmCXcD00nefGYFmyoSU9VAz+S9fjcwHiqG067ArM0Ef6GRD7J12yC+fro9tGzKyJSdz5gt9vhwPzTrAQ09Sr86EY5x+Sy5YsNKObafM6n3U0yJ/RLPkOryHAaEAgIvC8E+o+THJdAyN6Xfjxaa4coWj2QjSZYMV+sHLMJOYUzZ5x6CpAc0Wun/ZHwbdKd9rOFdl865hMpEFNYyQzFMsdkFCM/aqss6wFyICGTIRyk5UuGcpgIJ3J5XcbB0nyt2gmF4zHYBMw+dxCyeiUjLZbIc+yYj5b02Vphd9xhk88wYXG2rtjNiLhMsTqo6cCTdNyqHfFjpIsNDoctlvMJaLUo/UmiZFrIpNUtNYPCSv+zuxy00KD+q87YMF+vpuO8TCPracNP+iDezTcoaVUkPYfrGYeCk6RYEChZpvv3hgNbIBAhzg8OEkrWsRHShVqcQuXQauH5qsTZFfZCTs/PaRFF+AsIBATeOwJDxknCKhCy964x92z/MEWrcCaLhrCYjJIUs2IHZpioiUeGqbRI6GExmHx62AsKdWCFvbBDIUxybPWwF/ckZJIc6BYe37EkEDLPo4W9aJX9gt18zFedjhIURJ6qM3aFDAEyQpLOUIgQIjWBs3za4pkI0yBWG7LwImRBm8yxFtOPsl02IZOxt0xLkfBXi6dYHc+1P9vltEXOVl/6g6fKeiSeLvX0EbJdnrDFIm2EjBz7z8ctCqZrMcZjbsUbTQpsywturulkhxDSomi0W1goZyEyrAOxcCkg8P4QGDZOBkL2/jTkkVo8VNG81daEzBdN35vzRd6QhMJH3KLI4dD/LC05YZVEiOcbvlp0ucSyDjHimO7tLZNYyZlttNWXzJTErHOpPm1JiyJmBY8R5ylf4tdGyCjUyb2mLMkKKILBUiy8lSCbZPWSHwvRXTMgrUvUJiGruMUtzkHG2fAXEAgIBASGjpPBQhZ05l4IDFU0byVvjJB52/mxb1x3fGp4uuaWSfK/F3G8pGP/fUWk1Z2jdM6c3e9bxrPlo2loH2Fqu2cLKKezZVm3EuvJCJlzGyY7czgPCAQE3gMCQ8fJQMjeg1Y8QRuHKppXhEDIvNA87g092Kpcgckj+cv9sR+3vlBaQCAgEBB43wgMHScDIXvf+nLv1g9VtHtXFDI+MgI8DEW9Ifcjlx6KCwgEBAICAQGOwNBxMhCyoDn3QmCoot2rkpApIBAQCAgEBAICrxSBoeNkIGSv9EF/bLGHKtrHljfUHxAICAQEAgIBgedEYOg4GQjZcz6dN1TXUEV7Q00PTQkIBAQCAgGBgEAnAkPHyUDIOiENCVwIDFU0VxnhWkAgIBAQCAgEBN4qAkPHyUDI3qomPHG7hiraE4sTig8IBAQCAgGBgMCLQmDoOBkI2Yt6fK9HmKGK9npaFiQNCAQEAgIBgYDAwxEYOk4GQvZwzN9lCUMV7V2CFBodEAgIBAQCAu8WgaHjZCBk71ZVHtbwoYr2sNpC7oBAQCAgEBAICLwuBIaOk4GQva7n+2KkHapoL0bwIEhAICAQEAgIBASeAYGh42QgZM/wUN5iFUMV7S1iENoUEAgIBAQCAgEBHwJDx8lAyHxIhuutCAxVtNbCws2AQEAgIBAQCAi8MQSGjpOBkL0xBXiu5gxVtOeSK9QTEAgIBAQCAgGBl4DA0HEyELKX8NReoQxDFe0VNjGIHBAICAQEAgIBgXsjMHScDITs3lC/74x+RbtiO40QFeULBqhEEUWYbq9PLuPtfMTxfHuEerjMBqzXPRbpAvv7NuOyx3K5x+URpBtcRHVBeSxxqYbkvGC/XGJ/AcriqXSswu12wyCxhjThnmmr2w23ChimTwqve1Vb3XCjSnv83coNlpsSvTX9VmKz3KDsmaG6lDiWlxf3XHpAE5K8YwT846QblEDI3LiEqx0I+BWNE7J4+TEJWRcpFIRspzGZ6ozdco5JmiLV/03mWO7OvQYC12DpJg43nHZLzCdWXekMxe6kDWol1rUsCUZRhFEi8yywL7eYRlP4eaWeX+ZLsZAMriwQRQXMJ1XhclhjPkmQiLqSJMFkvsZhGHtyaBB/LowIXztkvxDZjBFFI0zWJ4G/IqUmrlfsF9S+MeIoxrjGTD9fW+10iEeXuuSysl23U0TTLTRNslMI2RT+hn6l2vMAUK6tdIs9rlC4me3mVVXnHYoZ5Ztgvj5qsii8LKF6nZptc+sStWVdEmwOHJh+jZDUz4O3jdK7cCbS6aN/zvJ9rRj4DH3FhOsBgYci4B8n3SUHQubGJVztQKBN0WjQeA7rk1/ELkLG7ytr0w37RYy74qiRIVH67YjiLsZi3/0p7xosXdcumwzRdI2TXeTthPU0QrbpabMaOvBUB+RRhOIo2uYgZEy2JMfBYhjXQ44kytBXNNbuUSLILREjIo6KWLgGZPU8b9jNIiRFidtli2l8h9WJ7iqC4cK1UaYHHza4x2NFvJORIqaNPFzmeOwiSh4iohpCzINZjN3vQ/Oes11aGY371x2mcYLiSMp0wXYaI1mWDQJriCROrvuFwMAm+5y8+kiQ67rrGhz6VcvRwFk92zqNdmCXz871Z8hIn7AWN8rWCgqHAYFnRKBtnHSJEQiZC5VwrROBNkW77JfY9J2L6KzpPgn4QOefNr2h3PCpL156hXKZIJ5ucLrq3+gVrqcNG/CWpX7dLVNjsCQK4Zhau+3niHsTsgrnbY5sPGaWBrJWzVbCCsIGHmERYpYUt1zyanXIEUeaxUInIixRkyDIvO3EQqWSR0a76wFSK7++JnNov7c95tEEW8ZLKxzyCHeMkalB2yhfZrXLtM9FOntwN4hDI48ms6xH+22Upd3jh235m/eMdrEpaU4ExzH/yDHuAzit7hDlB2VZOm+QRnPw7weFV0Ms44I7na9tZREjinMctVfCIEjMBMaUXxFdoz6Xhcwtg8xmy2Kfy3Tst/EMjbvhJCDwbAi0jZMuIQIhc6ESrnUiMEjRyF9knoHIRJJkyLdyCpB3wrOCpshSpOMRxlmOnW4guh6xFnnH4wSzYoezNhCQ78o8G2PMyp6h2JOfCR/oosWK1UtTffF4ivVJy9ho4Q3n4xbrQpu2nMxRrLc9fcA4cTAGRw8hAypcdjnujClIsiJFmK71KUs24uIumkHNrp6wTiPMacQdMvBcD8iTSLOeuAfMR7WQSRNkLadGQOprjQdBLAN32lQqG3zne9y8FrIeU5YaYW0M5rolpyGXJrND1EZZjTRt+Zv3bMJFxVUVWb5chOyKnbiuqqV3Kgb3GGgnOWaeCPJxyevOthFZjuk9JkvuWSb1TFmuNQupmkp2T1m2y2rLYp/XgtBB4xkad8NJQODZEBg0TgIIhOzZHs3bqqi/op2wos57JfyAxBRgzj6veSd8N98KklWhLBJEdyuwGSrwvEk9lcinZGI2OHOykkRyuoY64hNObKqND3RRnGHFLHU3HPIY0XSn+dfI5+H3jbF9fVL55S+zGr8lijhqWA7YACun7vT8jUHDMyBdNsgi2Q5q4x75XQyGX6MMQyBxUuFyXGEyIl+sErdSDpIp0oaFjLI8jg+ZQSxqOflz4dN/chrTIbNOkNj4OkXEpqf41BoRB6N8u4i6PvsGP28M5np9LK8kDzQF1iRNeqmsLDl1ppE+laYtf/OeoS/C9yrfl15CJomaqk/XI/1YpWgcXbaYRBFSRrC092FM74zmH1edsMrE1D6bSo6RrbgjfwNTvRKG6QSbEy0SuOF6LnE+2T6E7bLa5dvnenWckOnP0LgbTgICz4ZA/3GSixQI2bM9mrdVUW9FKwvEUY6DZpw6FuYUlOFfw6ZcuM8QTbFFZB3Sfa2Y9STF5iwsUvqAUUPMBzp9ypJ14JrVpU76SAes/GyBRWZaDmzioJy2dYdzmpay/Hi0wf122mE5n3BrwyzH5igcvDpWWVbHJZLxGNl8DZnFaK5ORIwbDz8x2l0TJI2A1NccdTEdUDpz3qRiWk4N2qp8aR3TfbxsbOU9j2+UjkNDLk1mh6itxIClt+RjJFibNrac+h1VcJJcHlFeKpA7wJKWmYq/chkj1k1bQn5uUVV4yfSu3/MmQzwaIa4/hHgqo23XPeajiE3rSwt1dd5ikY6QH6qmhey8MRfIzHIsl0ssl2tsD8dnIGRti11cKIRrAYHHR6D3OCmqDoTs8Z/Buyixr6JxImQOQMzyxKxFfMAwCBkbUPj0Cc9rrQKs7zdJlwK+ec9ZlsrQXN3mWhmmpTcOheN5QVa/0wq61U4RByPHsBPdqmXJlbaGvahq36JraU7HTuYFNvsTLrXPnGYZkXU4yIN6du1NMNpdkxyN3NTXXOWcsU4iTDcXVMKiyqZovVOWrjL81wyiQcnuRcgoPEaFi2t1ob9qsy5fOs13zLbSJiMr3AfpWzyt/e3IwlxbkDW8fFWB6W6C1YlPi9JCCvntZOJU4XK2VnpohfrCXnjDVVRnHHYHzf3ATR6r2xXny61B+EzZlCBspSa1qXX1sUofjgICT4lA33FSyhAImUQi/A5CoLeiHSm0gnQytqtwEDJmUeMO3b0sZLNdc2Wk9CHTLAddhMyWTJ27Bwp5/1aukMU0dSNDMwB8ReIIs93FO7V2PQq/OUl+2K8d9kLW4vltJTUqz3U3RZzk2J2v9WBLA+XKWJGn0tdHOlGpL/Y7UJZAaZ2ypv+6ZL/sMKcps2iEbCVXv6pnYRA+4lR2uAgD1xT6dDPTBTnNSOn0qduGXJxE6qssZTiQ2arE6SkIWQvEdrsp6WXPF32kyRhjWiwiGVUXIbP9Ci87zEYx0uLApvbdpOeK42qGlHw2BcaEx2TujinmLsPl58WfrQrrIvSGLGvbEmcL5+YzFCt6ZyuUjenQFkDDrYDAEyLQe5wUMgRC9oQP4y0X3V/RbD8wCm55Fr5cvBNOlzKgpPARk75eVYklOaLbPmTiflUuDWsUbiV2LF7DPSxktBpS+nrZg7mIteR8nhTglDuumbcrHlPJNYBWxxzxXYGDPhVLuaszNq6wF14LWYsfVi2NZpWqr4mDLsLVcf+8ThFHZF2xC/ada7I0iI8vj37dT8j0VPax6xnYaerzgXJ5CUddoHXQgSlLzWSQPlCS0Gq/ui+iVbx5qvAyr9NZhWORcr9C/eb1iNV8zXw4m22rcMzd4WEumymibNMIMtwgTvW71Ud3lWBNWdS9xlHjGV6wncSIJ9uGfI284UJA4BER6D9O8koDIXtE8N9TUYMUTayypFWS9GWdzgoR54oPGHfZFFmaCn+njfaFT1/ScpVlylZp2qssr8cVZqm2ypIFVr0nIdMsao/1LF1kYDghcwVw7S+hspApBvgYFjIKSJpnQ2LOvTdC5pgGrgmJRrDomk6yGoSi/7M2U7YRMjOl66xJgu5JyFx+ngPb2JTFJbG41ij7hkMxwaQ4OKzpLeWEWwGBByIwaJwMqywfiPY7zj5U0dxQ8QHD8CFzJ3zyq0OmvIYI4yJklH/QlKXXQsYH9TryfotgTh+yw7l9gGL1tkS4r0oUSYK1in7QIgG/JbcAotWiD9n2yXZub6t4SNqhIRNYcFVtAUabHL3vtfiQcZ+ylmdiVPKwrZPcJGjglGUdfNYioIyY9t/2yy2L0Vh18kDdUgWFo4DAwxAYOk4GC9nD8H63uYcqmhuol0PI3PKFq34ELtjNMiy0FX/+tOHOq0RgwF6Wr7J9QeiAwBMjMHScDITsiR/IWy1+qKK5cQiEzI1LuBoQCAgEBAICrx2BoePkmyRktMnzdr3EcrO3tsJ57Y/35cg/VNFejuTPIQlF/T/irFy27l+pwwn8odNkvhAF9xfyiXKShaZeMdhVB/fX0l2xunIMvT9o6lMWXrfBN33It/GytxpjU+hP2RgpH02f07Sia9q1ll1LTIe3M47HjunuOsvQ5zI0fV3RoAM5dU5jxbHvi0q+sEv3atI+lcs6+6QdqmtD3+lB7e4jcEjjRGDoOPnmCBlfeRchiujfMP8WJ6LhohMBv6KdsU5jjBavw4GWHNProKuaw/VkvsRORsB0IiAvusiXz5n6iuMmx0yrh/kETeZY65FbdZ8xOxbYYo/SCgEgJal/PT5Icnx3++M8ToT+WgZx4POhs9O5zht5dVxqDKUfkhtztYF204dJ7g9p1H074yCf0WTOQi7I6FsNefSMlz2Wy71YxafIl8rjls+3R6jKJytpWSCgh+2QyfVfJ26EB8fOrQ/2bgjaggz7I8FXPiN5drs97ZDK2RWqA1ag3VoP1PPVfSobvqFMJtWWJs66fyeFoZHbvLlCdeggdx2rOllKzzsqn4lLLnatsRLcE+wYFCePr/R2SeYq35WOyxriunmx6bjhHyfdGd8cIaPI1UTGpnxnYnerw9UHI+BXtArn3VpFk++s6YTVXXMfvc5sj5HgtscivkNxbJqybscCd/FCbNLcVpk94FBa1zWxQi3fN7dv0rdDaqtK3PMNoN6sLPK93KybtvqbmlviUCyrTYYoycXqV1USj6mWYaOCw6ubXUfVAbnYTkrfqaHOpg3kLOCpNtjQ+NwYNGwiYKymc2Fe1+Q4sAZIlsIK0VKdsZ3GdaDVhjx6qYZsShaVR12jbDpRJGKo4pxxkqTyyUo8RIYISRchk0WwGLhWYFnGMzR98BKFFDWBNdrKCvVvIt54F0wcSDTW1vrZq+2xNLG1Q9dzk7eb95o4UlqVzr7PPuhloF0KSpzEmMqNZA19k3XKX40o2h9RjGyqOmUO+WvLQNf7XpNlNN7pVlmb5bP8emw+RnTFx05HWVKG8NtEwD9ONtPSlQcTMhZ2gCkgdSoZ5rRfHpVMX5rrOTIW3JFiPGXI9/xbsywkaZLfnjfs52TRGolNcekrZYWpyDtKZh0bQ/PGMSVmljFhIaN4VUyZIkSzDfbrGajjT8SysLY6WLBFioodxRhPF5intE/hEqUbx3d3daii+QFqdtD+tI98h8U5izHdnFAHrKcqqitOGwqmukTZOWXmkt91jYL4J7jrS8iIDOQZWKiQlDZznmElrGh656lbA3zo0NY4fC9IYUWw9yjUBqhmGf6BpJlWXbmdtlgkFDD3iOMqQ5wssD01ia/M0WsAsomAMVC4Maf3uGGRTFNMplOkIytkB9uWS23ZxGRjZJYHNnbJKOU3ov1rJETlccunk4O6LOeA7Muv5+o6Fv2s3AtWJG8M5rjhtCsEbjJYsaYH9nOwzw0xbLntc5scNO8bxQ3UVYU/Y8FYCIuaJJfGfXB8+J6evFYWnFpuKWXomymVcebEQ8PPSMyv2zHcmFyCpMp33JTVKKT5kdUhq11WUwe08jvK0lKGQwuBoePkwwgZe1BEtiZY7XZYz1OMZ2IDZ7aFzAgJRVouZkgYUZqCPjZYHCY6X+zFsvsjCjqPc9DuMxVZJ+h8NEFezJGyr+xuawXNu88zTsaSWYHlpsRNysjKyzDP51gebq11VKWoP04xLwrMU04gAyFT2uZXNNHByJherHPKsTkUgmCPkOViaocsQykFiIzAI3TL6acLDqQzFA08STBbHZVV6XJAMUswpr336JlGfFsmPuPhyyc6w/UO6ynVp6xFzB9mu0Yh94qkwXpeYL3t6QNGVqAoYvv5KXR8g8oV5WrKSb6camEfM3f1x4os47S6QyTfJbp4WiMVOx60dp6yAPFbnTeYxtpXvm0REekey0J22dFHD33EFNhpBIz24ywI+1GC6VrtaiDFtQcIut64Zg90xkDBMed6pIWFYCsFXVM3JZaxZZl9MCGTW4QpK49qg08nKGhphIll0Vf5aoRYH9mIZC/1qHULLVFG3RfKrZb4dVufmC5MN3xbozpY8ZFtcM4sebZFzn4uUmT2a7fbPrefc/O+UdxDCJkoqKr4NlEUbsfE2aETQseYkczQN1Mq48yJh4eQsa2vEiR3sdGHmHLx0uvpV/qgqq1ZninLDlnt8m0dMNrTUZaRNpwYCPjHSSNZffIwQlYu+aCYui0JVe2RKzaCjiIs9jdADGKR3OyZdYQR4vyISrxw5P+1FOYJ1kHIvLXo7gOmaDRlaZiZaeBOUNTmDkEanHX8VljrtE2imUIGC5mOuF/RXIQsQrLY8Q7+vEYWxbUllMd90gdG+kqlaSJB7HHRoteLKSUZGp6Rfr4ROdjXrS+fet45i+RfodKmy+z9Au3z2r1FB0Aes62epO7WF43B08hvd9aezo7pfLZCKYxK132OO/HB0tp5ShHIyrFdIB2lWOwuuOjxoBoWMsr0WD5kav/MWhTjwHX/jA1ZoNMN9JBm7F2WU1kEYit2XQO5IYTog6TuyHtPNGXJ2qBImqyN/d72mNOHhbUFmD1gGnnuc8KIVcx2vTiuEmN62tYn9jGwOIhttiocFhHuVntGyFjMQPs5sHNJRpUvF18oYD8X+1wQMvmcUw9OdZs9xIbdb94zdEiQ13xf1m0xcW7KZsSl87yrtWji4LKdIIpSbEiZtX5GWuXq9Jc9FrRn6/aCW1mwXUcWwmfNlKvOwQ7Y85pscLrdcKO9PstzcwuvDlnt8m0dMGpkZcmdI+RHs5EinHgQ8I+T7gwPI2Q4Y5NJ69EY00IMuqIu9lU8474H3MleEiU5Rcmd7rkCxxZZy7G/ksLdcDvkzC9stOqeMGSK5iJk+nRjTQhddfwP9uVMU5VLWR1TyEDIdBXyK5qLkE2xlbPTYjqnDgYrsJUGNd4Bis5MVMj0g6ZZrjtMI5288Q6UkW9WjiefJPm5HGT0ljzkmLc1WyyQRRnvgFlxdsfe4l/CLISys9MjttO00RLzCR/gZvkG0u/fuypO4rWbsalO2tVAM1LVDW3tfOtU9zjQBh9Jam3fMHldJ6nMIn2XIeuyEjjKl07QTb89DfPaiqSRBf2avsKwujBXiwm7P0OxLWvrrD2IGQgZJEU9f5VHXdPzsenk6RTTWP9gtKxGLX5dEk/5K6e39DrISjoZRRjNdmLRwQ3leoJRNGYfvQ19qKfLU6TjMbJ8i3OlkR2jrXpNrjba1zy+cLVC2On18unYeq62v1aawoWBWUqFS3lEealgrma8Yje1prHpg0uOHayP0fsys1R+xsfE0SjGnfxwZDc0/Ojzp1yxmZ8k5/uGUhJyoZmMeD+i9EaUuplAPmP6neVLLJdLLNdbHI7PQci62s3lDP+bCPjHSTOdPHsgIaNirig3OTLmbxUhSlbM7+a2nzPrWZwusDmesc/5VKK0XNF97nx/4hapmE9nSouJJHD676MRMkmw2JQXl0vWM1r9Dz59GgiZ1BHnr1/RHkjIWGevERQ5cIrVUbtpjDvRiZHD+Z3Um45822nTmZkG8bUs3/mrTX05ULhsp2xPSppmJ/+wKCnA1wd0DSqOwhyX6ikKl2w6iWjklVaoG86HDZb5rO7MZ/ka28MZ19p67cDAMcixwaAeNBsVei/YA0sjIYv2z6dUyV8nlg7VrinLRmb9wuNgrpdoH7e2xSApShaVR12T5RIRTQSRZ9tbtbbd8ZwMvWixXHj3W+UOkg1CJgU0frXwHIwYu96NZhubRNko1HHiKsORTF4ycJcXrd8WQss+GOqvQYCNW0khfEf53rqJJFY9CBnrE5IVThdamaiTbJOQ0bh5bgm3YRJF2R5FJOUV+VudD9gdulaE0srLC86XypqqdS/0obIpVEfF2hIImcR6yK9/nHSX8mBCVtVOzyesadohGmFV8i8NsjJJXT/S5s0uy9V8gQWtMpIbSpMFhXzGdD8ft+zOq6wDdNUjv3JYrrY6pOyaX8dliwmRN6MMZ/Xv5qJf0R5IyBjW9lSSgpV1eGkGsmAYoSla81kyqeL8R60d/Q0lOarHGVYn+QJcccgTRKMZdhffoKI7S2vWGjvshV8qdqffAFqhXCaIpytmCZBF3mj/SX3lmLyh/7a2XU/Yfdwa7+h6QEEhUmrLTYUzLaaIU9DUsiIzej2cmDS5oRtzhlXtb6NhTnun0kek7KD0uy+ebgAAIABJREFUKuSx8D+7nksWd2tHfZgvPcNMTtupaTfVBl0+ssws2BQVn0LnFV62s9pqpfJJYVp+exAFaVlyWY+8+lSdsStmbP/ZRJC/JKH3bl1ba02peBvzA5/ZuJwoxteOfeA2YBtE6sxajLMH6moT5wqn9ZQvpmHWQRnKhDlfYhr5iQlfkay52uxmGMUpCuYmYRMy0YrbCdt8wvf4ZRiT32yKWXFwbITuKYOJpq2UpaKZTjQ/bMk/dnW4NN6txntCew6TPLMVylMIe2Ho3IAT/zjpLuRBhIw7348wKbY47JbImPM9rUjiS/yJgMXZAsuczOMWIZPTSOJ6pq2rZ4MuXWeLBY7YUVygielf4m6OMPV3EjKgrQ5p3YuCU78PZvgVzSI/rMPUOzHeaddTlmL6eFHHRRA+ZNOt6pCuZ1wY7xFOt0eXWG35LJnq7FwWv6O0ywrAM9Pm3Kd6GrYuENxvUh981T3DWVpdBm4HFDRdR6Y27c9rIXP6gGkZ2aFbBrrlHYBlEa2DHI8zR5bwk0zv/b1hN2v6R9XJz1vk+oINceN2PrHn3RwsKYG/XXW52kFbW83yrakwsbhDnxZqJWRanfqhqsOS+3py6g/EF67KJ0trs5DRQhX9HZN59F//YK6nUsc9NxKvjlgalroJ5oU2neYlZEVLqAwlRedRq66K3B5yUk8BNtm9u9o24lsdUaQTrKXTp6z6uMJ8TW+KC/8LNlmEqTb28Ww3HIs74VOti8LLUCFStA+MXn2CKqupX+pe46jRblqIEiOeaH10I1O4QAj4x0k3Pg8iZBDsnn1pstVuamk+Lnvk2YivKMtybNcL00ImYx8xQmYHcKVYVgVmbOqEr8Ajf5g+cTqZovUgZOTE7K9DfCUxgjlGNs34qs9gIau1yK9oFvnpImSQWI8wTlOwvgtiteR4zL/OJ3NsmCWqwmmV1isreYc6wXwtV2H68lky1a2wBsn6+kMP3OUOJmRtFplOEZ/KQkbvTY6siwBQ37BIMcoWWGQjpPT++qNeOFvjHjTc2DoL6CCf7vJ9JVl+Xf5kxh1VxzC5VT5Z3LD8Mpf6dRECdbd55CNk3Ipph2lo5pdXPHL3IVKyiPq3jZRq5IQIok6yGoSiLnDYwYPKceF/P0JWf8xq0rd9eGjJ6sOmftW3mgeNdt9wKCaYFK8j+HezQc93xT9OumV4GCFzl/mkV9n2E2x1iXD4r49N68KjCsEUMkxZ6pgOVTQ9772PaUWa5mfDymHhILQFGIMK7+7g9X69f9GeQciI76QNIJ4pS6+FjFkkWnyGakE9PmSlw7RX55Erw1qsg+T7lKyNFZEyO61enaRjjMZTzSH+inJLYU+IdJMVod+76h40Op6Z5VunB2CtLSK6RWfAA3bLI1vu/lVTtlzuvtU1fYg62t3pzN60AJp4OPRp8JSlCwPPu+BcoKHeCdfUqqv0XtdafMg4Bn5dN8p/0NZJLkJG8TqHTVnuFwoj8/ml7u2vjAaok0G6zPBz6IcqLhx5EBg6Tr4yQsZfbumAb/6KRQEeYB50ORCyBnxDFa1RwH0usPAoZgwlNvUc88Cd9yky5BmIwGWHWbbA3hu5Xy4o8JXbdd+X7+NfV+Tq48vymiRgjuGvSeAnknXIXpZPJEIo9pkRGDpOvjJC9sxoyuoCIZNI1L9DFa3O+KCDCqcN7f4wBnMyThJkXgfjB1UUMgcEAgIBgYBAQOBBCAwdJwMhexDc7zfzUEWjwK3lZolNeet2Kv9osCoZn16E9s1/n75+Xw1qY2xfCvO6a3N1SuEqx3XNLM0+64q5Zqe/9zlbUdljKpXSUbLbma28HOQWJ/M6sSHJ3frHpq77znX2bce9gXrKjMP1Q5emOc2r32UxHPizsy5/1NO+z0vqTtC7J3hcD9O7NoGGjpOBkLWhGe55EWhVNIoKPuMLOihaN5/dUj4UDQfUNh+PHquHWHme0AbkZ2GPZZeDCLja8N1SMnobrt+4l2OyKKDhKKsXzI8vB1rYMsZoNEYyW1mhBq440upj3R+KHc+Qb+Qih2aZ7IoI/EnL61PanmqSY1uvmLF9fly+S7o/iZ1e1um63rzm9vFS5Td0RRbv/CVCwy2oozqgqUhoh1mwBra+9dS+N23P/rLHcin1XnX2dV7vSlG3/ql8zkYbF/u2w8g06ISvsh0t7unQ7XnXuc9YUz8aokm/NrYK1gy/0YVT1/1GXdoFWlW9o72ZPaFSKloxPOFbvSXJBPlWxAQLeqeh+JDDj6x39xS9dZx0lBkImQOUcKkbgTZFY0FSKWzFrUSRiB0YtGXfjUHD00kzp9W+hGy6rSOqt0kvAz+yAK4U8DBWcYPcS9N9pdF2YLRLhbkHnUqtO1FTWAItJhA5nncRMrYtVIY120uoYsvg9e11zusE0XSjQoPUFfOtphKesb6qDm7YL2LcFUexjyxwo71jY7lXbMegKOXerUWwWRVzS8a64s7G/LoKKUIkq1l2Uxco5pGGVY/nL9vGnm2c48DMVny/wjpauk2grHNDDqaPmvx1oNw1VnLlq5VfysB+jXuqzYoQqGuUXieltL2OCmvAianKx2tpLPYgjEQIAqMdhlDy5ITVnb7bhbze95dW2a6xkdtG9M0m00n9ca4rMXGRWdQvxYjk2z/RI2b+o4natq8Tp8bHS39HeCJkx+MOBcXatIOqscVGMdsCicl63qDYCSdLQxdYtGMj3IfxvN6y3rEtwrpCs6gn3Tz6mHrXlKbvlbZx0lVGIGQuVMK1TgT8isY7/FzEFTtvUkRinzq5l5vRCVFNrZ10pyh8CrSXhYzvmTjbqYkmtm9fLrdUclsoGhKwkA4J4myF43GFLE6w2J5qgtNI72qf65qWkWEk93ql61bHziK7JzkOF33j7Aq3y6Ej8Ktr0NOv6ceaQPKwIXdHepmP/TbTdulC475RnnnCBmSNmBt5GX4ycCtZBsmCW0DtjmYF1tStWBr29aCvXTOlsJ+VanOdVy/byOzWP5XPSFyfsHeMthbrCPPBMyh56gKe86ChP3rlHbI1Nn/n7/Oc9kdmsDvIEporjSk46lrbEkuXoP2YPx+bkLFwNtY+rHU5Qe84FK3PvUbr6Q5a6+/QuwdI5R8n3YUGQubGJVztQMCraOxLaIKtXIV3LBDdURBRNdgYAyXVw14WzSrS+JLtuSy9Q2Yw2cz9Lmm7Hi4fE6TedNhZFK0wTEaIKaTDTiNgtxN2xRTjeIRkukYdvF8W4uoMXNdkevq9bJBFdyj4Xkxgnb4YdGWy62mHzVKftpwhX26wc0WslZloE/ZBFjLeWSlLFy27X2C/d1nIRCU0rbScs90U0lmBbR1mo9nxuXVBfUk37tftaB5Ux5wFSOVxNkUcLbntjU2g7IGyYYnTZNXy1uRIu9aQxChbWRDrvF5CRgE3tR1CRMEqX6MmenmYzsrA2q14XffI2d6pPLaj3AeU51ljRxHqY6r/cxamhE2H05T5OENeL6u1SAlra47NgcKakMV4hCyX07UOeX3vOvMr0DB3Zd3ZpJnHhovFpsMunPiWZvThovwDq+sJm2mM2HqfHFVal6y2s7t8Zxcpg5Wh8SHFP6y0D4P3oHenNTL2AST7eOrPxXiw3mE9pRkEGjN4eJyXpneNZzrggnec9JQRCJkHmHC5HQGvotlEox64xAu4vT6iU7/Lv8kTp4c6fFs2aqIxeKZsQHIFXpRoqK3C5BXz13Wfkb4ogrQashzGwKR8pvTSaCprHMVI5+QXRVsy0V19KtTTVklorZhcddnVCZt5hvG4nw9ZYWzoXpciDuxB1JxWup022rQwTyvJHX8k1iBr4EKbW8eINKuXXbt5TkGGJ4wUTOa0f6eazrItjPZ5g8gw8s6fGX9+PLQKG/TJ4mtZ2Aw5ap2nqwqfOm+qSJqRT9SpT02zEuQ0qZGYn7CN2bUgvawd0lpsO09SFoavOWXJ8kQR5EbXVXVDuTuK3TEAtsdoJEPLWKSEtTVCshCBu89rZPo+wA6Z+SWFi0riuqbuNp6RZRVrEjKxY0xjap9Hwu8f3FbKYLWdXea7h0zWe77NFG38XexwkkZ4Qxdkf9NimX2resdwUB9a8kMiihK2TRoFaq9oUcsL1Dv59O/z6x0nPYUFQuYBJlxuR8CraGwrpAEWso4AkXrwQ9f40i6ldZfJZu6TSX5HUT3doEijlZMNrI2NyGvfIosYGYLyMrMsAw0AzCWMCneRQ71SssaNEhSHEruCth5LsKitFHrCxz62V02aJKp+HlPZFmsQdbRLDZRWWoJhv6g3Pq/LJguc8DFyDcK+FlflEuloim15xHqemnuNsgFBs0xYhKpRT1kgjiKkmzPYlKDYl7duiz3Q6kIZ91Sb67waSdOznTcZoumUEdhCC56r8umpadUg+WhGqDe/ZmplEVwri5+Q5ah3L9PyVLcLTrsCaU3KLVLSGGh5e9s+anjxPJ3c95L2Cy0vB/e+l1Kecok4VmRGDurTHVcWJ05yEQAtXhEfKu37ccrKXL9W21kS3o4oWWDHFsbwDclr61vQOw5kQ08Elrl0FzHxflF6Z4o26Mw7TnpKCYTMA0y43I6AX9H4/oUZcyq/Mcd36aTs9yHbYyGtOs7f9inLhpOzVYbiR/yLOc7lCrEz1lkEOd1Td/Bbp8dxExBj4G3epivK8fjM9q1LpDO9g7ioEmhfzgi1Q7ooJ6qtFD4io4hhZ6Rzr+yKQCh52o7UKkKeinwIaaGDMBHcjmxhB/fzGVo28dYOglGLxn0XlX+gWAjBpsvrRN4Dsx6+L2o2neLuLkN2d4fpNAPpzUFaq7z42VYQ1WZFGNQ1KRBZupIow+YMMP9AbUcKlU+mpggZtIekvjE7v2e2Q0svD70WMp3oVDhvcxbvb5qvsd0+EiFr7HtJU+xi38vNrpuQ4YRVopznGWZaUGgnTrLdj/LrImTcj80goOSmob2rbVWbz+sN652PkBkLJF6q3rU9wfZ7/nHSnS8QMjcu4WoHAq2Kpu9jWvtU8c6MOi6zE+qoqG3g68iqTxfVSUXIB5quS8ZjTI3NrZWMdfq2Ayt0gpn0htNmhlGcYimtHVdyuI8wmmxwphWe2lSTmZcP2EYn7xhIzTzyrGcbvLg2yUKfaVJFegEWAiDjFonxOMFsXYoFD66ySW4+Desikb11xYWPYxCgrWe66qmOBUZs9R6ffo2nO1yZNWqEkQx74MVPEjJpjVPTk4ow6DhUuOwXSOqpG/4cL9sZRtGY6Y7KR/fISX2JLI6RFofGyuJOvBw4sTzaAgfpi1f7gRp5LFLSwNihu1I15a/3vdFxkYmtX9a3jDFOE4zHU6w1h00DJ4flne25LBYYKWusWx+sWsWp1XZ2VUyL5pqlp0HI+un3m9a7hp40sXzReudWiM6rreOkI3cgZA5QwqVuBIYqmm59agwabWEvrKkll2SsI3Z0tLLT1cmCK7+61pPMiAy33QxRNIO2aFMVhQrHlR7fS9664kRO92yQ030q5H36lb4va8jwYHyLKBmaQliOpK+QZRGkdrtIh16D7TsnsUp9/k1GZv2kxyBaJ/el7Ys7j0UUJbRIxP7jGzXf5ZKkkJ9QgsjYc7NvPYDyBdS2eqrI6iZW8rURMls0ca4Ig4XD9QTnOgwhhMpHBV2xy3Nsaycls7LGu2XeJocw5FGEhTY/aRMyNo0fLeopzMtujrvHnLJswe4h2yyZONkN5wsA7BWSKlWbbslUTRLB7rAQNXJLNx4Op56yZAmC3uG8QRqlIowPgdLE8rXqndQO1+/QcTIQMheK4VonAkMVTb6ATgtZKznpFAVdHXF3CTJF347zhtN2gXSUYbHIMEoL7M7Si1eW1fHb2eYLDsWMBW5lwVtnVIdaKdY58HZUbzu1q+QWWVA3PEdD0vvS9sWdYhHlyHyWRdr8eT5BkpDTvWtbrb71eJrKjF9PQMj81bE7Q/S7Wy9o4QNfETymoMknQe41CxnIijxPQdbNLJtivi4weyZC1gFF6+0unNrvt+mWWkhDbhcyjI/+0XM9rjBLyGo3RjbfWCutg95R/7/PM4xi2vaOfEQ5JgZBfqV616aUQ8fJQMja0Az3vAgMVTS9oFu5wXIjp7HoY6nLh6wZbV8vr8uHLO1tInNvXVPXVZVYT1KMR2NMiy1kNIdruUUxpfAAKSbrEoo21TmbB52ErJlFv+J2hlc+ZLRDQqsnnGNKR1nJ2vHW5XBOC5sJtDPb30zeUgOeLoM6Vj6EzG/IsHrJMvr89q/HV1q9wXiLlaczL/jq4L5q2bklkFZhNyHTEn+sww7d6/++mg3owqmdkNEaiQLJvXXLlMU8C3pn4vGRzp5I79paM3ScDISsDc1wz4vAUEXzFvTKbqipLLfgXffducLVXgjQytNsgWdZbNpLoJDozSAQdOvNPMqX1JCh42QgZC/p6b0iWYYq2itqWhA1IBAQCAgEBAICD0Zg6DgZCNmDIX+fBQxVNIWSb9pKpSDH3luveT+V52UeVbhRQ6oLymNZB9rskrVr6qU9v6izPRG/Sz5Xyw3KAe5vw2SzY5r1ESqkCQgEBAICbwOBoeNkIGRv47k/eyu6FK0671DMyKdpgvn6qPkz+Ry7ZRMe7gArS3rQb5e/Qar8mlw+cMw/SPoZufzFZMDKNMVkvoa+V3OXr0tru2SdIpHXv44EdMklVj/J2HG1H5fwSWvIVt046XQK1fWszUyNss3b4SwgEBAICLwqBLrGSbsxgZDZiITzXgi0Ktp1x6ONs30YeeTqZCmd3e1BusvhVTiq9/WA7iV9j0QWsTFy2PecxEbGoypQNu6b2wup4LG8llZiohFFWvGliJOIbm/LJgVvyECLKVyx0NoJsS1buxO5/az5udw6qSZ74tnaZUvRw29AICAQEHiNCLSOk44GBULmACVc6kagTdFOqztEuRYokcWgkXvh2YO0rIs2ll2jmE/qrXRm+RKbw1kEFZXpnunXR2yoevueRWxqq5SMoWbdx2mFu0jfqoZH++bR7LviJcn2e4iTLZtMzrYCipEftblgJpe+4S8l9pQryrFJ030ImRGcW8qnh5TQroXDgEBAICDwWhFoGyddbQqEzIVKuNaJgF/RyPoTwYgyz/bui7EsqVgXIeNbhsTTDU5XRRiqC49sr+/V1ynYYyXwERvWBNoaRdtqxiZclIaWW8oyrPu0NY69YTYRnZgD1DOumoc4yToNHDi+FJ+rez9NZbE0Ipt7piwDITOADicBgYBAQKBGwD9O1kmMg0DIDDjCSV8E/IrmIgo6CdOPZW0iyrrc51FexgWbaYQ4P/aL7VXne4QDJ7ER5dr3DEuTmGLN1jjJdDYhc+zPqFue9GN/S45sI2ZuiVQkKpVWuTpjhdMqQ3xX4HgTGx9nK+7Ib8lVZ6mbGTFL540WWdAm0yUFqxWBUUWaRydkcseF556ithsfzgMCAYGAwAMR8I+T7oIDIXPjEq52INCmaOUyRqzPS4mBf8cilboIGc2UHbGm6UqKss62AkqQJClmxa7ePqhDpMe9rflqkTymvxaRLs2p31ez3LPPXs1YLhHHmoVNThNygHpZyKpDjng0wsjeukmSQCbTFfv5CFE8xUZG+ado2IsUo/yAqkHIaDN4QSjZM5hgXqjNn4/PQch0vfHhGq4HBAICAYFXgEDbOOkSPxAyFyrhWicCrYpGe7vFam+3skig9nbzELLOGj9mApfVrylP7TvW2FtyjNjY7ueEVRJjKnZvZhHCY+lj18OHjG10HWO+v+BY3CGebnGR4hiEjCJunLUVrjKR+LWJYn3bH67iWu6wk1sUsHUBzelX0MrLyxkXkrPecocKt89lhRULc9LPMijzhN+AQEAgIPCyEWgdJx2iB0LmACVc6kagS9Eu+xzZeIw0GWM8XWt7u5mDsmsLoKY1ilttPt4sVj9C5kWtYYkCcNkjz8YYp7T/3RTrk/KdayUmtxM20xjxdMeJVlVimcYYzTZg+01bhEzKxPbaS7nVkVkgyRI5mWPjDEJmPiNZBv3asrEpy8Ym52RZW+Fwscvh52qVpZRngvnu0ihbrzccBwQCAgGB14ZA1zhptycQMhuRcN4LgaGKpgq1B2l1hx8NIz+X7QRxPAEzNlVHFKMIo4L7nBn3cMF2EiOecGtSdSwwikYo9FWHtij1eT+ZGFmRPlANK5kIS1GX6T+wSY+ekkJkpIudsojRTZqGzAvsKcCrg5BVx1z4kOklESncYBpl2NTmNXnfJk5qGpM5+veeVux61rI+/mu3e9gzMssKZwGBgEBA4GMjMHScDITsYz+xV1r/UEVTzewapPuRH1ne7VBgMilwYNHmz9jMUsw2Z3bbvHfDoZhgUhx4GI3zBrN0BpFUFsd+XVa7OmaWQbRMkmUTCqPQAScPKucRCZmLdw2TretZm6A0ym55RmbOcBYQCAgEBF4eAkPHyUDIXt4zfBUSDVU01aiurZOGETJV7sc/8vuQcQvTYs9WNXQKOmx7Iqs4ByGjFEOnLNcG8VQWMkZMe88dDyRk6xRp77KtdofTgEBAICDwwhAYOk4GQvbCHuBrEWeoor2Wdr1+OQfsZfn6GxtaEBAICAQEXiwCQ8fJQMhe7KN82YINVbSX3ZogXUAgIBAQCAgEBB4XgaHjZCBkj4v/uyltqKK9G2BCQwMCAYGAQEAgIABg6DgZCFlQm3shMFTR7lVJyBQQCAgEBAICAYFXisDQcTIQslf6oD+22EMV7WPLG+oPCAQEAgIBgYDAcyIwdJwMhOw5n84bqmuoor2hpoemBAQCAgGBgEBAoBOBoeNkIGSdkIYELgSGKpqrjHAtIBAQCAgEBAICbxWBoeNkIGRvVROeuF1DFe2JxQnFBwQCAgGBgEBA4EUhMHScDITsRT2+1yPMUEV7PS0LkgYEAgIBgYBAQODhCAwdJwMhezjm77KEoYr2LkEKjQ4IBAQCAgGBd4vA0HEyELJ3qyoPa/hQRXtYbSF3QCAgEBAICAQEXhcCQ8fJQMhe1/N9MdIOVbQXI3gQJCAQEAgIBAQCAs+AwNBxMhCyZ3gob7GKoYr2FjEIbQoIBAQCAgGBgIAPgaHjZCBkPiTD9VYEhipaa2HhZkAgIBAQCAgEBN4YAkPHyUDI3pgCPFdzhirac8kV6gkIBAQCAgGBgMBLQGDoOBkI2Ut4aq9QhqGK9gqbGEQOCAQEAgIBgYDAvREYOk4GQnZvqN93xqGK9r7RCq0PCAQEAgIBgfeGwNBxMhCy96Yhj9TeoYr2SNWGYgICAYGAQEAgIPAqEBg6TgZC9ioe68sTcqiivbwWBIkCAgGBgEBAICDwdAgMHScDIXu6Z/GmSx6qaG8ajNC4gEBAICAQEAgIWAgMHSd7EzIqOPwLGAQdCDoQdCDoQNCBoANBB/rpgMXRWk97E7LWUsLNd4cAvYzhLyAQEAgIBAQCAgEBNwJDx8lAyNw4hqsdCAxVtI7iwu2AQEAgIBAQCAi8KQSGjpOBkL2px/98jRmqaM8nWagpIBAQCAgEBAICHx+BoeNkIGQf/5m9SgmGKtqrbGQQOiAQEAgIBAQCAvdEYOg4GQjZPYF+79mGKtp7xyu0PyAQEAgIBATeFwJDx8lAyN6Xfjxaa4cq2qNVHAoKCAQEAgIBgYDAK0Bg6DgZCNkreKgvUcShivYS2/CxZapuN9yqjy1FqP/pEKhwKY8oL+Eh6xhXlxLH8oKnQeWG8/GI802v8TGPK9xuN7fs1Q233i/0BfvlEvuLS7YrttMI0+1Vu1miiCIUpXap4/BWbrDclGiH4oZys8SmvOG6nSKabqHX6q+iTX5/ro9xp1ynSNddwKl39bJfYul+MIPFHzpOBkI2GOKQgRAYqmi47pFnI4ySBOPi6O7QHgDteZ0iHi1waO99Wmq44rieIxvHiKIY42yO9VF0TWWBKJrC6B9bSup3i3ewZqfbL2dXqtthgVGcYn3uStl+/3baIs/GiKMI0SjBbHXs2Vl7yr0dsBjFSB8qmKf4+vJ1j0WaInX9o+fbOehwXZjY+SeaTtSV8YPrfoF0sbfwcQ2sIn25xTqfCRlnyNdblI6RkJVry6Gftww0Zt4EI/EcFS4L7B11Wk3jp0PfgesRG9G+yXyJrda4zoGf1VXAO4S2Ep9hxKVTFhuM6xZTT1/gL6vEOk1hPqqmnIw4iGc7jiPEY6XD67KZnvpUl57LehryVCesJyPWv5Guci6odLSRXrTdTewc8thYec6d9XjawnS165315F0I5S6LCFHNZK/YLxSuVD5Pp3Aw03sa0fPy0HEyELKewIZkJgJtinbbzxuD3ml1hyjdwMcR2P36pTHr6nNWnXdYb+5LGK7YTWNESY4Ds2ZUuByXyKZb3mkNHYz6CAzeoT0FIQMNhusdzj1NEC7sq7JAEiVYbE/sC7u6HLHMYsQSkx5tbOrBFcfNGru+gvWow5nE00H36txxw34R4644NC0LtwOKuxiLfZP1G4OMVj8NrOYzrlAWCeJsiWNtORP6Ficoyp4PDcCQgYM/zwhRUmBAFTW8503GBvL80EO+2x7z+A7zHR/yb6cNpnEMmdfAqq5BHlQ45PyjKPd8XbXnt4kCH2h1csP0QJDn9rKkTNovI2Qxxjoplsde4mDLROW5rmn1NA57pD9vkEYTbIXVzW4bvefs/b2VKJIIsx3psSIidnopgvt6mzyPi7m7fikdNcFPkimV/z1Rbddx8KfX6ux52DZOuooIhMyFSrjWiUCborleIKbkLZaJx3wJOoW3ElTHHHGUYeNji6+NkFnt6zptYn/BJotwV5SmJVMMRvmxx6DM+skhUyBdUg6439FBt5dUoVwmiKcra6qRpjRWmMYJlg5Gw0htbFt19A5f1Hpa4S7O4eI11SFHfLfCqV3A+m7zudW32AFNiV9OO6znGUajCdanK86bOcbjDPP1DqdLvynzW7lCFidYLKaI42k94Ju1qTNGxK2Pr/OlPSv4AAAgAElEQVQmRTTfM5Lr6h9Y7tsJ2wWR1RWOR6pzhInDKuvNzwqxiYLjGShRB07TtQ/+XrmIoEaSAMnKbTn59etxhZkkePQ7W4Eb6t3pZWn0y0hzPFaWYYMgnrC6i1AceQ71PBQ+Pvnd19vkUWXq8sljZ3ltRJdwaFifZWntz4RS+d8TIedszjCTH0/+9FqdPQ/bxklXEYGQuVAJ1zoR8CnaaZ0hGUWIZMewLmFfkyZ1XskV+zwFvQw0LVabkMW0xXafI6Pyiv8Pl8Ma8wmZmxOMaAptfaoJA3vJIzkg8s4i3xxQTPmU2yjLPf4aQLmMEU2ENczVcibLBMW6wJQ6uWiELJcmf57hcigwSxIk9K/uRPV7Y4zpHk2FlvRlymWsrSenFbNIFUd1b1aI9o5HGGc5hMFBwHZkAy3VNx4nmBWaRcwgkLzTmax32NDAHNFUyBTrE5EqD/aXLSZR7PBX4WVFoldnmKc5VgVNvclnwn1W7GfOfTisNqPCeSdxG5vTxOLL3S03QVDhtJkjE5inswJ7aXHq6tzTtX86jKOLq5xSTGiKZ4RkMkfhmVZEdUAejzAaxZiSeaLFQuYcjESdri99ll6+S/pALY/Nl0mWxH7P+yWW6y2O52v9nrAb1RXn4xbr5RI7pgdGNnFS4Xo+YrMgV4CJ0FngslsgZUSJCJ2bmA8mZAyvMUbjKYodt8gyIW4n7IopxvEIyVRZ11sxbFiexKDr8TdoL8uBSwvZ95XFn2GMOFNtkO//KKH+TOqj6LcEaaLaj7mcbuP3zPRKvurMrZDTnZqDNuRhpFBzu6A+gpF/hY+RXhUN9rEhPhTUtCqfAndPaqgytWLqQ2c9LbjWGX0HHXlNgsVl0y2mYcrSB2y4/moQ8BEyagB7ASxrmOuaaix/SdQ8PysEEZGH6QanG1BVwPmwY8cs33mDLLrDSpgT2EtuEbIoWYjpsTPWWYR46fJKcdStBONHjOBESBaC9JzXyKIYsjgagOJ4DtkXXjZTRNmGTXfye8qqcDufwMcxjZxUNIUgBnNWI793N9+KaUc+zcU7UEpwwiqJkBRHMa12wXYaIxYWCDgIWRRnWDEieONTQtOd8HdytN/Ib4KhP0eOeYZC+tpdtpjGyhKgpxVAMsdkSUIb2Bz5NCl/pkIun9xkaYqUn9z1dLL8t0y5733GsJBE31HKrcQqi5EUJW5MJ0eYbSSp4G2Q7WW5B1rInIOXQwz7kuk/ZvrMsCk7SehSjx/ZbY98lmNzODenbqsLyu0S89nabc0TU5a58OHpM2VZdTjD0/sv/9ox4e+OIgqOZyALklZcSXjbrDAyj0a2TRwFxlYZnCiRVfXM3tHRjPdnkpApOakCPl0u+zyJG58it9slBbrhtF0gHaVY7C64kC+jfLa6hcwmLUyvR0hS/jFMOurGlVvWIq2v5TX75KG798C8C9eatMp2a79227RbdMj6oRpol2wVqor6UO5eYKa3Cht42jZOuooKFjIXKuFaJwJtitYciN0kTVXCX5ImIUud04jV7YpzucYsUv45nBzIgZN3FvpA6JKJ1++oWwnGjxoERS+f50/1+U5mYZpjfztjk0aI685AL1iWcWQd9Z2x0EHeU1+7YP4hnIDS1FYUzcBcQGSRgqAwMQx5m+0zsWreNwmdrEBCEdX+gWY5dJ/8f6J6aqqJud4ujs2dZNSs+Bt2swhRfkAlOnVdJ4z6bjvMohjTmvwIOcu1GpDkwOT5bRqXuPN1PaB58rH7lPmyw2xERF1ZS6vTGpNRhMn6XLdB10MadA85Tcu5fchsvynWZuvjRrR04A9/zqYsA4sYkry3U/9AzCWJ8mLCdUy9cu3tHoLvMJJLFtwJRuSHWa/Yu2CXZxglK5T0EeZaNVmdsVvOkZJTfzrHckd6RH/N9JfdrLaO00er/We0jay4kepPWR/CppUVPkZ6URj7aEpW2K4z9t4L9zSnPKp+Vaa6po5c9ai79zi6ByHTLWT0PueHUyBk94A+ZHkhCDwPIdNM7NQRH1eYjsfI5gU2u8ciZH2nLHVZdGLBj+0XPGWWBz2d/eBEvjhGHMfMoqZc2Bz5WKfDl72zDq22BopytfsmoeKdo5fYOIgPrjtMabDQpk54LRdsJ8rS6JJDJ2H6Mc+vt0s/VtioPF1yM4XAiqak4zGmuq/RQ762yU5xPuLojJtgh1SocDlrpLluRiUGUf/AVE+JMtLXd5UlTb9bDuWWRaYWgQ4sHOyVe36/HAdJklO3NklduazOSgryY9OMW0B1w+XcDHvBnrtiUaoABxFxDui08vJyxqVBdHzPgMJXVLgMCvWgiSUO/XIT/ueW8Btc/1WTzdV/4zhGMplhPp9iNi+w3G8cBE7q2Q3nwwbLetVuilm+xvZwxrW2PMrFKkfcqgs20wgJ+xhS+DRwZRZvuRhDWOUXO7E605Zfx0aVqV+lDzYX5sOIrlkiO/MRMtKJyraQOfKzSyHshQ+ZcP0VIPDshOzCpyiL2qHcHNBNcmDeIzjVQN8E1+vUL+dJDIsTK02beuMkxbT0yDpcViB5j8vIVr1djyjuyNonvz+b8hPJisUKqie3kMHj1M+eQQzp1G9iTu3iHbH0MWtirrfLhc0AC5mEkX7FVGm2kfjpN61j9iylJdW6J079gyyXXw2iIoNvQOiYunHX3nLVW09LHs+txuDrSVdf7oFbnbY+aOLlw9Z33WUZYrLLacaaIE4wL1Y4XOw6uU7qH0wJ89tKMVuVOA0lZLcSm+UGbPZf9isNhagBYJakZsgLum/LqedxHfvSuxeh3M475OQGIf0oqMiKptd52IvxXPrMKvKk6wRZerNYd6Og/DxsRrKixT4+eaiie2BO0+DHUrhzuNrfco29F/qHCvny8ilkWunr1K0uS3rTfN4igP9W2zjpyhWmLF2ohGudCLQpGlvBk66NEBfspainGYjExIhrR3ox1bWgqSrxZ5Mgy2eIrfy675RldUQxijCqpwm5D5YKewHwlWXCR8mWRXRGcvrH9oWiDuksHJ6vO2tl2uWAPXOk5h2aXQbvP/m9dCmDOgofMen3VZVYunzI5H1DXkmSlCXDJFIO7FnfbYe9OCBPyKdPduS02nzKp06FkUiGypCrEJt6YLbZxuYmfMh4/g65Lwdspe8azlinEWpSbFmGjClIZunpQcjEAhMjLy1ccE0z9SRKT2YJkO+M/dsYqCxfsjbrml3WfQjZeY00iqCIMif60r9Sr4L1D0Mw1zMbx21EwUjITnQSwi40+gYrj/WsnYO9kWWYPK6FHbw42zorK/GX32ibzGL8ugkZJWn36/PXaxTvOHHKZeHqyHbvS93PyCx6aHozt3nWNk6aKflZIGQuVMK1TgRaFU0EgY3HSb1cmSl5TchuOBQTTLRYT/RFRisYRxQMcX0ik5YVjPWGYzFBMh4jzTLMihXy9L4+ZGdsZilmut8XrMCw6QwrOeA3ZOGdkSRTBBZfZTnGOKWvswnmG7kCtMJ5m2OSaKssj7TqzS6Dx0Ljjvn83l02RZamrM3ZfANjQdxVrrJM2crOrlWW/ilL+vC1sBdPvyswLCdkGSazFClr3wzFQbNSNfTAbrO2ypJw04Px1tY2D5Fk7Z/wVa00jZ3LBRD0gd4el6hLuf0dMpe/YRB5YH1uecwpLJMYWsTK5/D8mHINJWS3I1+oslggYaEy+PRgnC2wyMj3T/pG8dYPxtwN2mDLU5McuPoGrbLBmHp0RivSOBxc/gALmVGRPPETMpnC/TuwXVohTcwf/s5qxTcO/brVSMouDE3vLoVfbR0nHRkDIXOAEi51IzBU0bpLDCkUAjZxUXde0hEnZO3Wpo8ib5uFTExxtc1IqKX9NvHh5428XfU1MjwTKl1y+VZYusQbQsjIj2s0wkSEpWEr/5iT+oKHnrkeUGQjpCv50QIMxtwlI7s2jCgwq+UQS2EnpjKEhRTQ4Y9XT7M64mt1le/UJY8PmbY7gpSm7dcdkd+Xg7fLKY4vi7juxLyr3b6Pjo666DbTrQGCBkLWA9SQ5GUhEAjZUz6PQMieEt1Q9kdAoJIO6B+h7lBlQOAjITB0nAwWso/0oF57tUMV7bW393nlD4TsefEOtQUEAgIBgcdHYOg4GQjZ4z+Dd1HiUEW7NyjMlO0JXmkUesF+uWRTIo9pcjaq0E74cn61VFq79WSHVGe9ir2jlst+iWUd/0hPrHBSV13X1N37H7nxcU5Z3L+SjpxP1bZmtdWlGc6hmarPFeXXo1IPnyLyh+9QpT7JUevm354aRYgC+65L5xuhNOxMznMecoFCL1zKo7UtljPDs1/0v7N+Ue6HBcV2MVeL+mt44juO5/5UeluXy1ZY2lPLT9POoeNkIGRP8xzefKntitbhN2H4A3icl6Vfh8vJ9bLHIuVbGEk/FX0ZeSsh6/RV0MmfQzbmiyD9VFwDJ22gnZv70TGfkRnye29+Tupk1eVtB5ffj4GUXVdR1zX9/n2PLZlFMU6nXlpUcU/cnP5HUn88y/MpDlgxowURwk8sSUDbL217+t24yJe7XcDttMNyPmkErKWFGK6Ano1nzXBzPSMiGVacL+1R+XVAS9T6TJrp+lzx4dDMq0imW1aX/rhwaJbcuFL7wLnKvOG0W4pt2XS/wZm5lZNW6PUot3HT0k/mWMuFQFpa85AvHppQnzArxE4iPIUbAzM3nal0bVg4+i6qU/pSufrVZlWtV9pWDct9IVsLMNqiUqr2yWtd4wk9A73f5vnscurzWhdE+V0hMNIUfHslKU//3/ZxsllOIGRNTMKVHgi0K1pbR9FduNGhNzoOHquKbVXD4k/J7ZNUnfWL56qqUZ6WqHHP1XlTellX8/55nSCa8m2TtJJpHSYPxrhW4V/N+1oHagfhZJ1osy6Z39Ve85pWtgjdwPfEkx2ZbI8skX7bO8F+HS7fdqUORyGKN56vuHZ/3HSZ6ViE8aij2jbbxuLOxVNsLDYkt6qRcdbskvVzVxtc18Bit9HeoXYo9RtOa7XFFpGw/UIb3HXH7zTFumy2o2s1qa0DtDWMHo+LrdwUxNUpu95g/Vj/GKDtebSYYDRwectqvF+qTUrWe+Cgy2Z/uMh79SDcfI8uG4pCv26S49sJ66ketoMXxsLc3OUQO0PJGlgg3vwuxnxvP2uZRK6I5KFjWJiXeFpvuaYw4OlNwkNBgXlwapVO4SdraPtV+TyrGvXnaulf6ggXY8pn6m5r/6DVw/Y9rsOddH1MitYxPeJBspvtVf2WWfYaq0LsC1rrQjN33X+wXQxc94ddax8nm2UFQtbEJFzpgUC7og3rKOzqjA7d7sTZJrkT8BiqfPDlA76q0+h4moVjKjo2+1ZzgLM67/pLSsajsu5TP0dxx5Ich4tuuahwu1AcLytQY0MAccHZYTTr4qn5dTu2E8NA36zdVxe7rrBrTVbf9MlSJ2AH1bHAXUQbzec41AHmRPyyOgQKz/MouFFRbKPvCIpwymelZGvTj7Z7qgRPG1xBRtm+jn0IGS/9RluCUQgQGe4kNyOj83YJa4D9bugCNqwP7c/MeOesctpOXflc11gZDXmV3jVwp43Fl3MwSxJZng4X7xZCpnyedtbvVPM+I1gDCBmXtbGNBRPjKAd9Uyh+xvou+QFJl/jHpfxgaWCgl6Fhp9Ip/PSk/LhJbBlBkTFbtPKaeR1XavzUPe9zVkk6jsTH02xn7JfK2tfSd912M7bPscSto5L6do2boy0yEYulGI8wsoPiygQDf9vHyWZhgZA1MQlXeiDQrmi8o1CDovn15N+yhVdsvOh2x8ECxKpQCywt21RbdU71i+dqh12enqZxr9l50xeUjCFWlK77wPW0w2apT1vOkC832J1c2+zoAohjZ4fhrgunFZI4QXIntzjhZTgxEPvkyekSNT2nsHNI47jkkUVPedliNiICekZZiL0bRfON56vleTBuuLJ9IqNkibImgM22PYaFjAe93RjBj33tqi475HcRotoSwDd0blhlqiPy+A5qN4or29uTB1ZttqP5AaGB+aoJ2RnrJMJ0LTZpp3h2dzH0TbZ53+LyA/LoZv1Oue5XuOxy9vGg+iyySGkyaNBW5RJJnGF5lCSRbla4HJfIYtpIvFY+LRdA7+pdVECncpftxNz7VRImM6cRW0+92w6dqPO52lnfNMrTrvoPa/xUEqbvmnW0EStPTo+qLObReYOMkZ8EhYaZap+ZnJ1R4F6yQi7mzGLo3pzjiuNqyvb5pLiVclu1ulxHW0A7BWz4Ju30/KrTisk2WR1atr9yyGddah8nrcQAAiFrYhKu9ECgXdHaOgq7cN5x+KZSGoOO9TKpTkFZQuoXz66KzhukS0vUuOeSLcP6JNund3rNL9JGByWnAWr/Jq1u7ZB10nIj4NoqxwdxPRgtyJeOBq7tBbdSRNYXmxE3MeCBZ5PiyL5G+fScHDx4e+Rg1OxHbcd4vu1Rrpu9dPl3U76p8lYG/7zhtF0gjTO2WbxJXB4Ht+pyxGoywmiyRnlT0xapJ7r+w3zI+HOPogz6gKB00Y4vpesJB6r5fGhM/8iETA6uHfqpHrWwcEQOC6irrHLJ9uIsSvl8PO9s4z3s6zdFkjWxZvKyfmPELI/O6bRGnfIdV601jq4ltmvro2u9RasLotV3MWk1qyrTCUna1yWf+pU4sr5Dm9Jj6RR+hmzspIkDWyBxOeFYXlA12ks7Ky2Fn2OGLNM/omeYT+8Q3a1walZ0ryvU/7APNr3v2nIC7nw3yJ542rL+jtxV2GfxMkXMiLH5oUsWzyiRsvI9OGkaucbXmn690KzGKEHDp/N2xmE1A1kWMyOIeP8mt4+TzXICIWtiEq70QKBd0cwBvklMdAfMZsdhVG93HOcNUm0AYJaKnLZcUh2o74Vm5Wr+CySX6WdAnZAumyGJdiK3Mbmh3CyxkRvbaSnuf3jGJosxGsVqKyBWmIlTVa6QxhGS/ADZHdHm65MRJz0NDGwcDQuKws4tt33/iCKKsVRB9K1sVetq0GEBKK2i7dPqiCXtEmBE+dcT2bLr9+53TNMad/Eci3mMmOkeL8ckmgTwunbkJxKgf3SYeqd0rm3KsrEnouOZ6i0ydcDUHz0dHTdktxO4zln9NHibA5avrNPqjk01xfmxMf1oytptIfMZktoJGVnWTRzUghB743ZOduRHCrfqP/DjwfG8yiJGLBpjYuB/Jipdm247ZJ3lWC6XWK6PuDpkcT1ieY31s5qbgcJNJ26u46YVk/quJE6x2KldParzFvlkhu1ZJ9+i9usBeTZGTHnqjzx+j3ZImYzo3ZrXPn0KH55GnstfvguMmmWRbXyK3/ZxslljIGRNTMKVHggMUbT6RXCWa3aQRpLqhooc9w2fL9FZby6oaHuW2olWdU7t9Rk1aCuWzOvGmTaw2uTS/tpuc3SlvF2rdS5b8kFb4cTarZvybZyuOJ99zsNAcwk9OdjTtKbII7a24Q7ICjuj3fWJdf+yw2zMHYzrJI0Dx4AgLYTs1+yoH4Kb3AMezGpRqNVykzmKzR6nC21VRX8umeyBWAwqPisRRaEXVkkw/zC1AbOPiDSgab3gXznZCHHgHFQp/4XtpWq+B7b+SCF4OAjSu0gbcOVd/y/fXzVZlqho6ilKUBy5bjlxYAtwMmyO9D5LvVZ6ZcrKzCGP50NGQWmPtBVbk5D52/cUd6SVmiw8tM5niylNUYvpOhsDJ46DPqRa2jAo7IW9UIaX2/XO1is6HWLU76zjXh2eor53w/mkTQ839L7C9Xyp/dBaLWREfnVLpaNvNz+WFMlceT9Aa0EbB0PGScocCFkDwnChDwJDFM3uaMzy+UChWw8k6ZnM1yhPNiGjjmyHOa3uikbIVnwKTq18dHxhmRUaZ+2yGUkdJ75BzpHU+jJ3pjjkSCI5jUjNnGEUpygOZAPz1HU7YZtPkFLYBkZ0aC/NFLPiAPX9yWtjX6EZdxgfjxPM1nLzcjUwuuTSsXXfb7mqd36aFbMlh3XL024rFa47TOME+e6Mq3ThIb+QFRFc3afMytjo3K372in5gtEUcbLY19gSGZnGMYiYeEmN2HdT6rX8bUyRUF1eeRzPiKWNMTaIborJvMDqcLE+NprvmQz5MVuVOA0hZLcTNrMR4lThemW6O8Jkc27icDtimXKM6NGwj444w6rcoxCbtTffQ2mF1h4AO3TgYCQx28nbOMVm3U7InGEsUnfYC0ZEfITdkMVxQtPpU9rXNsV4nCHXrEQ2BoyQGVOWKdJMWy3Y630iKz4tjuDvPdM96ism8wGWff4syot8sRztsi7ZbbFu1/2Zq99nhMhvAm15R2Qt9/Ahk1kNwqtdvOfhkHGSqgiE7J5Av/dsQxSt++VsQdM7QNl5VEc9pL5eaTX/EzmY6r+61cvZiWoDpp7WaAE5q6YTrK3pT5qGnNNm605CdsEmizDVHZlYobQR+x3U1JBRk+NEYcdvSh8f+XVoTd9o7Wn7CmZl9SRk98ZNtMZnTegkk731iyq64lSe6y9xUTVAllzGpRxWJukXJi2TKhPOGz3sRd0QTKMmyfL5wtXFOQ566XZdrSU76eMowqiQ04taBdURK30zd3nregKtW7GfBU1R0QeCnFqn5Nfjjvn6+QmZrZOyEoJbX8Gsrrce1XrYJPhskcddgeYjOrNQNXxRhSrdbp+687Cjvs9LpfNjJCXhIT02OFtcin1I6D6QDkuR3sfpx9LHtM1C1kmqnP0Zl1q1T7bC+h30zqq8dbm1Lqh7+lGdTr94z+Mh4yRVEQjZPYF+79m8inaPF7sVy94vn+qcvC/UfWXreIF1+Z+qs35eQqa36IHHBnbqGdmlPhg3zUJWT+Q+soXMltk+d7bhXoTMNR3sx86WQ5573wOZQPttyn7GZpZidg9n5mZZWkXGoWpTU1Z1z8hy35NaDwMhIwgbhOweuPZ/zq7Cm89BpjJ1weVmID8Urd8Oq2Vdbq0Lskbzt05nXr7XmXec9JQWCJkHmHC5HYGhitZeWstd5oSvnJ5bUta3mv5T9a37HXQROa0jaPtqZF+ZWtphwng6sAFTlv767FWU/pT979hWNqvztBZPPApuTh+yjmXr99AvHwbeAWrQlCXtQmFjpc6ldcIng359yMDyoGk4vVKfpdBKw0+5jlCbmrJ26M/Q9+h2xvFI1k33ezR4yrLlGXVajZ1YuPw+PQnr7YYUfp6UzKp7XD90ytJdeuc7a+zIYpfRQbSGKLpddNf5ZY/lUrkd2MnZgoVHqn/oOBkImf00wnkvBIYqWq9CQ6JWBNiydWvqoTVDuBkQeC0I1CTjtQgc5AwIdCMwdJwMhKwb05DCgcBQRXMUES4FBAICAYGAQEDgzSIwdJwMhOzNqsLTNmyoohnS0NfwLZh6DEzCSUAgIBAQCAi8KQSGjpMfnZBVh5wFC5zu5BqcCuUqRRyNMG/s3vqmntWrbsxQRSOndNo82b/5cIVLucU6n4nwDRPMiw0OLbG2XjWAQfiAQEAgIBAQeNMIDB0nPzohY46wtGeYTcjiMeb1tTf9zF5l47oVzV4lpZxpm87PF+xmI4xma6hYNxWubKsMFXjzVQIVhA4IBAQCAgGBd4lA9zhpwvIIhOyCQzEFRSyPKH7OtMChjkjJB+UozbGSaWLa7JMHpGQrayLKJ//x7VjkdUXSANoiYcqCgdI2CVMUqhK+QieaYL1bY8b2qYowynKweJrU3uqMbZ4JGUdIZgX2dmAWE5dw1oFAt6L1J2Q8DpDce8yqmG3Ia+6VZ6UIpwGBgEBAICAQEHhxCHSPk6bIDyRkN+znFDE9wmiSY7nIEBO5ihfYs2BAgpBFnETlxZztv0fpiWxReIJ5xskYkSTaZ2t/EUugNasZbYXAyh1NkC8XyAT5W/BKBCGjeseY5gXmKZcpWZ9Za2m/MCKL6WKD3bbAZJxipe0ub0ISzvog0K1o/QkZ2+oi3YA/Lat2tnflXOiTdS+cBgQCAgGBgEBA4IUi0D1OmoI/jJCxwZKIkLRgVDgsOMHi0Y0lIZtiK6xmLHIwkbbZjkVudlnDzGtnbFIqk/bh447g1WHBrWrZhm1hItPXO7KzfcsiRFOqg/YP4zLNwhSo+fQfcNapaBSok0j1tmS+YxSDS+772JiyZPsDxphuTmYUdNqiZRojKcTebw+QN2QNCAQEAgIBgYDAcyLQOU5awjyIkF13U06MGPHhJUufsGixx03stRVFU9RciEXJJRK3BO3VKcmUPj1pXBMDu1EGi95OJItb4oz0JIasQ8h1Y/uscVI2yuZYH+UCAguNcNobgS5Fu+1mBmnWI803CBnVeiuxTCPE8RgJC7qYYESWUKZHvcUKCQMCAYGAQEAgIPAiEOgaJ20hn4yQxV2ELFmzKaoGmbJJWhshE1OjjTIsQkaNvp0PWE3HfOozihGsZbYqDDtvVTRh8cr3BxR30im/zamf6lb3pSTsubZtMisTht+AQEAgIBAQCAi8MARax0mHrA8iZJBTllH3lKW0kJ3XKbOccMKmWci2ymplEiw5ZRl1TlnWVjabkFUq5tVtL6Y7xZSpA5NwqQcCfkW7YDsbIRbWyaoskEQJFrsj26h3ur02Nh/m1QVC1gP2kCQgEBAICAQEXgkC/nHS3YCHETIop/44Wyin/pHDqT+doyhmSJjTv3KqZw7ddI0c9vMcuzan/jjDonbqH8F26ncTsgu20whxOsd6d8BmkTBCmKxObkTC1V4IeBXtssN8voG+iPV22uFw5s/hwYSsOqIYRRgVRyia3UvkkCggEBAICAQEAgLPhoB3nPRI8EBCRqXqYS94SAkVkUI69SeYTCnYK622zJDTUsr674LdXEwljhIUh5vTr0wPezFKZo6wF1osM8tCdj2uMEt5HfE4xSTfGoShFiUc9EZgqKLpU5K6D1n3BrVyY2W5wfgZm1mK2ca5JrO3/CFhQCAgEBAICAQEnhKBoePkIxCytuZIQqY59bclD5jDgocAABX1SURBVPdeDQJDFa07Uv+raXoQNCAQEAgIBAQCAp0IDB0nAyHrhDQkcCEwVNGMMsJelgYc4SQgEBAICAQE3h4CQ8fJQMjeng48S4uGKtqzCBUqCQgEBAICAYGAwAtBYOg4+cSE7IWgEsR4dASGKtqjCxAKfDkI/MMv8cm3/hH//nIkejpJfvuP+Manv8TnT1fD05Z832d133yP2ZqPLcMf/hO33/+fx2xRKOuNIzB0nAyE7I0rxFM1b6iiPZUcr7Xc6pe/wjcmH/Dlr3+GP/3BF/jJn32Gr/7Vv77C5vwRv/pLaoMKW3O/RvwOP/z6B/zJn31h7tbQs7B//5uf40vPQJRuf1vik+/85tFX+D6P/F3P6r/wqx+U+PLkA74y+Tl+/FsJflc+ma7f7+2nn+NPPv0MP/ynful5qqYM1T/9M767+Ax/8ukHfGnyGebf+2d8wfjS/8KPv/UBX/revwypoDPt0z2jh+l+Lfgf/hc+/GWJr0w+4EuffsCXv/kL/PiXbA/DOsnQgy9+8Bk+mXyOv//90JwhPSEwdJwMhCzozb0QGKpo96rkwZl+h+9/7QO++w8PLuiRC7jhJ9/+gC995wsxsD/NAPLIQnuKI4w/w48GDa6uov4TX/ztCT/62f923ey89nSDpV41f25//tP/0i/e47ipl88jv/msiEgbpOX3v8bi0w9otq8j31AErr/Bj/5Kkqe+mU0ZqvKX+MqnH/CNH/wOtz8A+P3v8OM/+zm++7P/BPA079PTPaOhut/UH+Df8ePFB3zyzV/iw2//C/jDf+BXf/M5vvLpZ/juPxAmff6a5RLp/eEPfvM+rN99IBqYZug4GQjZQIBDco7AUEX7OLj9C7776UskZPaAYZ9/HLTuVes//Qpf/fqv8MW9Mj9epqcbLDUZ//AF/vzTz/H3RAAe9NfUy2eR33pWn3/PsiKx6VjH+9KV70FY9MxsyHDFj775AZ/UHzR2GU/zPj3LM7Kb4jxv6k/1s1/gk091qyZl/C98/r3P8KVv/gq1sdNZnrzYLFfeCb/3Q2DoOBkI2f1wfve5fIpmd1pk8v7SpyX+TsxoVT/9HF/69Bf4IDr/v6ANTemPOlz64v0bYSFhg98HfPdnv2Gkyr7+yV/I6Yh/w4++/gH//a8tD6brr/HnX/+ATz79gD/52mf46tc/x09Ihj9c8fffK/GVr33G/s3/Un39Mdm/ecLf/eBz/Kkw+3/jr/5VWLH+iN/+9BeYi3xf+dbn+GEppgP+z7/h72SZkw/402+f8OH6R9Ew0cn9NZfnS5/+//h/vvMZviymWbhc9gDyR/z7z05YfJNk/IAvf63Ed//237gcvXGT9Qsx6GeQnL/ABwBV+SssvvmBTWPRNAj9q7EEQM/3v/3l70Ql/4nf/vSExbcI78/wJzSNVOPH2/jV75zYNNNXv0bPpVQYDrVqXH+D79N01YTq+jm+8U3SM8236/ob/PDbP2fP+MvGdBaJ+kfcZLvY8yzx3f0VFVqeMWFBuvvtX4sp1bb2/Af+bqFbQKnOf2XW2m/88H869ZK/N5/j+07d6/fs/vyvv8B3vyV0/uu/wE9+a+qAelb/gZ8YOvgZ8v/XkuvPfl1bRVrz7f838A+/ZO/0j/e/4O/N936D6jdfMPxJD2gK7SuLE34l3a9YekkeOI7//a/+GT/69s/Z9OMnE9IL06qjZKBd1v4Z36APrZ+Z7RNKqCxkf3byl/n7f2X3eD/wc/z534j3iwoR90hv6P78e7/Gb/8AmH3bfzAr9yeLf1T3vv45vv+90tB9NWFIVjDZf9B04uf4YW0Ntt5/geeP9r9gbg30zv3pd37NSZWzX/sjPhC5dvlxkt9dTdR4XzT/nnhHyWXim7/A3/3mD4CzXLvNhH3Le9UmN+H62y/Yuy8xX/zgX+7lnqCe88s/8o2TPskDIfMhE663IuBVNEas5BQW72iIFMlpkM//4oMY1DiR+uoP/o3VQ50dpaun8UrqSD7HT37/X/j772jXWQfzAZ9IqwzrnD/D93/pELfxxS860W+LweYPV/zoWx9q/yfe4X7An/7lv+D2hz+CfF0+kWRSTOfkP6WB4o+4/eZ3+C0bYMyOGbjhA32Vfu2EXzGReCdIpPRH5M/xB5rusjpg+/yXJz7V8DPRnf+GHMk/YLH/DwB9cbPxuIecos2LvyWS/Ef8+98KPGp/ErJUfIbvl3Jg/CO++Ok/41e/Fyak/9ve+ftGkWRx/P4fz/RMzw//XHt1R0S0kSMiCNaBhWQJEbCJE4JDSJYQARdsgoQIICGBACGdTtoAgrU1li0jG05YrBDIiGMtVrsywTu9V1Xdr6t7ZrrM2B57vkjD9HTXj1efel31+tWr9qttWrjk+sa2mZdUxFj9Su8erxu+Up7PxJdf/T54S3cWV+nC7V07oO/Txh1tkJmll+T6wZ5ZznHGlM937z1ts0xd+5jr/pNWV1bp2lPuA/7Xuz3mwYP11ybfeknzrEv8O6eXbuLrontUsu8WN+jZLi9XfaAH7EFKHlpYBr+vCngXyFUqn0zEq3RueVv6/q+DA6JXb+jZlmP1QTxaieFeYJDNXurQvV85/b6535dfJwZhToZMfss382Xb1rVMox8LP9uHrc+7dGtxlW6+4HvT151PtL1lHhJTg8zq7uIWbVgj01zjJVP7QGlZOn3Z/7e5d56w8cPjxwtecuV7g+8dry8szws3XtObg6/ysLpwaZWSh9dcP3n5NYtMWjMWzV/fsbF2X4wHzY1VmbSmkLTN/Ntn491XPeW2YRo3bJjG5z3a3s0a3Vrss3LcdZ7s0kAYZF3A4HRvAt0Vzdy04tGSCW6Dbq6s0TkJsDXGxJXHn6RwiWGRgdfcrMsrmzRvvRz8ROye+Hgwc94PzjO/sknL9qnPTHzKM6LF9gcY+e2MRZPw3cNO4vXIDj7+xGk8HBdW3tA7HiTdP2uApkYJP2G/pmuJEWoGQWd4mmz+AKp/WwN0+bV6ejQBzbPWCC3LzYko34eRU4xi58lIeWQnhiL2X+mvP/5Hb9a3hYPpb91GK5n1gpo/gVZwPdOA9Ifpc2vc2NO674qu0zobRNz3lm+RN8F6sXJ9LHVwPyoW/iTKaXR7Dnbp5qXUgHN9Jia2r5fkDDLFUqcp2XfuvmJRZDlSt1HKU+UXya/rtFyN8dgnn0zE2fvKZaeDL/Tx1Vt6wDGTTp6MQZXvd92XUo4veyZ/UpM66FOmPNTppWfjYWKD0eiO7ue0WCfX81/ZmOI06TjgrqW7b62eyUOAGffmf9abdrQX1ZM31z4zhiT9m+snL38qsmf8e+VwOqdb/ECbKzerl73vK7KeUs1O12fHsMVNes4xbiPyr/s8WQwABlkxF5ztQ6C7oln3Ocd38MB3dYfevdg0HiMx0NTAzec5JucPO9nt8vIkPzUaAy0xYt7t0GWZTM3Ac+vFHj25yh6jfdnhl3jVfJn9AUYGOrOzkZdSko9dnskNqn5+DhxeMcsqCyv2KTM3eLIQeiDSx05AfwDVv/WxS58dGKkstzR7wWBZQk7ribpyl5dzvtCbux2aZa+AdYAxL2Nou4p4qXVLllkWrm/So6d9DLIMp+J2u5L1d66fPIOm6Ho62fSpp6iPuXLWZWdQiDBF5ei+thOQGNZmQnbeklSWtFU5mbXuHULHfIMs31cF8us6rWil8hXJx0vKP/G9tkG37v/3mwyynAxllyzVLkvN1xyv0gU9BvDx3d+8Zcm0f/jI5WNPvvH6px4eXb7LlfaB1gt3VRvNXl/keHr5C/pJVh54rE2LN0cyVriQEa8cTqHL0se2HN0ufZxUo/P0k5v2afuxWdY+v7RJT16l/JLyzthB93myuKEwyIq54GwfAr0UTTxaiy/pEXuzOL5IDLEOPXloDTNXtjXQ7t1/SfPiNjeG2JWHO9YwcwmN1235/jYtyzImiSF27vaOLIW4PzLvUifferDgk9awK1zeTAZc5Q3w87uC7RKHvP7APWEmy3bFHrLk6VbK8AbgjLeiv4fM8Fyj/tycwOpJOEjOL7L8emGpI8brtZVtFRtn+sotRUtNu2aJ0ux04zN6AvDbrCYDiQUquK7E14fmSV0tB3p91/tJvoivLt0e6z7m7es/r1HygCBJCuR1+uJim4RHh578wvGRyqPn0qndv7nJTqc5hI6lxgALW9BXGZ2zbdZ1yqmS+XITsQm6n+dYMlt0Rp5M+jzHLIsiGboF9f9Jf8nDQp8y7QNNspxsZeSvQt2x141cHOe6R+JZX9yk1cySpRo7Er67SZjB0XnIiLoF9fMyexrUr+9H2yjxFlqvVq7/nRFq2lXIJvE89/OQKcgurINjie3Dnb56lo57zZNF7YRBVkQF5/oS6Klo9sY+/yPv2EpjJNgjNXubByj3zwycnM55WmTQ46dVvlldMvoqSzDnOJ3zUvBAIum0i9x4zsQrx3ntEpKJ++ITfiwOh9Z8sLFg2cFHqtYD1B+/0bOn7+0y4hd6fsPFwn2SAG4X3JvGkDlPUsEgmAzWbmNCdgIxW/rX6JYXQ5YadeW48SDN72i6JbExh5BTPBGab9Ih1sjWyz5EJHFS/H4p7nMOnH9JHPti5LZtvP6aPsog7OJXHKcsA1VT/lAM+VW68tDuFDnYo2fXVQyZH2PmYshsXFKO7+ff6NmLT0Rd+5gfCJRnVyTq1x5OZAwH1u9ZvQSd08s+ukfhfZcxgISX11dkDVMX08Pian3n32XzZQwszmgeoC7eNfGhHCR/7+ohlywLZSDxvmdee8E6cGONzsl77PK6lDXyjHxJjKHEhO6ZmDVfd2ifNp6ajT+FZdx5K0anucaeKGNh8OoAx4jdWTfLcx9tvKQfQ2aue/LmeHpjSIH+FL724j7HaOrXXphyLt5xwfT2VRkuXq+g3EybfTbefWU2d+jxQsv9hbafcnypWeYteqhizTlr/3rOkwWNhUFWAAWn+hPorWhmiUaWI+0TkMSESWB6uu+Ia3HnE0+LeAOcsZPKYW7g1dRLYQePNHie0/5Oz2936HIS7G1fdMkvSVxaowccJ+F2Wf64Shd5h91PG/Royw2a3stF9QT1+b3spJQ8smvMBadnd8Dx9aJdlqkxxXJ6AzB7eJ6u0/lLHfs+L7XLknep6V2WFkkpbq926NrSOj0SA+lwcspuQdlZ6ZZ4zY5P3vCQ7jh0/fQ7rd7uyK66i1c7xLu5eKdrxiDj88nOO44ncXE4eSau1KJvt/tTdln+tEl3VpRBxhnUbjCzU06/98osrV5b4h2slu/j97TfrY9ZJ13gcyKMlbdre0xCfpEs75LLenHzepmZ+Dir1j3+rXbIltExbZDJveM2NCTyE7mXEwvDu+9zdZbOlzMgSILWL/MO4aUOLSxv0j3uH/UwxUH85sWz+X7XLLrJIAb/VvbFsJdXduyE37tMQaB2UjLPi8usixaO3cGb7rJkQyJvNDvDngPzjcwdury8RryD+Ae+Z3/RO7/VLkvW/767LB0flkkbNvw7rz8ied8Xw5py5q+u04K9Bxeub6e7XwvK1X0hdfS6r3J6oOXep23eZSrjmeFzz22AkILP5n+958l8m2GQ5ZngTAkCoYpWokgkGTYCvBtRLcvIJCgbLDr0r3/6RkY/4fOTZL8cw3Kdjd9kh2AiVMn2yJJQdnk1KeJYDownLGsQlqn4sPnKlF02zTDIUE7WnOFSLtsxp9IG0jFXPaLVhc6TMMhGVFG+tdndFO2HJeMRcO+swrd5d9dp48D6YWIB0ziZdDl2k/7z61vx/JRtl+8VLJtvWNOVaU+yjGT/1NKwtgVyfds9yveKNsiGlaf2tA2LjBIq8K2T0RDn7zZPdhMZBlk3Mjjfk0CoovUsDBeHlMBH8xJdXt6VpQb99wJDRS7pUQot9sTS927Pm/sd4tixZDfuicmJio+DgDbIjqO+w9UBD9nhuB0+V+g8CYPs8KxHOmeooo00LDQeBEAABEBg5AiEzpMwyEZORQbT4FBFG0ytKAUEQAAEQAAETgeB0HkSBtnp6NehkzJU0YauARAIBEAABEAABI6QQOg8CYPsCDvjLBcdqmhnmQXaBgIgAAIgAAI+gdB5EgaZTxC/SxEIVbRShSIRCIAACIAACJwRAqHzJAyyM9Lxx92MUEU7bvlQHwiAAAiAAAicJIHQeRIG2Un21imuO1TRTnFTIToIgAAIgAAIBBMInSdhkAUjRgYmEKpooAYCIAACIAACo0QgdJ6EQTZK2jHAtoYq2gCrRlEgAAIgAAIgMPQEQudJGGRD36XDKWCoog1nKyAVCIAACIAACBwNgdB5EgbZ0fTDmS81VNHOPBA0EARAAARAAAQUgdB5EgaZgofD8gRCFa18yUgJAiAAAiAAAqefQOg8CYPs9Pf5ibQgVNFOREhUCgIgAAIgAAInRCB0noRBdkIdddqrDVW0095eyA8CIAACIAACIQRC50kYZCF0kTYhEKpoSUYcgAAIgAAIgMAIEAidJ2GQjYBSHEUTQxXtKGRAmSAAAiAAAiAwrARC50kYZMPak0MuV6iiDXlzIB4IgAAIgAAIDJRA6DwJg2yg+EensFBFGx0yaCkIgAAIgAAIhP9Fm9IGGU/A+IABdAA6AB2ADkAHoAPQgXI6EGKYljLIQgpEWhAAARAAARAAARAAgTACMMjCeCE1CIAACIAACIAACAycAAyygSNFgSAAAiAAAiAAAiAQRgAGWRgvpAYBEAABEAABEACBgROAQTZwpCgQBEAABEAABEAABMIIwCAL44XUIAACIAACIAACIDBwAjDIBo4UBYIACIAACIAACIBAGAEYZGG8kBoEQAAEQAAEQAAEBk4ABtnAkaJAEAABEAABEAABEAgj8LdqtUrH9YmiiPp9jksW1HN8/X5Y1vV6nWq1mugMl1GpVOTjyvN1yZ0/im+/Lv4dUk9R/ijiPqgQf8dxner1mvweGxujSmVMfvM5/tRqXF8lSW9+mz4sLjsSdszQXY/jmPjDTEX2KKIal891x3Wq1SOKahHV4xrFjZiiWoWiWpVqdf5EVIvNMZ+v1sbkeqU6RtVojOqNiOJmTdLU44garbq5Hpl0zXZM45Mtao83qT3RpMmpcZqamaSJqXFqTTSp0YopbtUpbtapFkdU5brrVao3anKejxvtWNLzd3O8Qc2JBrWn+NOkxnhMUVylscjIwzIyI2bLHz6u1aoiK8vMH65vcnpCyuLjqFml5mST2jNtak035dOeaVJrpkHN6Zha0zGNzzRpYrZFk3MtmppVn7kWTc42aXwmpnFJF1N7uk4TMw2a+M58puZaNP19W9LE7SpV6mNUb1WlDZPftWlqdpym58Zp5vsJ+u7vkzQ116b2dEzNyRpx+omZFs39Y5qm5yZp5vspmpyZEF7VqEKVanpvcFvjuEaNZp1i7otGRM0W93Ek7Wa2jXaDWu2G9O9YxfCoRlW5PlapUJXHaubndKXKDFkPre5UmWt6D+h70+mb/x1yv5RJq+svkx5phn/MH6U+Kro//g8tN2swEYUS0gAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "0fe12ba4",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e47f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "2/2 [==============================] - 1s 218ms/step - loss: 2.0482 - accuracy: 0.7259 - val_loss: 1.3696 - val_accuracy: 0.7547\n",
      "Epoch 2/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 1.5577 - accuracy: 0.7259 - val_loss: 0.9768 - val_accuracy: 0.7547\n",
      "Epoch 3/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.0864 - accuracy: 0.7259 - val_loss: 0.6556 - val_accuracy: 0.7547\n",
      "Epoch 4/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7075 - accuracy: 0.7243 - val_loss: 0.7205 - val_accuracy: 0.6429\n",
      "Epoch 5/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7634 - accuracy: 0.5865 - val_loss: 0.8499 - val_accuracy: 0.4472\n",
      "Epoch 6/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.7918 - accuracy: 0.5268 - val_loss: 0.5612 - val_accuracy: 0.7236\n",
      "Epoch 7/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5716 - accuracy: 0.7136 - val_loss: 0.4968 - val_accuracy: 0.7516\n",
      "Epoch 8/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5545 - accuracy: 0.7228 - val_loss: 0.5096 - val_accuracy: 0.7578\n",
      "Epoch 9/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.5838 - accuracy: 0.7243 - val_loss: 0.5104 - val_accuracy: 0.7578\n",
      "Epoch 10/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5876 - accuracy: 0.7243 - val_loss: 0.4879 - val_accuracy: 0.7578\n",
      "Epoch 11/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5596 - accuracy: 0.7274 - val_loss: 0.4497 - val_accuracy: 0.7609\n",
      "Epoch 12/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5128 - accuracy: 0.7320 - val_loss: 0.4129 - val_accuracy: 0.7609\n",
      "Epoch 13/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4633 - accuracy: 0.7443 - val_loss: 0.4008 - val_accuracy: 0.7764\n",
      "Epoch 14/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.4353 - accuracy: 0.7810 - val_loss: 0.4156 - val_accuracy: 0.7702\n",
      "Epoch 15/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4326 - accuracy: 0.7902 - val_loss: 0.4201 - val_accuracy: 0.7857\n",
      "Epoch 16/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.4281 - accuracy: 0.8193 - val_loss: 0.3954 - val_accuracy: 0.8106\n",
      "Epoch 17/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4088 - accuracy: 0.8346 - val_loss: 0.3660 - val_accuracy: 0.8230\n",
      "Epoch 18/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3899 - accuracy: 0.8423 - val_loss: 0.3474 - val_accuracy: 0.8478\n",
      "Epoch 19/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3813 - accuracy: 0.8545 - val_loss: 0.3369 - val_accuracy: 0.8540\n",
      "Epoch 20/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3763 - accuracy: 0.8576 - val_loss: 0.3289 - val_accuracy: 0.8602\n",
      "Epoch 21/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3710 - accuracy: 0.8591 - val_loss: 0.3216 - val_accuracy: 0.8634\n",
      "Epoch 22/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3605 - accuracy: 0.8760 - val_loss: 0.3175 - val_accuracy: 0.8789\n",
      "Epoch 23/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3501 - accuracy: 0.8851 - val_loss: 0.3187 - val_accuracy: 0.8851\n",
      "Epoch 24/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3453 - accuracy: 0.9020 - val_loss: 0.3209 - val_accuracy: 0.8882\n",
      "Epoch 25/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3422 - accuracy: 0.9020 - val_loss: 0.3151 - val_accuracy: 0.8944\n",
      "Epoch 26/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3365 - accuracy: 0.9035 - val_loss: 0.3033 - val_accuracy: 0.8944\n",
      "Epoch 27/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3287 - accuracy: 0.9051 - val_loss: 0.2917 - val_accuracy: 0.8944\n",
      "Epoch 28/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.3232 - accuracy: 0.9066 - val_loss: 0.2836 - val_accuracy: 0.8913\n",
      "Epoch 29/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3194 - accuracy: 0.9066 - val_loss: 0.2780 - val_accuracy: 0.8913\n",
      "Epoch 30/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3165 - accuracy: 0.9066 - val_loss: 0.2736 - val_accuracy: 0.8975\n",
      "Epoch 31/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3128 - accuracy: 0.9066 - val_loss: 0.2703 - val_accuracy: 0.8975\n",
      "Epoch 32/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3079 - accuracy: 0.9081 - val_loss: 0.2690 - val_accuracy: 0.9037\n",
      "Epoch 33/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3036 - accuracy: 0.9096 - val_loss: 0.2697 - val_accuracy: 0.9099\n",
      "Epoch 34/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3011 - accuracy: 0.9112 - val_loss: 0.2690 - val_accuracy: 0.9068\n",
      "Epoch 35/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2985 - accuracy: 0.9142 - val_loss: 0.2655 - val_accuracy: 0.9099\n",
      "Epoch 36/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2957 - accuracy: 0.9142 - val_loss: 0.2614 - val_accuracy: 0.9130\n",
      "Epoch 37/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2925 - accuracy: 0.9158 - val_loss: 0.2554 - val_accuracy: 0.9099\n",
      "Epoch 38/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2910 - accuracy: 0.9127 - val_loss: 0.2507 - val_accuracy: 0.9068\n",
      "Epoch 39/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2877 - accuracy: 0.9127 - val_loss: 0.2483 - val_accuracy: 0.9099\n",
      "Epoch 40/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2853 - accuracy: 0.9127 - val_loss: 0.2459 - val_accuracy: 0.9099\n",
      "Epoch 41/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2831 - accuracy: 0.9127 - val_loss: 0.2440 - val_accuracy: 0.9130\n",
      "Epoch 42/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2810 - accuracy: 0.9142 - val_loss: 0.2423 - val_accuracy: 0.9130\n",
      "Epoch 43/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2786 - accuracy: 0.9142 - val_loss: 0.2418 - val_accuracy: 0.9161\n",
      "Epoch 44/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2764 - accuracy: 0.9173 - val_loss: 0.2411 - val_accuracy: 0.9161\n",
      "Epoch 45/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2748 - accuracy: 0.9158 - val_loss: 0.2395 - val_accuracy: 0.9161\n",
      "Epoch 46/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2731 - accuracy: 0.9158 - val_loss: 0.2372 - val_accuracy: 0.9161\n",
      "Epoch 47/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2708 - accuracy: 0.9158 - val_loss: 0.2358 - val_accuracy: 0.9193\n",
      "Epoch 48/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2688 - accuracy: 0.9158 - val_loss: 0.2333 - val_accuracy: 0.9161\n",
      "Epoch 49/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2660 - accuracy: 0.9173 - val_loss: 0.2293 - val_accuracy: 0.9099\n",
      "Epoch 50/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2636 - accuracy: 0.9188 - val_loss: 0.2257 - val_accuracy: 0.9130\n",
      "Epoch 51/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2618 - accuracy: 0.9188 - val_loss: 0.2232 - val_accuracy: 0.9068\n",
      "Epoch 52/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2609 - accuracy: 0.9219 - val_loss: 0.2225 - val_accuracy: 0.9037\n",
      "Epoch 53/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2592 - accuracy: 0.9234 - val_loss: 0.2238 - val_accuracy: 0.9068\n",
      "Epoch 54/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2585 - accuracy: 0.9204 - val_loss: 0.2249 - val_accuracy: 0.9068\n",
      "Epoch 55/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2581 - accuracy: 0.9204 - val_loss: 0.2245 - val_accuracy: 0.9037\n",
      "Epoch 56/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2569 - accuracy: 0.9188 - val_loss: 0.2224 - val_accuracy: 0.9068\n",
      "Epoch 57/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2557 - accuracy: 0.9204 - val_loss: 0.2208 - val_accuracy: 0.9037\n",
      "Epoch 58/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2550 - accuracy: 0.9219 - val_loss: 0.2204 - val_accuracy: 0.9037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2540 - accuracy: 0.9234 - val_loss: 0.2214 - val_accuracy: 0.9037\n",
      "Epoch 60/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2535 - accuracy: 0.9204 - val_loss: 0.2212 - val_accuracy: 0.9037\n",
      "Epoch 61/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2524 - accuracy: 0.9204 - val_loss: 0.2193 - val_accuracy: 0.9037\n",
      "Epoch 62/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2516 - accuracy: 0.9234 - val_loss: 0.2175 - val_accuracy: 0.9068\n",
      "Epoch 63/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2511 - accuracy: 0.9234 - val_loss: 0.2162 - val_accuracy: 0.9068\n",
      "Epoch 64/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2508 - accuracy: 0.9234 - val_loss: 0.2153 - val_accuracy: 0.9068\n",
      "Epoch 65/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2505 - accuracy: 0.9234 - val_loss: 0.2145 - val_accuracy: 0.9099\n",
      "Epoch 66/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2503 - accuracy: 0.9219 - val_loss: 0.2145 - val_accuracy: 0.9099\n",
      "Epoch 67/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2499 - accuracy: 0.9219 - val_loss: 0.2153 - val_accuracy: 0.9099\n",
      "Epoch 68/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2486 - accuracy: 0.9204 - val_loss: 0.2179 - val_accuracy: 0.9068\n",
      "Epoch 69/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2472 - accuracy: 0.9234 - val_loss: 0.2210 - val_accuracy: 0.9099\n",
      "Epoch 70/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2468 - accuracy: 0.9234 - val_loss: 0.2223 - val_accuracy: 0.9099\n",
      "Epoch 71/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2462 - accuracy: 0.9250 - val_loss: 0.2211 - val_accuracy: 0.9099\n",
      "Epoch 72/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2454 - accuracy: 0.9234 - val_loss: 0.2191 - val_accuracy: 0.9068\n",
      "Epoch 73/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2445 - accuracy: 0.9234 - val_loss: 0.2169 - val_accuracy: 0.9068\n",
      "Epoch 74/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2437 - accuracy: 0.9234 - val_loss: 0.2152 - val_accuracy: 0.9099\n",
      "Epoch 75/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2438 - accuracy: 0.9219 - val_loss: 0.2144 - val_accuracy: 0.9099\n",
      "Epoch 76/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2436 - accuracy: 0.9219 - val_loss: 0.2146 - val_accuracy: 0.9099\n",
      "Epoch 77/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2424 - accuracy: 0.9219 - val_loss: 0.2146 - val_accuracy: 0.9099\n",
      "Epoch 78/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2419 - accuracy: 0.9234 - val_loss: 0.2155 - val_accuracy: 0.9068\n",
      "Epoch 79/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2406 - accuracy: 0.9234 - val_loss: 0.2158 - val_accuracy: 0.9130\n",
      "Epoch 80/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2399 - accuracy: 0.9250 - val_loss: 0.2166 - val_accuracy: 0.9099\n",
      "Epoch 81/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2396 - accuracy: 0.9250 - val_loss: 0.2176 - val_accuracy: 0.9099\n",
      "Epoch 82/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2397 - accuracy: 0.9219 - val_loss: 0.2189 - val_accuracy: 0.9068\n",
      "Epoch 83/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2389 - accuracy: 0.9219 - val_loss: 0.2157 - val_accuracy: 0.9099\n",
      "Epoch 84/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2378 - accuracy: 0.9234 - val_loss: 0.2118 - val_accuracy: 0.9130\n",
      "Epoch 85/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2371 - accuracy: 0.9265 - val_loss: 0.2094 - val_accuracy: 0.9068\n",
      "Epoch 86/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2372 - accuracy: 0.9250 - val_loss: 0.2082 - val_accuracy: 0.9130\n",
      "Epoch 87/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2377 - accuracy: 0.9219 - val_loss: 0.2077 - val_accuracy: 0.9161\n",
      "Epoch 88/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2379 - accuracy: 0.9219 - val_loss: 0.2080 - val_accuracy: 0.9161\n",
      "Epoch 89/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2365 - accuracy: 0.9219 - val_loss: 0.2090 - val_accuracy: 0.9099\n",
      "Epoch 90/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2347 - accuracy: 0.9250 - val_loss: 0.2121 - val_accuracy: 0.9130\n",
      "Epoch 91/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2341 - accuracy: 0.9265 - val_loss: 0.2168 - val_accuracy: 0.9099\n",
      "Epoch 92/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2349 - accuracy: 0.9234 - val_loss: 0.2187 - val_accuracy: 0.9099\n",
      "Epoch 93/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2351 - accuracy: 0.9219 - val_loss: 0.2162 - val_accuracy: 0.9099\n",
      "Epoch 94/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2336 - accuracy: 0.9219 - val_loss: 0.2107 - val_accuracy: 0.9099\n",
      "Epoch 95/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2334 - accuracy: 0.9265 - val_loss: 0.2076 - val_accuracy: 0.9130\n",
      "Epoch 96/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2321 - accuracy: 0.9250 - val_loss: 0.2074 - val_accuracy: 0.9099\n",
      "Epoch 97/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2316 - accuracy: 0.9250 - val_loss: 0.2082 - val_accuracy: 0.9130\n",
      "Epoch 98/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2311 - accuracy: 0.9250 - val_loss: 0.2097 - val_accuracy: 0.9099\n",
      "Epoch 99/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2306 - accuracy: 0.9265 - val_loss: 0.2106 - val_accuracy: 0.9130\n",
      "Epoch 100/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2305 - accuracy: 0.9265 - val_loss: 0.2117 - val_accuracy: 0.9130\n",
      "Epoch 101/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2308 - accuracy: 0.9265 - val_loss: 0.2124 - val_accuracy: 0.9130\n",
      "Epoch 102/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2299 - accuracy: 0.9265 - val_loss: 0.2095 - val_accuracy: 0.9130\n",
      "Epoch 103/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2288 - accuracy: 0.9265 - val_loss: 0.2068 - val_accuracy: 0.9130\n",
      "Epoch 104/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2289 - accuracy: 0.9265 - val_loss: 0.2057 - val_accuracy: 0.9130\n",
      "Epoch 105/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2283 - accuracy: 0.9265 - val_loss: 0.2068 - val_accuracy: 0.9099\n",
      "Epoch 106/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2277 - accuracy: 0.9265 - val_loss: 0.2059 - val_accuracy: 0.9161\n",
      "Epoch 107/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2270 - accuracy: 0.9265 - val_loss: 0.2040 - val_accuracy: 0.9130\n",
      "Epoch 108/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2268 - accuracy: 0.9265 - val_loss: 0.2033 - val_accuracy: 0.9161\n",
      "Epoch 109/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2265 - accuracy: 0.9265 - val_loss: 0.2045 - val_accuracy: 0.9161\n",
      "Epoch 110/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2264 - accuracy: 0.9265 - val_loss: 0.2059 - val_accuracy: 0.9161\n",
      "Epoch 111/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2260 - accuracy: 0.9265 - val_loss: 0.2064 - val_accuracy: 0.9161\n",
      "Epoch 112/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2257 - accuracy: 0.9265 - val_loss: 0.2075 - val_accuracy: 0.9130\n",
      "Epoch 113/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2254 - accuracy: 0.9280 - val_loss: 0.2088 - val_accuracy: 0.9161\n",
      "Epoch 114/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2250 - accuracy: 0.9280 - val_loss: 0.2066 - val_accuracy: 0.9161\n",
      "Epoch 115/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2240 - accuracy: 0.9280 - val_loss: 0.2031 - val_accuracy: 0.9130\n",
      "Epoch 116/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2232 - accuracy: 0.9280 - val_loss: 0.2006 - val_accuracy: 0.9130\n",
      "Epoch 117/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2235 - accuracy: 0.9280 - val_loss: 0.1997 - val_accuracy: 0.9130\n",
      "Epoch 118/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2233 - accuracy: 0.9280 - val_loss: 0.2000 - val_accuracy: 0.9130\n",
      "Epoch 119/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2225 - accuracy: 0.9280 - val_loss: 0.2003 - val_accuracy: 0.9130\n",
      "Epoch 120/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2218 - accuracy: 0.9280 - val_loss: 0.2002 - val_accuracy: 0.9161\n",
      "Epoch 121/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2212 - accuracy: 0.9280 - val_loss: 0.1998 - val_accuracy: 0.9161\n",
      "Epoch 122/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2207 - accuracy: 0.9280 - val_loss: 0.1997 - val_accuracy: 0.9161\n",
      "Epoch 123/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2203 - accuracy: 0.9280 - val_loss: 0.2008 - val_accuracy: 0.9161\n",
      "Epoch 124/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2197 - accuracy: 0.9280 - val_loss: 0.2018 - val_accuracy: 0.9130\n",
      "Epoch 125/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2193 - accuracy: 0.9280 - val_loss: 0.2025 - val_accuracy: 0.9130\n",
      "Epoch 126/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2189 - accuracy: 0.9280 - val_loss: 0.2020 - val_accuracy: 0.9161\n",
      "Epoch 127/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2184 - accuracy: 0.9280 - val_loss: 0.2005 - val_accuracy: 0.9161\n",
      "Epoch 128/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2183 - accuracy: 0.9280 - val_loss: 0.1994 - val_accuracy: 0.9161\n",
      "Epoch 129/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2177 - accuracy: 0.9280 - val_loss: 0.2005 - val_accuracy: 0.9161\n",
      "Epoch 130/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2177 - accuracy: 0.9280 - val_loss: 0.2009 - val_accuracy: 0.9161\n",
      "Epoch 131/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2171 - accuracy: 0.9280 - val_loss: 0.1988 - val_accuracy: 0.9161\n",
      "Epoch 132/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2166 - accuracy: 0.9280 - val_loss: 0.1967 - val_accuracy: 0.9193\n",
      "Epoch 133/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2160 - accuracy: 0.9280 - val_loss: 0.1964 - val_accuracy: 0.9161\n",
      "Epoch 134/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2155 - accuracy: 0.9280 - val_loss: 0.1971 - val_accuracy: 0.9161\n",
      "Epoch 135/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2151 - accuracy: 0.9280 - val_loss: 0.1975 - val_accuracy: 0.9161\n",
      "Epoch 136/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2147 - accuracy: 0.9280 - val_loss: 0.1967 - val_accuracy: 0.9161\n",
      "Epoch 137/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2140 - accuracy: 0.9280 - val_loss: 0.1950 - val_accuracy: 0.9161\n",
      "Epoch 138/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2139 - accuracy: 0.9265 - val_loss: 0.1940 - val_accuracy: 0.9193\n",
      "Epoch 139/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2145 - accuracy: 0.9265 - val_loss: 0.1939 - val_accuracy: 0.9193\n",
      "Epoch 140/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2143 - accuracy: 0.9265 - val_loss: 0.1946 - val_accuracy: 0.9161\n",
      "Epoch 141/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2128 - accuracy: 0.9280 - val_loss: 0.1984 - val_accuracy: 0.9130\n",
      "Epoch 142/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2117 - accuracy: 0.9280 - val_loss: 0.2019 - val_accuracy: 0.9193\n",
      "Epoch 143/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2136 - accuracy: 0.9296 - val_loss: 0.2015 - val_accuracy: 0.9193\n",
      "Epoch 144/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2135 - accuracy: 0.9265 - val_loss: 0.1967 - val_accuracy: 0.9193\n",
      "Epoch 145/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2117 - accuracy: 0.9280 - val_loss: 0.1937 - val_accuracy: 0.9161\n",
      "Epoch 146/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2106 - accuracy: 0.9280 - val_loss: 0.1923 - val_accuracy: 0.9193\n",
      "Epoch 147/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2103 - accuracy: 0.9280 - val_loss: 0.1919 - val_accuracy: 0.9255\n",
      "Epoch 148/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2106 - accuracy: 0.9265 - val_loss: 0.1934 - val_accuracy: 0.9255\n",
      "Epoch 149/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2102 - accuracy: 0.9265 - val_loss: 0.1972 - val_accuracy: 0.9193\n",
      "Epoch 150/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2101 - accuracy: 0.9280 - val_loss: 0.1990 - val_accuracy: 0.9161\n",
      "Epoch 151/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2095 - accuracy: 0.9280 - val_loss: 0.1991 - val_accuracy: 0.9193\n",
      "Epoch 152/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2088 - accuracy: 0.9265 - val_loss: 0.1991 - val_accuracy: 0.9193\n",
      "Epoch 153/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2086 - accuracy: 0.9280 - val_loss: 0.1977 - val_accuracy: 0.9193\n",
      "Epoch 154/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2078 - accuracy: 0.9265 - val_loss: 0.1983 - val_accuracy: 0.9193\n",
      "Epoch 155/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2077 - accuracy: 0.9265 - val_loss: 0.1978 - val_accuracy: 0.9193\n",
      "Epoch 156/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2061 - accuracy: 0.9280 - val_loss: 0.1919 - val_accuracy: 0.9255\n",
      "Epoch 157/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2067 - accuracy: 0.9296 - val_loss: 0.1883 - val_accuracy: 0.9255\n",
      "Epoch 158/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2077 - accuracy: 0.9204 - val_loss: 0.1880 - val_accuracy: 0.9255\n",
      "Epoch 159/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2068 - accuracy: 0.9219 - val_loss: 0.1894 - val_accuracy: 0.9255\n",
      "Epoch 160/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2047 - accuracy: 0.9296 - val_loss: 0.1939 - val_accuracy: 0.9161\n",
      "Epoch 161/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2044 - accuracy: 0.9280 - val_loss: 0.1973 - val_accuracy: 0.9193\n",
      "Epoch 162/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2046 - accuracy: 0.9280 - val_loss: 0.1955 - val_accuracy: 0.9193\n",
      "Epoch 163/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2037 - accuracy: 0.9280 - val_loss: 0.1921 - val_accuracy: 0.9193\n",
      "Epoch 164/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2026 - accuracy: 0.9280 - val_loss: 0.1901 - val_accuracy: 0.9286\n",
      "Epoch 165/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2024 - accuracy: 0.9296 - val_loss: 0.1892 - val_accuracy: 0.9255\n",
      "Epoch 166/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2024 - accuracy: 0.9296 - val_loss: 0.1899 - val_accuracy: 0.9286\n",
      "Epoch 167/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2014 - accuracy: 0.9296 - val_loss: 0.1936 - val_accuracy: 0.9224\n",
      "Epoch 168/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2024 - accuracy: 0.9280 - val_loss: 0.1966 - val_accuracy: 0.9224\n",
      "Epoch 169/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2018 - accuracy: 0.9296 - val_loss: 0.1923 - val_accuracy: 0.9224\n",
      "Epoch 170/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2008 - accuracy: 0.9280 - val_loss: 0.1896 - val_accuracy: 0.9224\n",
      "Epoch 171/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2005 - accuracy: 0.9280 - val_loss: 0.1900 - val_accuracy: 0.9224\n",
      "Epoch 172/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1993 - accuracy: 0.9280 - val_loss: 0.1941 - val_accuracy: 0.9224\n",
      "Epoch 173/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2002 - accuracy: 0.9311 - val_loss: 0.1973 - val_accuracy: 0.9224\n",
      "Epoch 174/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2005 - accuracy: 0.9311 - val_loss: 0.1918 - val_accuracy: 0.9286\n",
      "Epoch 175/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1983 - accuracy: 0.9280 - val_loss: 0.1858 - val_accuracy: 0.9286\n",
      "Epoch 176/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1978 - accuracy: 0.9280 - val_loss: 0.1830 - val_accuracy: 0.9255\n",
      "Epoch 177/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1986 - accuracy: 0.9280 - val_loss: 0.1830 - val_accuracy: 0.9255\n",
      "Epoch 178/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1978 - accuracy: 0.9296 - val_loss: 0.1856 - val_accuracy: 0.9286\n",
      "Epoch 179/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1960 - accuracy: 0.9280 - val_loss: 0.1903 - val_accuracy: 0.9255\n",
      "Epoch 180/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1969 - accuracy: 0.9296 - val_loss: 0.1939 - val_accuracy: 0.9224\n",
      "Epoch 181/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1969 - accuracy: 0.9342 - val_loss: 0.1888 - val_accuracy: 0.9255\n",
      "Epoch 182/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1953 - accuracy: 0.9280 - val_loss: 0.1834 - val_accuracy: 0.9286\n",
      "Epoch 183/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1950 - accuracy: 0.9280 - val_loss: 0.1811 - val_accuracy: 0.9286\n",
      "Epoch 184/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1962 - accuracy: 0.9296 - val_loss: 0.1816 - val_accuracy: 0.9286\n",
      "Epoch 185/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1950 - accuracy: 0.9280 - val_loss: 0.1860 - val_accuracy: 0.9224\n",
      "Epoch 186/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1943 - accuracy: 0.9311 - val_loss: 0.1904 - val_accuracy: 0.9255\n",
      "Epoch 187/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1943 - accuracy: 0.9357 - val_loss: 0.1867 - val_accuracy: 0.9317\n",
      "Epoch 188/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1923 - accuracy: 0.9311 - val_loss: 0.1806 - val_accuracy: 0.9286\n",
      "Epoch 189/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1934 - accuracy: 0.9265 - val_loss: 0.1791 - val_accuracy: 0.9317\n",
      "Epoch 190/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1948 - accuracy: 0.9234 - val_loss: 0.1797 - val_accuracy: 0.9286\n",
      "Epoch 191/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1926 - accuracy: 0.9280 - val_loss: 0.1851 - val_accuracy: 0.9317\n",
      "Epoch 192/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1923 - accuracy: 0.9296 - val_loss: 0.1937 - val_accuracy: 0.9348\n",
      "Epoch 193/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1939 - accuracy: 0.9357 - val_loss: 0.1895 - val_accuracy: 0.9379\n",
      "Epoch 194/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1919 - accuracy: 0.9357 - val_loss: 0.1818 - val_accuracy: 0.9286\n",
      "Epoch 195/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1891 - accuracy: 0.9326 - val_loss: 0.1778 - val_accuracy: 0.9317\n",
      "Epoch 196/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1908 - accuracy: 0.9326 - val_loss: 0.1769 - val_accuracy: 0.9317\n",
      "Epoch 197/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1892 - accuracy: 0.9311 - val_loss: 0.1810 - val_accuracy: 0.9348\n",
      "Epoch 198/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1881 - accuracy: 0.9342 - val_loss: 0.1873 - val_accuracy: 0.9410\n",
      "Epoch 199/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1903 - accuracy: 0.9372 - val_loss: 0.1849 - val_accuracy: 0.9348\n",
      "Epoch 200/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1893 - accuracy: 0.9342 - val_loss: 0.1780 - val_accuracy: 0.9348\n",
      "Epoch 201/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1871 - accuracy: 0.9326 - val_loss: 0.1757 - val_accuracy: 0.9348\n",
      "Epoch 202/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1870 - accuracy: 0.9342 - val_loss: 0.1748 - val_accuracy: 0.9379\n",
      "Epoch 203/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1869 - accuracy: 0.9342 - val_loss: 0.1761 - val_accuracy: 0.9348\n",
      "Epoch 204/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1866 - accuracy: 0.9372 - val_loss: 0.1802 - val_accuracy: 0.9379\n",
      "Epoch 205/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1856 - accuracy: 0.9372 - val_loss: 0.1793 - val_accuracy: 0.9379\n",
      "Epoch 206/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1851 - accuracy: 0.9372 - val_loss: 0.1765 - val_accuracy: 0.9348\n",
      "Epoch 207/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1841 - accuracy: 0.9372 - val_loss: 0.1731 - val_accuracy: 0.9379\n",
      "Epoch 208/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1847 - accuracy: 0.9326 - val_loss: 0.1719 - val_accuracy: 0.9379\n",
      "Epoch 209/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1846 - accuracy: 0.9311 - val_loss: 0.1730 - val_accuracy: 0.9379\n",
      "Epoch 210/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1839 - accuracy: 0.9357 - val_loss: 0.1751 - val_accuracy: 0.9348\n",
      "Epoch 211/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1826 - accuracy: 0.9372 - val_loss: 0.1744 - val_accuracy: 0.9379\n",
      "Epoch 212/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1825 - accuracy: 0.9387 - val_loss: 0.1726 - val_accuracy: 0.9348\n",
      "Epoch 213/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1827 - accuracy: 0.9403 - val_loss: 0.1713 - val_accuracy: 0.9348\n",
      "Epoch 214/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1823 - accuracy: 0.9387 - val_loss: 0.1726 - val_accuracy: 0.9348\n",
      "Epoch 215/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1804 - accuracy: 0.9418 - val_loss: 0.1794 - val_accuracy: 0.9410\n",
      "Epoch 216/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1815 - accuracy: 0.9387 - val_loss: 0.1883 - val_accuracy: 0.9441\n",
      "Epoch 217/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1855 - accuracy: 0.9433 - val_loss: 0.1816 - val_accuracy: 0.9410\n",
      "Epoch 218/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1814 - accuracy: 0.9403 - val_loss: 0.1704 - val_accuracy: 0.9441\n",
      "Epoch 219/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1793 - accuracy: 0.9387 - val_loss: 0.1669 - val_accuracy: 0.9379\n",
      "Epoch 220/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1819 - accuracy: 0.9357 - val_loss: 0.1661 - val_accuracy: 0.9410\n",
      "Epoch 221/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1823 - accuracy: 0.9357 - val_loss: 0.1663 - val_accuracy: 0.9441\n",
      "Epoch 222/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1797 - accuracy: 0.9403 - val_loss: 0.1719 - val_accuracy: 0.9410\n",
      "Epoch 223/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1786 - accuracy: 0.9418 - val_loss: 0.1816 - val_accuracy: 0.9441\n",
      "Epoch 224/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1810 - accuracy: 0.9449 - val_loss: 0.1761 - val_accuracy: 0.9441\n",
      "Epoch 225/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1770 - accuracy: 0.9464 - val_loss: 0.1663 - val_accuracy: 0.9472\n",
      "Epoch 226/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1775 - accuracy: 0.9403 - val_loss: 0.1649 - val_accuracy: 0.9379\n",
      "Epoch 227/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1806 - accuracy: 0.9326 - val_loss: 0.1658 - val_accuracy: 0.9379\n",
      "Epoch 228/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1787 - accuracy: 0.9342 - val_loss: 0.1701 - val_accuracy: 0.9472\n",
      "Epoch 229/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1757 - accuracy: 0.9433 - val_loss: 0.1770 - val_accuracy: 0.9441\n",
      "Epoch 230/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1775 - accuracy: 0.9479 - val_loss: 0.1779 - val_accuracy: 0.9441\n",
      "Epoch 231/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1768 - accuracy: 0.9479 - val_loss: 0.1686 - val_accuracy: 0.9472\n",
      "Epoch 232/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1737 - accuracy: 0.9464 - val_loss: 0.1629 - val_accuracy: 0.9472\n",
      "Epoch 233/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1752 - accuracy: 0.9403 - val_loss: 0.1620 - val_accuracy: 0.9441\n",
      "Epoch 234/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1749 - accuracy: 0.9387 - val_loss: 0.1647 - val_accuracy: 0.9472\n",
      "Epoch 235/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1736 - accuracy: 0.9449 - val_loss: 0.1732 - val_accuracy: 0.9503\n",
      "Epoch 236/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1740 - accuracy: 0.9495 - val_loss: 0.1717 - val_accuracy: 0.9503\n",
      "Epoch 237/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1729 - accuracy: 0.9479 - val_loss: 0.1652 - val_accuracy: 0.9441\n",
      "Epoch 238/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1715 - accuracy: 0.9464 - val_loss: 0.1627 - val_accuracy: 0.9472\n",
      "Epoch 239/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1708 - accuracy: 0.9479 - val_loss: 0.1637 - val_accuracy: 0.9441\n",
      "Epoch 240/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1708 - accuracy: 0.9479 - val_loss: 0.1646 - val_accuracy: 0.9472\n",
      "Epoch 241/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1701 - accuracy: 0.9479 - val_loss: 0.1623 - val_accuracy: 0.9472\n",
      "Epoch 242/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1700 - accuracy: 0.9479 - val_loss: 0.1602 - val_accuracy: 0.9441\n",
      "Epoch 243/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1706 - accuracy: 0.9479 - val_loss: 0.1601 - val_accuracy: 0.9441\n",
      "Epoch 244/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1703 - accuracy: 0.9495 - val_loss: 0.1622 - val_accuracy: 0.9503\n",
      "Epoch 245/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1685 - accuracy: 0.9495 - val_loss: 0.1631 - val_accuracy: 0.9503\n",
      "Epoch 246/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1679 - accuracy: 0.9495 - val_loss: 0.1656 - val_accuracy: 0.9503\n",
      "Epoch 247/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1687 - accuracy: 0.9510 - val_loss: 0.1648 - val_accuracy: 0.9503\n",
      "Epoch 248/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1668 - accuracy: 0.9495 - val_loss: 0.1580 - val_accuracy: 0.9472\n",
      "Epoch 249/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1675 - accuracy: 0.9449 - val_loss: 0.1564 - val_accuracy: 0.9472\n",
      "Epoch 250/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1679 - accuracy: 0.9433 - val_loss: 0.1575 - val_accuracy: 0.9472\n",
      "Epoch 251/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1659 - accuracy: 0.9495 - val_loss: 0.1579 - val_accuracy: 0.9472\n",
      "Epoch 252/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1651 - accuracy: 0.9510 - val_loss: 0.1569 - val_accuracy: 0.9441\n",
      "Epoch 253/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1648 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9472\n",
      "Epoch 254/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1648 - accuracy: 0.9510 - val_loss: 0.1551 - val_accuracy: 0.9472\n",
      "Epoch 255/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1645 - accuracy: 0.9510 - val_loss: 0.1559 - val_accuracy: 0.9441\n",
      "Epoch 256/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1633 - accuracy: 0.9510 - val_loss: 0.1572 - val_accuracy: 0.9503\n",
      "Epoch 257/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1627 - accuracy: 0.9510 - val_loss: 0.1584 - val_accuracy: 0.9503\n",
      "Epoch 258/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1623 - accuracy: 0.9525 - val_loss: 0.1584 - val_accuracy: 0.9534\n",
      "Epoch 259/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1616 - accuracy: 0.9510 - val_loss: 0.1605 - val_accuracy: 0.9534\n",
      "Epoch 260/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1620 - accuracy: 0.9525 - val_loss: 0.1586 - val_accuracy: 0.9534\n",
      "Epoch 261/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1607 - accuracy: 0.9510 - val_loss: 0.1532 - val_accuracy: 0.9472\n",
      "Epoch 262/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1605 - accuracy: 0.9495 - val_loss: 0.1516 - val_accuracy: 0.9472\n",
      "Epoch 263/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1608 - accuracy: 0.9495 - val_loss: 0.1526 - val_accuracy: 0.9472\n",
      "Epoch 264/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1598 - accuracy: 0.9510 - val_loss: 0.1553 - val_accuracy: 0.9534\n",
      "Epoch 265/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1591 - accuracy: 0.9510 - val_loss: 0.1559 - val_accuracy: 0.9565\n",
      "Epoch 266/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1585 - accuracy: 0.9510 - val_loss: 0.1528 - val_accuracy: 0.9472\n",
      "Epoch 267/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1581 - accuracy: 0.9510 - val_loss: 0.1510 - val_accuracy: 0.9472\n",
      "Epoch 268/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1577 - accuracy: 0.9495 - val_loss: 0.1528 - val_accuracy: 0.9472\n",
      "Epoch 269/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1570 - accuracy: 0.9495 - val_loss: 0.1587 - val_accuracy: 0.9565\n",
      "Epoch 270/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1582 - accuracy: 0.9541 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "Epoch 271/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1578 - accuracy: 0.9525 - val_loss: 0.1533 - val_accuracy: 0.9565\n",
      "Epoch 272/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1562 - accuracy: 0.9510 - val_loss: 0.1515 - val_accuracy: 0.9565\n",
      "Epoch 273/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1550 - accuracy: 0.9510 - val_loss: 0.1516 - val_accuracy: 0.9565\n",
      "Epoch 274/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1546 - accuracy: 0.9525 - val_loss: 0.1524 - val_accuracy: 0.9565\n",
      "Epoch 275/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1544 - accuracy: 0.9525 - val_loss: 0.1511 - val_accuracy: 0.9565\n",
      "Epoch 276/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1539 - accuracy: 0.9510 - val_loss: 0.1492 - val_accuracy: 0.9503\n",
      "Epoch 277/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1532 - accuracy: 0.9510 - val_loss: 0.1491 - val_accuracy: 0.9565\n",
      "Epoch 278/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1530 - accuracy: 0.9510 - val_loss: 0.1530 - val_accuracy: 0.9534\n",
      "Epoch 279/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1545 - accuracy: 0.9541 - val_loss: 0.1546 - val_accuracy: 0.9534\n",
      "Epoch 280/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1528 - accuracy: 0.9541 - val_loss: 0.1459 - val_accuracy: 0.9503\n",
      "Epoch 281/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1542 - accuracy: 0.9495 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "Epoch 282/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1530 - accuracy: 0.9510 - val_loss: 0.1498 - val_accuracy: 0.9565\n",
      "Epoch 283/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1523 - accuracy: 0.9525 - val_loss: 0.1574 - val_accuracy: 0.9534\n",
      "Epoch 284/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1524 - accuracy: 0.9541 - val_loss: 0.1475 - val_accuracy: 0.9596\n",
      "Epoch 285/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1512 - accuracy: 0.9510 - val_loss: 0.1440 - val_accuracy: 0.9472\n",
      "Epoch 286/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1502 - accuracy: 0.9510 - val_loss: 0.1475 - val_accuracy: 0.9565\n",
      "Epoch 287/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1488 - accuracy: 0.9541 - val_loss: 0.1528 - val_accuracy: 0.9565\n",
      "Epoch 288/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1499 - accuracy: 0.9541 - val_loss: 0.1483 - val_accuracy: 0.9565\n",
      "Epoch 289/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1489 - accuracy: 0.9525 - val_loss: 0.1432 - val_accuracy: 0.9503\n",
      "Epoch 290/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1489 - accuracy: 0.9510 - val_loss: 0.1443 - val_accuracy: 0.9565\n",
      "Epoch 291/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1473 - accuracy: 0.9525 - val_loss: 0.1506 - val_accuracy: 0.9565\n",
      "Epoch 292/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1487 - accuracy: 0.9541 - val_loss: 0.1501 - val_accuracy: 0.9565\n",
      "Epoch 293/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1479 - accuracy: 0.9525 - val_loss: 0.1430 - val_accuracy: 0.9503\n",
      "Epoch 294/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1471 - accuracy: 0.9510 - val_loss: 0.1426 - val_accuracy: 0.9503\n",
      "Epoch 295/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1468 - accuracy: 0.9510 - val_loss: 0.1450 - val_accuracy: 0.9596\n",
      "Epoch 296/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1454 - accuracy: 0.9510 - val_loss: 0.1475 - val_accuracy: 0.9565\n",
      "Epoch 297/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1459 - accuracy: 0.9541 - val_loss: 0.1451 - val_accuracy: 0.9565\n",
      "Epoch 298/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1444 - accuracy: 0.9525 - val_loss: 0.1400 - val_accuracy: 0.9534\n",
      "Epoch 299/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1460 - accuracy: 0.9525 - val_loss: 0.1394 - val_accuracy: 0.9503\n",
      "Epoch 300/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1457 - accuracy: 0.9525 - val_loss: 0.1436 - val_accuracy: 0.9565\n",
      "Epoch 301/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1464 - accuracy: 0.9556 - val_loss: 0.1477 - val_accuracy: 0.9565\n",
      "Epoch 302/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1444 - accuracy: 0.9556 - val_loss: 0.1405 - val_accuracy: 0.9596\n",
      "Epoch 303/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1438 - accuracy: 0.9525 - val_loss: 0.1391 - val_accuracy: 0.9596\n",
      "Epoch 304/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1431 - accuracy: 0.9525 - val_loss: 0.1426 - val_accuracy: 0.9565\n",
      "Epoch 305/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1430 - accuracy: 0.9556 - val_loss: 0.1446 - val_accuracy: 0.9565\n",
      "Epoch 306/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1427 - accuracy: 0.9556 - val_loss: 0.1410 - val_accuracy: 0.9596\n",
      "Epoch 307/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1424 - accuracy: 0.9525 - val_loss: 0.1415 - val_accuracy: 0.9596\n",
      "Epoch 308/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1413 - accuracy: 0.9525 - val_loss: 0.1468 - val_accuracy: 0.9565\n",
      "Epoch 309/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1430 - accuracy: 0.9556 - val_loss: 0.1440 - val_accuracy: 0.9565\n",
      "Epoch 310/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1400 - accuracy: 0.9556 - val_loss: 0.1367 - val_accuracy: 0.9503\n",
      "Epoch 311/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1432 - accuracy: 0.9525 - val_loss: 0.1364 - val_accuracy: 0.9503\n",
      "Epoch 312/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1419 - accuracy: 0.9510 - val_loss: 0.1436 - val_accuracy: 0.9565\n",
      "Epoch 313/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1420 - accuracy: 0.9617 - val_loss: 0.1532 - val_accuracy: 0.9565\n",
      "Epoch 314/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1426 - accuracy: 0.9648 - val_loss: 0.1382 - val_accuracy: 0.9596\n",
      "Epoch 315/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1421 - accuracy: 0.9541 - val_loss: 0.1352 - val_accuracy: 0.9503\n",
      "Epoch 316/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1426 - accuracy: 0.9510 - val_loss: 0.1373 - val_accuracy: 0.9596\n",
      "Epoch 317/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1380 - accuracy: 0.9525 - val_loss: 0.1445 - val_accuracy: 0.9565\n",
      "Epoch 318/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1390 - accuracy: 0.9602 - val_loss: 0.1482 - val_accuracy: 0.9565\n",
      "Epoch 319/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1399 - accuracy: 0.9617 - val_loss: 0.1394 - val_accuracy: 0.9596\n",
      "Epoch 320/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1386 - accuracy: 0.9525 - val_loss: 0.1350 - val_accuracy: 0.9534\n",
      "Epoch 321/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1394 - accuracy: 0.9525 - val_loss: 0.1351 - val_accuracy: 0.9596\n",
      "Epoch 322/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1376 - accuracy: 0.9525 - val_loss: 0.1380 - val_accuracy: 0.9596\n",
      "Epoch 323/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1360 - accuracy: 0.9541 - val_loss: 0.1421 - val_accuracy: 0.9565\n",
      "Epoch 324/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1371 - accuracy: 0.9632 - val_loss: 0.1374 - val_accuracy: 0.9596\n",
      "Epoch 325/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1359 - accuracy: 0.9556 - val_loss: 0.1333 - val_accuracy: 0.9596\n",
      "Epoch 326/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1366 - accuracy: 0.9525 - val_loss: 0.1330 - val_accuracy: 0.9596\n",
      "Epoch 327/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1361 - accuracy: 0.9525 - val_loss: 0.1332 - val_accuracy: 0.9596\n",
      "Epoch 328/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1350 - accuracy: 0.9525 - val_loss: 0.1376 - val_accuracy: 0.9596\n",
      "Epoch 329/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1362 - accuracy: 0.9602 - val_loss: 0.1399 - val_accuracy: 0.9596\n",
      "Epoch 330/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1341 - accuracy: 0.9648 - val_loss: 0.1320 - val_accuracy: 0.9596\n",
      "Epoch 331/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1360 - accuracy: 0.9525 - val_loss: 0.1311 - val_accuracy: 0.9627\n",
      "Epoch 332/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1349 - accuracy: 0.9525 - val_loss: 0.1372 - val_accuracy: 0.9596\n",
      "Epoch 333/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1339 - accuracy: 0.9617 - val_loss: 0.1429 - val_accuracy: 0.9596\n",
      "Epoch 334/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1349 - accuracy: 0.9663 - val_loss: 0.1352 - val_accuracy: 0.9565\n",
      "Epoch 335/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1322 - accuracy: 0.9602 - val_loss: 0.1315 - val_accuracy: 0.9596\n",
      "Epoch 336/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1326 - accuracy: 0.9525 - val_loss: 0.1314 - val_accuracy: 0.9596\n",
      "Epoch 337/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1321 - accuracy: 0.9525 - val_loss: 0.1368 - val_accuracy: 0.9596\n",
      "Epoch 338/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1321 - accuracy: 0.9648 - val_loss: 0.1429 - val_accuracy: 0.9596\n",
      "Epoch 339/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1336 - accuracy: 0.9663 - val_loss: 0.1345 - val_accuracy: 0.9596\n",
      "Epoch 340/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1311 - accuracy: 0.9587 - val_loss: 0.1305 - val_accuracy: 0.9596\n",
      "Epoch 341/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1313 - accuracy: 0.9525 - val_loss: 0.1317 - val_accuracy: 0.9596\n",
      "Epoch 342/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1302 - accuracy: 0.9602 - val_loss: 0.1372 - val_accuracy: 0.9596\n",
      "Epoch 343/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1308 - accuracy: 0.9648 - val_loss: 0.1338 - val_accuracy: 0.9596\n",
      "Epoch 344/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1290 - accuracy: 0.9632 - val_loss: 0.1287 - val_accuracy: 0.9596\n",
      "Epoch 345/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1298 - accuracy: 0.9556 - val_loss: 0.1283 - val_accuracy: 0.9596\n",
      "Epoch 346/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1305 - accuracy: 0.9556 - val_loss: 0.1306 - val_accuracy: 0.9627\n",
      "Epoch 347/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1285 - accuracy: 0.9632 - val_loss: 0.1407 - val_accuracy: 0.9534\n",
      "Epoch 348/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1319 - accuracy: 0.9740 - val_loss: 0.1370 - val_accuracy: 0.9596\n",
      "Epoch 349/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1281 - accuracy: 0.9678 - val_loss: 0.1264 - val_accuracy: 0.9627\n",
      "Epoch 350/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1303 - accuracy: 0.9556 - val_loss: 0.1262 - val_accuracy: 0.9627\n",
      "Epoch 351/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1279 - accuracy: 0.9541 - val_loss: 0.1385 - val_accuracy: 0.9596\n",
      "Epoch 352/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1340 - accuracy: 0.9678 - val_loss: 0.1461 - val_accuracy: 0.9565\n",
      "Epoch 353/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1336 - accuracy: 0.9709 - val_loss: 0.1269 - val_accuracy: 0.9627\n",
      "Epoch 354/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1308 - accuracy: 0.9525 - val_loss: 0.1260 - val_accuracy: 0.9534\n",
      "Epoch 355/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1326 - accuracy: 0.9525 - val_loss: 0.1270 - val_accuracy: 0.9627\n",
      "Epoch 356/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1281 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9596\n",
      "Epoch 357/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1271 - accuracy: 0.9678 - val_loss: 0.1287 - val_accuracy: 0.9627\n",
      "Epoch 358/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1243 - accuracy: 0.9632 - val_loss: 0.1249 - val_accuracy: 0.9627\n",
      "Epoch 359/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1266 - accuracy: 0.9556 - val_loss: 0.1245 - val_accuracy: 0.9627\n",
      "Epoch 360/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1263 - accuracy: 0.9571 - val_loss: 0.1283 - val_accuracy: 0.9627\n",
      "Epoch 361/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1244 - accuracy: 0.9648 - val_loss: 0.1346 - val_accuracy: 0.9565\n",
      "Epoch 362/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1259 - accuracy: 0.9724 - val_loss: 0.1285 - val_accuracy: 0.9627\n",
      "Epoch 363/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1243 - accuracy: 0.9663 - val_loss: 0.1247 - val_accuracy: 0.9658\n",
      "Epoch 364/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1240 - accuracy: 0.9602 - val_loss: 0.1293 - val_accuracy: 0.9627\n",
      "Epoch 365/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1229 - accuracy: 0.9694 - val_loss: 0.1424 - val_accuracy: 0.9565\n",
      "Epoch 366/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1290 - accuracy: 0.9740 - val_loss: 0.1301 - val_accuracy: 0.9627\n",
      "Epoch 367/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1226 - accuracy: 0.9678 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "Epoch 368/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1256 - accuracy: 0.9556 - val_loss: 0.1236 - val_accuracy: 0.9627\n",
      "Epoch 369/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1248 - accuracy: 0.9541 - val_loss: 0.1298 - val_accuracy: 0.9627\n",
      "Epoch 370/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1259 - accuracy: 0.9648 - val_loss: 0.1361 - val_accuracy: 0.9627\n",
      "Epoch 371/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1265 - accuracy: 0.9678 - val_loss: 0.1257 - val_accuracy: 0.9627\n",
      "Epoch 372/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1238 - accuracy: 0.9602 - val_loss: 0.1250 - val_accuracy: 0.9627\n",
      "Epoch 373/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1214 - accuracy: 0.9632 - val_loss: 0.1314 - val_accuracy: 0.9596\n",
      "Epoch 374/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1217 - accuracy: 0.9709 - val_loss: 0.1278 - val_accuracy: 0.9627\n",
      "Epoch 375/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1201 - accuracy: 0.9694 - val_loss: 0.1234 - val_accuracy: 0.9627\n",
      "Epoch 376/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1207 - accuracy: 0.9617 - val_loss: 0.1226 - val_accuracy: 0.9627\n",
      "Epoch 377/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1245 - val_accuracy: 0.9627\n",
      "Epoch 378/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1212 - accuracy: 0.9663 - val_loss: 0.1276 - val_accuracy: 0.9627\n",
      "Epoch 379/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1211 - accuracy: 0.9678 - val_loss: 0.1253 - val_accuracy: 0.9627\n",
      "Epoch 380/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1194 - accuracy: 0.9663 - val_loss: 0.1233 - val_accuracy: 0.9627\n",
      "Epoch 381/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1186 - accuracy: 0.9663 - val_loss: 0.1244 - val_accuracy: 0.9627\n",
      "Epoch 382/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1196 - accuracy: 0.9678 - val_loss: 0.1266 - val_accuracy: 0.9627\n",
      "Epoch 383/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1191 - accuracy: 0.9663 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "Epoch 384/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1202 - accuracy: 0.9617 - val_loss: 0.1212 - val_accuracy: 0.9627\n",
      "Epoch 385/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9627\n",
      "Epoch 386/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1185 - accuracy: 0.9709 - val_loss: 0.1366 - val_accuracy: 0.9596\n",
      "Epoch 387/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1218 - accuracy: 0.9724 - val_loss: 0.1275 - val_accuracy: 0.9627\n",
      "Epoch 388/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1184 - accuracy: 0.9694 - val_loss: 0.1214 - val_accuracy: 0.9627\n",
      "Epoch 389/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1211 - val_accuracy: 0.9627\n",
      "Epoch 390/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "Epoch 391/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1168 - accuracy: 0.9678 - val_loss: 0.1242 - val_accuracy: 0.9627\n",
      "Epoch 392/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1163 - accuracy: 0.9663 - val_loss: 0.1217 - val_accuracy: 0.9627\n",
      "Epoch 393/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1160 - accuracy: 0.9678 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "Epoch 394/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1157 - accuracy: 0.9663 - val_loss: 0.1241 - val_accuracy: 0.9627\n",
      "Epoch 395/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1154 - accuracy: 0.9678 - val_loss: 0.1227 - val_accuracy: 0.9627\n",
      "Epoch 396/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1150 - accuracy: 0.9678 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "Epoch 397/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1149 - accuracy: 0.9663 - val_loss: 0.1266 - val_accuracy: 0.9627\n",
      "Epoch 398/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1169 - accuracy: 0.9709 - val_loss: 0.1242 - val_accuracy: 0.9627\n",
      "Epoch 399/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1156 - accuracy: 0.9678 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "Epoch 400/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1158 - accuracy: 0.9632 - val_loss: 0.1197 - val_accuracy: 0.9627\n",
      "Epoch 401/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1140 - accuracy: 0.9678 - val_loss: 0.1278 - val_accuracy: 0.9596\n",
      "Epoch 402/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1163 - accuracy: 0.9724 - val_loss: 0.1244 - val_accuracy: 0.9627\n",
      "Epoch 403/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1145 - accuracy: 0.9724 - val_loss: 0.1186 - val_accuracy: 0.9627\n",
      "Epoch 404/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1145 - accuracy: 0.9648 - val_loss: 0.1182 - val_accuracy: 0.9627\n",
      "Epoch 405/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "Epoch 406/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1145 - accuracy: 0.9648 - val_loss: 0.1222 - val_accuracy: 0.9627\n",
      "Epoch 407/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1126 - accuracy: 0.9709 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "Epoch 408/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1134 - accuracy: 0.9663 - val_loss: 0.1201 - val_accuracy: 0.9627\n",
      "Epoch 409/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1124 - accuracy: 0.9694 - val_loss: 0.1288 - val_accuracy: 0.9627\n",
      "Epoch 410/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1156 - accuracy: 0.9724 - val_loss: 0.1302 - val_accuracy: 0.9596\n",
      "Epoch 411/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1141 - accuracy: 0.9724 - val_loss: 0.1176 - val_accuracy: 0.9627\n",
      "Epoch 412/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1137 - accuracy: 0.9648 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "Epoch 413/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1161 - accuracy: 0.9602 - val_loss: 0.1199 - val_accuracy: 0.9627\n",
      "Epoch 414/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1127 - accuracy: 0.9709 - val_loss: 0.1319 - val_accuracy: 0.9596\n",
      "Epoch 415/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1178 - accuracy: 0.9724 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "Epoch 416/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1117 - accuracy: 0.9724 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "Epoch 417/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1111 - accuracy: 0.9663 - val_loss: 0.1197 - val_accuracy: 0.9627\n",
      "Epoch 418/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1127 - accuracy: 0.9678 - val_loss: 0.1236 - val_accuracy: 0.9627\n",
      "Epoch 419/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1134 - accuracy: 0.9709 - val_loss: 0.1212 - val_accuracy: 0.9627\n",
      "Epoch 420/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1111 - accuracy: 0.9709 - val_loss: 0.1187 - val_accuracy: 0.9627\n",
      "Epoch 421/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "Epoch 422/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1097 - accuracy: 0.9678 - val_loss: 0.1164 - val_accuracy: 0.9627\n",
      "Epoch 423/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1113 - accuracy: 0.9663 - val_loss: 0.1184 - val_accuracy: 0.9627\n",
      "Epoch 424/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1114 - accuracy: 0.9694 - val_loss: 0.1208 - val_accuracy: 0.9627\n",
      "Epoch 425/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1106 - accuracy: 0.9724 - val_loss: 0.1186 - val_accuracy: 0.9627\n",
      "Epoch 426/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "Epoch 427/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1094 - accuracy: 0.9694 - val_loss: 0.1184 - val_accuracy: 0.9627\n",
      "Epoch 428/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1083 - accuracy: 0.9694 - val_loss: 0.1231 - val_accuracy: 0.9627\n",
      "Epoch 429/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1104 - accuracy: 0.9740 - val_loss: 0.1223 - val_accuracy: 0.9627\n",
      "Epoch 430/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1117 - accuracy: 0.9678 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "Epoch 431/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1098 - accuracy: 0.9663 - val_loss: 0.1171 - val_accuracy: 0.9627\n",
      "Epoch 432/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1084 - accuracy: 0.9678 - val_loss: 0.1167 - val_accuracy: 0.9627\n",
      "Epoch 433/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1079 - accuracy: 0.9694 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "Epoch 434/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1075 - accuracy: 0.9724 - val_loss: 0.1198 - val_accuracy: 0.9627\n",
      "Epoch 435/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1074 - accuracy: 0.9724 - val_loss: 0.1153 - val_accuracy: 0.9627\n",
      "Epoch 436/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1065 - accuracy: 0.9678 - val_loss: 0.1135 - val_accuracy: 0.9627\n",
      "Epoch 437/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1090 - accuracy: 0.9663 - val_loss: 0.1137 - val_accuracy: 0.9627\n",
      "Epoch 438/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1076 - accuracy: 0.9694 - val_loss: 0.1166 - val_accuracy: 0.9627\n",
      "Epoch 439/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1078 - accuracy: 0.9724 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "Epoch 440/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1067 - accuracy: 0.9709 - val_loss: 0.1139 - val_accuracy: 0.9627\n",
      "Epoch 441/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1064 - accuracy: 0.9678 - val_loss: 0.1150 - val_accuracy: 0.9627\n",
      "Epoch 442/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1058 - accuracy: 0.9694 - val_loss: 0.1175 - val_accuracy: 0.9627\n",
      "Epoch 443/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1057 - accuracy: 0.9724 - val_loss: 0.1175 - val_accuracy: 0.9627\n",
      "Epoch 444/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1056 - accuracy: 0.9724 - val_loss: 0.1151 - val_accuracy: 0.9627\n",
      "Epoch 445/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1049 - accuracy: 0.9694 - val_loss: 0.1128 - val_accuracy: 0.9627\n",
      "Epoch 446/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1077 - accuracy: 0.9648 - val_loss: 0.1127 - val_accuracy: 0.9627\n",
      "Epoch 447/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1078 - accuracy: 0.9648 - val_loss: 0.1154 - val_accuracy: 0.9627\n",
      "Epoch 448/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1050 - accuracy: 0.9724 - val_loss: 0.1200 - val_accuracy: 0.9627\n",
      "Epoch 449/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1061 - accuracy: 0.9724 - val_loss: 0.1164 - val_accuracy: 0.9627\n",
      "Epoch 450/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1041 - accuracy: 0.9694 - val_loss: 0.1132 - val_accuracy: 0.9627\n",
      "Epoch 451/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.1133 - val_accuracy: 0.9627\n",
      "Epoch 452/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "Epoch 453/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1039 - accuracy: 0.9724 - val_loss: 0.1188 - val_accuracy: 0.9627\n",
      "Epoch 454/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1043 - accuracy: 0.9724 - val_loss: 0.1152 - val_accuracy: 0.9627\n",
      "Epoch 455/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1034 - accuracy: 0.9724 - val_loss: 0.1125 - val_accuracy: 0.9627\n",
      "Epoch 456/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1044 - accuracy: 0.9724 - val_loss: 0.1127 - val_accuracy: 0.9627\n",
      "Epoch 457/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1048 - accuracy: 0.9709 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "Epoch 458/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1035 - accuracy: 0.9709 - val_loss: 0.1111 - val_accuracy: 0.9627\n",
      "Epoch 459/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1046 - accuracy: 0.9663 - val_loss: 0.1117 - val_accuracy: 0.9627\n",
      "Epoch 460/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1039 - accuracy: 0.9724 - val_loss: 0.1191 - val_accuracy: 0.9658\n",
      "Epoch 461/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1045 - accuracy: 0.9709 - val_loss: 0.1134 - val_accuracy: 0.9627\n",
      "Epoch 462/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1033 - accuracy: 0.9709 - val_loss: 0.1106 - val_accuracy: 0.9627\n",
      "Epoch 463/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1035 - accuracy: 0.9678 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "Epoch 464/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1018 - accuracy: 0.9709 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "Epoch 465/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1016 - accuracy: 0.9724 - val_loss: 0.1145 - val_accuracy: 0.9627\n",
      "Epoch 466/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1027 - accuracy: 0.9724 - val_loss: 0.1139 - val_accuracy: 0.9627\n",
      "Epoch 467/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1024 - accuracy: 0.9724 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "Epoch 468/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1017 - accuracy: 0.9724 - val_loss: 0.1115 - val_accuracy: 0.9627\n",
      "Epoch 469/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1020 - accuracy: 0.9709 - val_loss: 0.1122 - val_accuracy: 0.9627\n",
      "Epoch 470/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1012 - accuracy: 0.9724 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "Epoch 471/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1012 - accuracy: 0.9724 - val_loss: 0.1138 - val_accuracy: 0.9627\n",
      "Epoch 472/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1011 - accuracy: 0.9724 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "Epoch 473/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1018 - accuracy: 0.9678 - val_loss: 0.1105 - val_accuracy: 0.9627\n",
      "Epoch 474/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1011 - accuracy: 0.9694 - val_loss: 0.1121 - val_accuracy: 0.9627\n",
      "Epoch 475/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1002 - accuracy: 0.9724 - val_loss: 0.1141 - val_accuracy: 0.9627\n",
      "Epoch 476/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1003 - accuracy: 0.9724 - val_loss: 0.1144 - val_accuracy: 0.9658\n",
      "Epoch 477/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1002 - accuracy: 0.9724 - val_loss: 0.1100 - val_accuracy: 0.9627\n",
      "Epoch 478/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1000 - accuracy: 0.9724 - val_loss: 0.1089 - val_accuracy: 0.9627\n",
      "Epoch 479/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1024 - accuracy: 0.9678 - val_loss: 0.1091 - val_accuracy: 0.9627\n",
      "Epoch 480/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1006 - accuracy: 0.9709 - val_loss: 0.1147 - val_accuracy: 0.9658\n",
      "Epoch 481/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1014 - accuracy: 0.9740 - val_loss: 0.1147 - val_accuracy: 0.9658\n",
      "Epoch 482/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0997 - accuracy: 0.9724 - val_loss: 0.1085 - val_accuracy: 0.9627\n",
      "Epoch 483/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1005 - accuracy: 0.9678 - val_loss: 0.1081 - val_accuracy: 0.9627\n",
      "Epoch 484/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0999 - accuracy: 0.9678 - val_loss: 0.1121 - val_accuracy: 0.9627\n",
      "Epoch 485/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0993 - accuracy: 0.9724 - val_loss: 0.1111 - val_accuracy: 0.9627\n",
      "Epoch 486/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0988 - accuracy: 0.9724 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "Epoch 487/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0988 - accuracy: 0.9709 - val_loss: 0.1092 - val_accuracy: 0.9627\n",
      "Epoch 488/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0992 - accuracy: 0.9709 - val_loss: 0.1114 - val_accuracy: 0.9658\n",
      "Epoch 489/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0979 - accuracy: 0.9709 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "Epoch 490/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0990 - accuracy: 0.9709 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "Epoch 491/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1016 - accuracy: 0.9678 - val_loss: 0.1086 - val_accuracy: 0.9627\n",
      "Epoch 492/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0989 - accuracy: 0.9709 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "Epoch 493/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0998 - accuracy: 0.9740 - val_loss: 0.1094 - val_accuracy: 0.9627\n",
      "Epoch 494/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0972 - accuracy: 0.9724 - val_loss: 0.1066 - val_accuracy: 0.9689\n",
      "Epoch 495/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0999 - accuracy: 0.9678 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "Epoch 496/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.1105 - val_accuracy: 0.9658\n",
      "Epoch 497/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0983 - accuracy: 0.9724 - val_loss: 0.1124 - val_accuracy: 0.9658\n",
      "Epoch 498/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0985 - accuracy: 0.9724 - val_loss: 0.1078 - val_accuracy: 0.9627\n",
      "Epoch 499/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0965 - accuracy: 0.9709 - val_loss: 0.1074 - val_accuracy: 0.9627\n",
      "Epoch 500/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0961 - accuracy: 0.9724 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "Epoch 501/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.1069 - val_accuracy: 0.9627\n",
      "Epoch 502/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0964 - accuracy: 0.9724 - val_loss: 0.1126 - val_accuracy: 0.9658\n",
      "Epoch 503/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0970 - accuracy: 0.9724 - val_loss: 0.1156 - val_accuracy: 0.9627\n",
      "Epoch 504/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0973 - accuracy: 0.9755 - val_loss: 0.1103 - val_accuracy: 0.9627\n",
      "Epoch 505/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0963 - accuracy: 0.9694 - val_loss: 0.1103 - val_accuracy: 0.9627\n",
      "Epoch 506/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0989 - accuracy: 0.9678 - val_loss: 0.1118 - val_accuracy: 0.9596\n",
      "Epoch 507/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0991 - accuracy: 0.9678 - val_loss: 0.1097 - val_accuracy: 0.9627\n",
      "Epoch 508/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0957 - accuracy: 0.9709 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "Epoch 509/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0950 - accuracy: 0.9709 - val_loss: 0.1071 - val_accuracy: 0.9627\n",
      "Epoch 510/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0960 - accuracy: 0.9709 - val_loss: 0.1066 - val_accuracy: 0.9627\n",
      "Epoch 511/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0979 - accuracy: 0.9724 - val_loss: 0.1071 - val_accuracy: 0.9627\n",
      "Epoch 512/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0977 - accuracy: 0.9709 - val_loss: 0.1081 - val_accuracy: 0.9627\n",
      "Epoch 513/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0962 - accuracy: 0.9709 - val_loss: 0.1095 - val_accuracy: 0.9658\n",
      "Epoch 514/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0942 - accuracy: 0.9724 - val_loss: 0.1118 - val_accuracy: 0.9658\n",
      "Epoch 515/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.1082 - val_accuracy: 0.9627\n",
      "Epoch 516/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0955 - accuracy: 0.9709 - val_loss: 0.1066 - val_accuracy: 0.9658\n",
      "Epoch 517/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0967 - accuracy: 0.9678 - val_loss: 0.1088 - val_accuracy: 0.9627\n",
      "Epoch 518/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0957 - accuracy: 0.9709 - val_loss: 0.1120 - val_accuracy: 0.9658\n",
      "Epoch 519/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.1075 - val_accuracy: 0.9658\n",
      "Epoch 520/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.1057 - val_accuracy: 0.9627\n",
      "Epoch 521/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0937 - accuracy: 0.9709 - val_loss: 0.1076 - val_accuracy: 0.9658\n",
      "Epoch 522/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0940 - accuracy: 0.9724 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "Epoch 523/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0948 - accuracy: 0.9740 - val_loss: 0.1072 - val_accuracy: 0.9658\n",
      "Epoch 524/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0923 - accuracy: 0.9709 - val_loss: 0.1050 - val_accuracy: 0.9627\n",
      "Epoch 525/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0933 - accuracy: 0.9724 - val_loss: 0.1055 - val_accuracy: 0.9627\n",
      "Epoch 526/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0929 - accuracy: 0.9724 - val_loss: 0.1084 - val_accuracy: 0.9658\n",
      "Epoch 527/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0925 - accuracy: 0.9724 - val_loss: 0.1069 - val_accuracy: 0.9658\n",
      "Epoch 528/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0921 - accuracy: 0.9709 - val_loss: 0.1056 - val_accuracy: 0.9627\n",
      "Epoch 529/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0924 - accuracy: 0.9709 - val_loss: 0.1073 - val_accuracy: 0.9658\n",
      "Epoch 530/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0924 - accuracy: 0.9724 - val_loss: 0.1063 - val_accuracy: 0.9658\n",
      "Epoch 531/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0919 - accuracy: 0.9709 - val_loss: 0.1040 - val_accuracy: 0.9658\n",
      "Epoch 532/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0925 - accuracy: 0.9724 - val_loss: 0.1045 - val_accuracy: 0.9627\n",
      "Epoch 533/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0917 - accuracy: 0.9709 - val_loss: 0.1060 - val_accuracy: 0.9658\n",
      "Epoch 534/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0919 - accuracy: 0.9755 - val_loss: 0.1056 - val_accuracy: 0.9658\n",
      "Epoch 535/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0916 - accuracy: 0.9709 - val_loss: 0.1041 - val_accuracy: 0.9627\n",
      "Epoch 536/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0910 - accuracy: 0.9709 - val_loss: 0.1060 - val_accuracy: 0.9658\n",
      "Epoch 537/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0913 - accuracy: 0.9786 - val_loss: 0.1054 - val_accuracy: 0.9658\n",
      "Epoch 538/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0915 - accuracy: 0.9740 - val_loss: 0.1034 - val_accuracy: 0.9627\n",
      "Epoch 539/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0918 - accuracy: 0.9709 - val_loss: 0.1037 - val_accuracy: 0.9627\n",
      "Epoch 540/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0918 - accuracy: 0.9709 - val_loss: 0.1067 - val_accuracy: 0.9658\n",
      "Epoch 541/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0921 - accuracy: 0.9770 - val_loss: 0.1124 - val_accuracy: 0.9627\n",
      "Epoch 542/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0921 - accuracy: 0.9755 - val_loss: 0.1036 - val_accuracy: 0.9627\n",
      "Epoch 543/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0905 - accuracy: 0.9724 - val_loss: 0.1031 - val_accuracy: 0.9689\n",
      "Epoch 544/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0937 - accuracy: 0.9678 - val_loss: 0.1041 - val_accuracy: 0.9627\n",
      "Epoch 545/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0905 - accuracy: 0.9740 - val_loss: 0.1119 - val_accuracy: 0.9627\n",
      "Epoch 546/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0915 - accuracy: 0.9755 - val_loss: 0.1056 - val_accuracy: 0.9658\n",
      "Epoch 547/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0888 - accuracy: 0.9709 - val_loss: 0.1026 - val_accuracy: 0.9689\n",
      "Epoch 548/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0936 - accuracy: 0.9678 - val_loss: 0.1024 - val_accuracy: 0.9689\n",
      "Epoch 549/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0909 - accuracy: 0.9709 - val_loss: 0.1107 - val_accuracy: 0.9627\n",
      "Epoch 550/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0933 - accuracy: 0.9755 - val_loss: 0.1118 - val_accuracy: 0.9627\n",
      "Epoch 551/1500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0911 - accuracy: 0.9724 - val_loss: 0.1021 - val_accuracy: 0.9689\n",
      "Epoch 552/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0916 - accuracy: 0.9694 - val_loss: 0.1026 - val_accuracy: 0.9689\n",
      "Epoch 553/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 0.1053 - val_accuracy: 0.9658\n",
      "Epoch 554/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0894 - accuracy: 0.9755 - val_loss: 0.1093 - val_accuracy: 0.9627\n",
      "Epoch 555/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0898 - accuracy: 0.9755 - val_loss: 0.1030 - val_accuracy: 0.9627\n",
      "Epoch 556/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0896 - accuracy: 0.9724 - val_loss: 0.1017 - val_accuracy: 0.9658\n",
      "Epoch 557/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0896 - accuracy: 0.9709 - val_loss: 0.1046 - val_accuracy: 0.9658\n",
      "Epoch 558/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0881 - accuracy: 0.9755 - val_loss: 0.1089 - val_accuracy: 0.9627\n",
      "Epoch 559/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0892 - accuracy: 0.9786 - val_loss: 0.1028 - val_accuracy: 0.9658\n",
      "Epoch 560/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0886 - accuracy: 0.9724 - val_loss: 0.1015 - val_accuracy: 0.9689\n",
      "Epoch 561/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0899 - accuracy: 0.9709 - val_loss: 0.1031 - val_accuracy: 0.9627\n",
      "Epoch 562/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.1082 - val_accuracy: 0.9627\n",
      "Epoch 563/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0883 - accuracy: 0.9755 - val_loss: 0.1055 - val_accuracy: 0.9658\n",
      "Epoch 564/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0871 - accuracy: 0.9740 - val_loss: 0.1025 - val_accuracy: 0.9658\n",
      "Epoch 565/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0875 - accuracy: 0.9724 - val_loss: 0.1019 - val_accuracy: 0.9658\n",
      "Epoch 566/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0878 - accuracy: 0.9709 - val_loss: 0.1028 - val_accuracy: 0.9627\n",
      "Epoch 567/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0864 - accuracy: 0.9740 - val_loss: 0.1061 - val_accuracy: 0.9627\n",
      "Epoch 568/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0868 - accuracy: 0.9786 - val_loss: 0.1058 - val_accuracy: 0.9627\n",
      "Epoch 569/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0866 - accuracy: 0.9770 - val_loss: 0.1023 - val_accuracy: 0.9627\n",
      "Epoch 570/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0876 - accuracy: 0.9724 - val_loss: 0.1017 - val_accuracy: 0.9627\n",
      "Epoch 571/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0880 - accuracy: 0.9724 - val_loss: 0.1035 - val_accuracy: 0.9627\n",
      "Epoch 572/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0860 - accuracy: 0.9755 - val_loss: 0.1061 - val_accuracy: 0.9627\n",
      "Epoch 573/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0866 - accuracy: 0.9770 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "Epoch 574/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0870 - accuracy: 0.9770 - val_loss: 0.1024 - val_accuracy: 0.9658\n",
      "Epoch 575/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0858 - accuracy: 0.9755 - val_loss: 0.1000 - val_accuracy: 0.9658\n",
      "Epoch 576/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0862 - accuracy: 0.9709 - val_loss: 0.1030 - val_accuracy: 0.9658\n",
      "Epoch 577/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0863 - accuracy: 0.9740 - val_loss: 0.1090 - val_accuracy: 0.9596\n",
      "Epoch 578/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0872 - accuracy: 0.9755 - val_loss: 0.1005 - val_accuracy: 0.9658\n",
      "Epoch 579/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0877 - accuracy: 0.9724 - val_loss: 0.0998 - val_accuracy: 0.9689\n",
      "Epoch 580/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0878 - accuracy: 0.9694 - val_loss: 0.1026 - val_accuracy: 0.9627\n",
      "Epoch 581/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0842 - accuracy: 0.9755 - val_loss: 0.1086 - val_accuracy: 0.9627\n",
      "Epoch 582/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0870 - accuracy: 0.9755 - val_loss: 0.1052 - val_accuracy: 0.9627\n",
      "Epoch 583/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0840 - accuracy: 0.9770 - val_loss: 0.1003 - val_accuracy: 0.9658\n",
      "Epoch 584/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0904 - accuracy: 0.9678 - val_loss: 0.1002 - val_accuracy: 0.9658\n",
      "Epoch 585/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0842 - accuracy: 0.9740 - val_loss: 0.1171 - val_accuracy: 0.9627\n",
      "Epoch 586/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0922 - accuracy: 0.9755 - val_loss: 0.1140 - val_accuracy: 0.9627\n",
      "Epoch 587/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0882 - accuracy: 0.9740 - val_loss: 0.1004 - val_accuracy: 0.9689\n",
      "Epoch 588/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0909 - accuracy: 0.9663 - val_loss: 0.1008 - val_accuracy: 0.9689\n",
      "Epoch 589/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0887 - accuracy: 0.9678 - val_loss: 0.1074 - val_accuracy: 0.9596\n",
      "Epoch 590/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0859 - accuracy: 0.9755 - val_loss: 0.1158 - val_accuracy: 0.9596\n",
      "Epoch 591/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0880 - accuracy: 0.9755 - val_loss: 0.1000 - val_accuracy: 0.9658\n",
      "Epoch 592/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.1017 - val_accuracy: 0.9689\n",
      "Epoch 593/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0916 - accuracy: 0.9663 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "Epoch 594/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0835 - accuracy: 0.9724 - val_loss: 0.1091 - val_accuracy: 0.9658\n",
      "Epoch 595/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0867 - accuracy: 0.9755 - val_loss: 0.1099 - val_accuracy: 0.9658\n",
      "Epoch 596/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0850 - accuracy: 0.9755 - val_loss: 0.0993 - val_accuracy: 0.9658\n",
      "Epoch 597/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0852 - accuracy: 0.9709 - val_loss: 0.0989 - val_accuracy: 0.9689\n",
      "Epoch 598/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0834 - accuracy: 0.9709 - val_loss: 0.1052 - val_accuracy: 0.9627\n",
      "Epoch 599/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.1099 - val_accuracy: 0.9627\n",
      "Epoch 600/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0849 - accuracy: 0.9755 - val_loss: 0.0990 - val_accuracy: 0.9689\n",
      "Epoch 601/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0988 - val_accuracy: 0.9689\n",
      "Epoch 602/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0858 - accuracy: 0.9694 - val_loss: 0.1006 - val_accuracy: 0.9689\n",
      "Epoch 603/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0814 - accuracy: 0.9786 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "Epoch 604/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0896 - accuracy: 0.9755 - val_loss: 0.1045 - val_accuracy: 0.9627\n",
      "Epoch 605/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0831 - accuracy: 0.9755 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "Epoch 606/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0823 - accuracy: 0.9724 - val_loss: 0.0986 - val_accuracy: 0.9658\n",
      "Epoch 607/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0809 - accuracy: 0.9770 - val_loss: 0.1036 - val_accuracy: 0.9627\n",
      "Epoch 608/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0814 - accuracy: 0.9740 - val_loss: 0.1025 - val_accuracy: 0.9627\n",
      "Epoch 609/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0817 - accuracy: 0.9740 - val_loss: 0.1001 - val_accuracy: 0.9658\n",
      "Epoch 610/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0810 - accuracy: 0.9770 - val_loss: 0.1002 - val_accuracy: 0.9658\n",
      "Epoch 611/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0808 - accuracy: 0.9770 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "Epoch 612/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0803 - accuracy: 0.9770 - val_loss: 0.0998 - val_accuracy: 0.9658\n",
      "Epoch 613/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0798 - accuracy: 0.9770 - val_loss: 0.0991 - val_accuracy: 0.9658\n",
      "Epoch 614/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0798 - accuracy: 0.9770 - val_loss: 0.0998 - val_accuracy: 0.9658\n",
      "Epoch 615/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0793 - accuracy: 0.9770 - val_loss: 0.1028 - val_accuracy: 0.9627\n",
      "Epoch 616/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0801 - accuracy: 0.9755 - val_loss: 0.1031 - val_accuracy: 0.9658\n",
      "Epoch 617/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0801 - accuracy: 0.9755 - val_loss: 0.1012 - val_accuracy: 0.9658\n",
      "Epoch 618/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0810 - accuracy: 0.9770 - val_loss: 0.1022 - val_accuracy: 0.9627\n",
      "Epoch 619/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0812 - accuracy: 0.9755 - val_loss: 0.1021 - val_accuracy: 0.9658\n",
      "Epoch 620/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0793 - accuracy: 0.9755 - val_loss: 0.0979 - val_accuracy: 0.9658\n",
      "Epoch 621/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0809 - accuracy: 0.9740 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 622/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0802 - accuracy: 0.9770 - val_loss: 0.1068 - val_accuracy: 0.9658\n",
      "Epoch 623/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0821 - accuracy: 0.9740 - val_loss: 0.1012 - val_accuracy: 0.9627\n",
      "Epoch 624/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0794 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 625/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0785 - accuracy: 0.9770 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "Epoch 626/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0781 - accuracy: 0.9755 - val_loss: 0.1006 - val_accuracy: 0.9658\n",
      "Epoch 627/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0782 - accuracy: 0.9755 - val_loss: 0.1018 - val_accuracy: 0.9658\n",
      "Epoch 628/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0783 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 629/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0774 - accuracy: 0.9770 - val_loss: 0.0977 - val_accuracy: 0.9658\n",
      "Epoch 630/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 0.0984 - val_accuracy: 0.9658\n",
      "Epoch 631/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 0.1026 - val_accuracy: 0.9627\n",
      "Epoch 632/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0786 - accuracy: 0.9740 - val_loss: 0.0985 - val_accuracy: 0.9658\n",
      "Epoch 633/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0780 - accuracy: 0.9786 - val_loss: 0.0967 - val_accuracy: 0.9658\n",
      "Epoch 634/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0775 - accuracy: 0.9755 - val_loss: 0.0996 - val_accuracy: 0.9658\n",
      "Epoch 635/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0765 - accuracy: 0.9770 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "Epoch 636/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0802 - accuracy: 0.9724 - val_loss: 0.1031 - val_accuracy: 0.9596\n",
      "Epoch 637/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0769 - accuracy: 0.9755 - val_loss: 0.0968 - val_accuracy: 0.9689\n",
      "Epoch 638/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0791 - accuracy: 0.9694 - val_loss: 0.0983 - val_accuracy: 0.9720\n",
      "Epoch 639/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0835 - accuracy: 0.9678 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "Epoch 640/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0749 - accuracy: 0.9770 - val_loss: 0.1199 - val_accuracy: 0.9720\n",
      "Epoch 641/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0893 - accuracy: 0.9724 - val_loss: 0.1029 - val_accuracy: 0.9627\n",
      "Epoch 642/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0767 - accuracy: 0.9740 - val_loss: 0.0987 - val_accuracy: 0.9720\n",
      "Epoch 643/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0862 - accuracy: 0.9678 - val_loss: 0.0984 - val_accuracy: 0.9720\n",
      "Epoch 644/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0836 - accuracy: 0.9678 - val_loss: 0.0996 - val_accuracy: 0.9658\n",
      "Epoch 645/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0766 - accuracy: 0.9755 - val_loss: 0.1155 - val_accuracy: 0.9689\n",
      "Epoch 646/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.0991 - val_accuracy: 0.9627\n",
      "Epoch 647/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0765 - accuracy: 0.9770 - val_loss: 0.0959 - val_accuracy: 0.9658\n",
      "Epoch 648/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0756 - accuracy: 0.9770 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "Epoch 649/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.1023 - val_accuracy: 0.9658\n",
      "Epoch 650/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.1014 - val_accuracy: 0.9627\n",
      "Epoch 651/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0763 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9627\n",
      "Epoch 652/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0753 - accuracy: 0.9770 - val_loss: 0.0994 - val_accuracy: 0.9658\n",
      "Epoch 653/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0751 - accuracy: 0.9770 - val_loss: 0.1002 - val_accuracy: 0.9627\n",
      "Epoch 654/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0749 - accuracy: 0.9770 - val_loss: 0.0965 - val_accuracy: 0.9658\n",
      "Epoch 655/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0743 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9627\n",
      "Epoch 656/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0755 - accuracy: 0.9786 - val_loss: 0.0957 - val_accuracy: 0.9658\n",
      "Epoch 657/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0751 - accuracy: 0.9786 - val_loss: 0.0968 - val_accuracy: 0.9658\n",
      "Epoch 658/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0749 - accuracy: 0.9786 - val_loss: 0.0985 - val_accuracy: 0.9689\n",
      "Epoch 659/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0756 - accuracy: 0.9786 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "Epoch 660/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0760 - accuracy: 0.9755 - val_loss: 0.0991 - val_accuracy: 0.9658\n",
      "Epoch 661/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0752 - accuracy: 0.9801 - val_loss: 0.0958 - val_accuracy: 0.9658\n",
      "Epoch 662/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0744 - accuracy: 0.9786 - val_loss: 0.0952 - val_accuracy: 0.9658\n",
      "Epoch 663/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0739 - accuracy: 0.9770 - val_loss: 0.0966 - val_accuracy: 0.9658\n",
      "Epoch 664/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0995 - val_accuracy: 0.9627\n",
      "Epoch 665/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0735 - accuracy: 0.9770 - val_loss: 0.1003 - val_accuracy: 0.9658\n",
      "Epoch 666/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0735 - accuracy: 0.9770 - val_loss: 0.0966 - val_accuracy: 0.9658\n",
      "Epoch 667/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0950 - val_accuracy: 0.9627\n",
      "Epoch 668/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0737 - accuracy: 0.9755 - val_loss: 0.0960 - val_accuracy: 0.9658\n",
      "Epoch 669/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0724 - accuracy: 0.9786 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "Epoch 670/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0722 - accuracy: 0.9770 - val_loss: 0.1005 - val_accuracy: 0.9658\n",
      "Epoch 671/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 672/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0724 - accuracy: 0.9770 - val_loss: 0.0961 - val_accuracy: 0.9658\n",
      "Epoch 673/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0722 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9627\n",
      "Epoch 674/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.0968 - val_accuracy: 0.9658\n",
      "Epoch 675/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0716 - accuracy: 0.9770 - val_loss: 0.0960 - val_accuracy: 0.9658\n",
      "Epoch 676/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0725 - accuracy: 0.9786 - val_loss: 0.0973 - val_accuracy: 0.9658\n",
      "Epoch 677/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "Epoch 678/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0716 - accuracy: 0.9770 - val_loss: 0.0956 - val_accuracy: 0.9658\n",
      "Epoch 679/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0708 - accuracy: 0.9786 - val_loss: 0.0940 - val_accuracy: 0.9658\n",
      "Epoch 680/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0715 - accuracy: 0.9770 - val_loss: 0.0952 - val_accuracy: 0.9627\n",
      "Epoch 681/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0718 - accuracy: 0.9786 - val_loss: 0.0959 - val_accuracy: 0.9627\n",
      "Epoch 682/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0711 - accuracy: 0.9786 - val_loss: 0.0947 - val_accuracy: 0.9658\n",
      "Epoch 683/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0707 - accuracy: 0.9786 - val_loss: 0.1005 - val_accuracy: 0.9720\n",
      "Epoch 684/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0727 - accuracy: 0.9740 - val_loss: 0.1043 - val_accuracy: 0.9720\n",
      "Epoch 685/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0731 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9658\n",
      "Epoch 686/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0714 - accuracy: 0.9786 - val_loss: 0.0947 - val_accuracy: 0.9720\n",
      "Epoch 687/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0774 - accuracy: 0.9709 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "Epoch 688/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0717 - accuracy: 0.9786 - val_loss: 0.1120 - val_accuracy: 0.9689\n",
      "Epoch 689/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0778 - accuracy: 0.9709 - val_loss: 0.0951 - val_accuracy: 0.9658\n",
      "Epoch 690/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0688 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9720\n",
      "Epoch 691/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0792 - accuracy: 0.9694 - val_loss: 0.0934 - val_accuracy: 0.9658\n",
      "Epoch 692/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0731 - accuracy: 0.9755 - val_loss: 0.1100 - val_accuracy: 0.9720\n",
      "Epoch 693/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0774 - accuracy: 0.9740 - val_loss: 0.0970 - val_accuracy: 0.9689\n",
      "Epoch 694/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0948 - val_accuracy: 0.9720\n",
      "Epoch 695/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0786 - accuracy: 0.9694 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "Epoch 696/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0741 - accuracy: 0.9724 - val_loss: 0.1015 - val_accuracy: 0.9689\n",
      "Epoch 697/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0727 - accuracy: 0.9755 - val_loss: 0.1035 - val_accuracy: 0.9689\n",
      "Epoch 698/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0718 - accuracy: 0.9755 - val_loss: 0.0926 - val_accuracy: 0.9720\n",
      "Epoch 699/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0733 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "Epoch 700/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.1035 - val_accuracy: 0.9720\n",
      "Epoch 701/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0730 - accuracy: 0.9755 - val_loss: 0.1028 - val_accuracy: 0.9689\n",
      "Epoch 702/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0739 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9658\n",
      "Epoch 703/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0687 - accuracy: 0.9786 - val_loss: 0.0941 - val_accuracy: 0.9627\n",
      "Epoch 704/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9689\n",
      "Epoch 705/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0946 - val_accuracy: 0.9658\n",
      "Epoch 706/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0681 - accuracy: 0.9770 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "Epoch 707/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0983 - val_accuracy: 0.9658\n",
      "Epoch 708/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.0969 - val_accuracy: 0.9689\n",
      "Epoch 709/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0676 - accuracy: 0.9770 - val_loss: 0.0941 - val_accuracy: 0.9627\n",
      "Epoch 710/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0950 - val_accuracy: 0.9627\n",
      "Epoch 711/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0702 - accuracy: 0.9724 - val_loss: 0.0983 - val_accuracy: 0.9627\n",
      "Epoch 712/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0701 - accuracy: 0.9755 - val_loss: 0.0974 - val_accuracy: 0.9627\n",
      "Epoch 713/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0701 - accuracy: 0.9770 - val_loss: 0.0975 - val_accuracy: 0.9689\n",
      "Epoch 714/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0682 - accuracy: 0.9755 - val_loss: 0.0966 - val_accuracy: 0.9689\n",
      "Epoch 715/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0680 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9689\n",
      "Epoch 716/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.0944 - val_accuracy: 0.9689\n",
      "Epoch 717/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.0964 - val_accuracy: 0.9689\n",
      "Epoch 718/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0675 - accuracy: 0.9755 - val_loss: 0.0957 - val_accuracy: 0.9689\n",
      "Epoch 719/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0674 - accuracy: 0.9770 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "Epoch 720/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0668 - accuracy: 0.9786 - val_loss: 0.0971 - val_accuracy: 0.9658\n",
      "Epoch 721/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0670 - accuracy: 0.9770 - val_loss: 0.0988 - val_accuracy: 0.9720\n",
      "Epoch 722/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0685 - accuracy: 0.9755 - val_loss: 0.0968 - val_accuracy: 0.9689\n",
      "Epoch 723/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "Epoch 724/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0671 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9689\n",
      "Epoch 725/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9658\n",
      "Epoch 726/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0662 - accuracy: 0.9786 - val_loss: 0.0937 - val_accuracy: 0.9658\n",
      "Epoch 727/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0666 - accuracy: 0.9786 - val_loss: 0.0963 - val_accuracy: 0.9689\n",
      "Epoch 728/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0676 - accuracy: 0.9770 - val_loss: 0.0963 - val_accuracy: 0.9689\n",
      "Epoch 729/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.0927 - val_accuracy: 0.9658\n",
      "Epoch 730/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.0926 - val_accuracy: 0.9658\n",
      "Epoch 731/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0672 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9689\n",
      "Epoch 732/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0657 - accuracy: 0.9770 - val_loss: 0.0958 - val_accuracy: 0.9720\n",
      "Epoch 733/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.0975 - val_accuracy: 0.9720\n",
      "Epoch 734/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 735/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 0.0923 - val_accuracy: 0.9689\n",
      "Epoch 736/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0658 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "Epoch 737/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0653 - accuracy: 0.9770 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "Epoch 738/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0651 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9720\n",
      "Epoch 739/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 740/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0658 - accuracy: 0.9755 - val_loss: 0.0973 - val_accuracy: 0.9720\n",
      "Epoch 741/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "Epoch 742/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 743/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0656 - accuracy: 0.9786 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 744/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "Epoch 745/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "Epoch 746/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0648 - accuracy: 0.9755 - val_loss: 0.0992 - val_accuracy: 0.9689\n",
      "Epoch 747/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9689\n",
      "Epoch 748/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0648 - accuracy: 0.9770 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "Epoch 749/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0652 - accuracy: 0.9740 - val_loss: 0.0967 - val_accuracy: 0.9720\n",
      "Epoch 750/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0654 - accuracy: 0.9755 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 751/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "Epoch 752/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 753/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.1008 - val_accuracy: 0.9720\n",
      "Epoch 754/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.0975 - val_accuracy: 0.9720\n",
      "Epoch 755/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0688 - accuracy: 0.9740 - val_loss: 0.0915 - val_accuracy: 0.9658\n",
      "Epoch 756/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0659 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "Epoch 757/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.0974 - val_accuracy: 0.9720\n",
      "Epoch 758/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 759/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0627 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 760/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0629 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 761/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0622 - accuracy: 0.9770 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "Epoch 762/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0630 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "Epoch 763/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0635 - accuracy: 0.9786 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "Epoch 764/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0622 - accuracy: 0.9770 - val_loss: 0.0968 - val_accuracy: 0.9720\n",
      "Epoch 765/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0635 - accuracy: 0.9755 - val_loss: 0.0982 - val_accuracy: 0.9752\n",
      "Epoch 766/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0637 - accuracy: 0.9755 - val_loss: 0.0917 - val_accuracy: 0.9720\n",
      "Epoch 767/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0632 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 768/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 769/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0634 - accuracy: 0.9786 - val_loss: 0.0994 - val_accuracy: 0.9752\n",
      "Epoch 770/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0652 - accuracy: 0.9740 - val_loss: 0.1000 - val_accuracy: 0.9752\n",
      "Epoch 771/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0650 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9658\n",
      "Epoch 772/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "Epoch 773/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0674 - accuracy: 0.9740 - val_loss: 0.0949 - val_accuracy: 0.9689\n",
      "Epoch 774/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.1040 - val_accuracy: 0.9689\n",
      "Epoch 775/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0657 - accuracy: 0.9770 - val_loss: 0.0922 - val_accuracy: 0.9658\n",
      "Epoch 776/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0638 - accuracy: 0.9740 - val_loss: 0.0950 - val_accuracy: 0.9658\n",
      "Epoch 777/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0707 - accuracy: 0.9724 - val_loss: 0.0929 - val_accuracy: 0.9720\n",
      "Epoch 778/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 779/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "Epoch 780/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0626 - accuracy: 0.9801 - val_loss: 0.0911 - val_accuracy: 0.9658\n",
      "Epoch 781/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.0910 - val_accuracy: 0.9689\n",
      "Epoch 782/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.0968 - val_accuracy: 0.9720\n",
      "Epoch 783/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.0996 - val_accuracy: 0.9689\n",
      "Epoch 784/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9689\n",
      "Epoch 785/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.0917 - val_accuracy: 0.9720\n",
      "Epoch 786/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.1012 - val_accuracy: 0.9752\n",
      "Epoch 787/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0647 - accuracy: 0.9755 - val_loss: 0.0969 - val_accuracy: 0.9720\n",
      "Epoch 788/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0604 - accuracy: 0.9786 - val_loss: 0.0912 - val_accuracy: 0.9689\n",
      "Epoch 789/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0629 - accuracy: 0.9786 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 790/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0647 - accuracy: 0.9740 - val_loss: 0.0920 - val_accuracy: 0.9720\n",
      "Epoch 791/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.1014 - val_accuracy: 0.9814\n",
      "Epoch 792/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0640 - accuracy: 0.9740 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "Epoch 793/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0589 - accuracy: 0.9801 - val_loss: 0.0941 - val_accuracy: 0.9720\n",
      "Epoch 794/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 795/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0648 - accuracy: 0.9755 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "Epoch 796/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0612 - accuracy: 0.9770 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "Epoch 797/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 798/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0606 - accuracy: 0.9801 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "Epoch 799/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0607 - accuracy: 0.9801 - val_loss: 0.0912 - val_accuracy: 0.9720\n",
      "Epoch 800/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0588 - accuracy: 0.9786 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 801/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0604 - accuracy: 0.9770 - val_loss: 0.0987 - val_accuracy: 0.9814\n",
      "Epoch 802/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0615 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 803/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0594 - accuracy: 0.9786 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 804/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0592 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "Epoch 805/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "Epoch 806/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0582 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 807/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 808/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0596 - accuracy: 0.9801 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 809/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0598 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9689\n",
      "Epoch 810/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0587 - accuracy: 0.9801 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "Epoch 811/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.0910 - val_accuracy: 0.9658\n",
      "Epoch 812/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0599 - accuracy: 0.9770 - val_loss: 0.0911 - val_accuracy: 0.9689\n",
      "Epoch 813/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0582 - accuracy: 0.9801 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "Epoch 814/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0576 - accuracy: 0.9786 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "Epoch 815/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 0.0952 - val_accuracy: 0.9752\n",
      "Epoch 816/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 817/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0596 - accuracy: 0.9801 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "Epoch 818/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 819/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 820/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0577 - accuracy: 0.9786 - val_loss: 0.0933 - val_accuracy: 0.9689\n",
      "Epoch 821/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0916 - val_accuracy: 0.9689\n",
      "Epoch 822/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0586 - accuracy: 0.9801 - val_loss: 0.0919 - val_accuracy: 0.9689\n",
      "Epoch 823/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.0963 - val_accuracy: 0.9720\n",
      "Epoch 824/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0583 - accuracy: 0.9786 - val_loss: 0.0944 - val_accuracy: 0.9689\n",
      "Epoch 825/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0565 - accuracy: 0.9786 - val_loss: 0.0920 - val_accuracy: 0.9689\n",
      "Epoch 826/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0591 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 827/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.0954 - val_accuracy: 0.9752\n",
      "Epoch 828/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0579 - accuracy: 0.9770 - val_loss: 0.0977 - val_accuracy: 0.9783\n",
      "Epoch 829/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0595 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9720\n",
      "Epoch 830/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.0920 - val_accuracy: 0.9720\n",
      "Epoch 831/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0571 - accuracy: 0.9816 - val_loss: 0.0942 - val_accuracy: 0.9689\n",
      "Epoch 832/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0583 - accuracy: 0.9801 - val_loss: 0.0952 - val_accuracy: 0.9689\n",
      "Epoch 833/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 834/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 835/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0563 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9814\n",
      "Epoch 836/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0588 - accuracy: 0.9770 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "Epoch 837/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0587 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "Epoch 838/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.0910 - val_accuracy: 0.9720\n",
      "Epoch 839/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0567 - accuracy: 0.9786 - val_loss: 0.0986 - val_accuracy: 0.9814\n",
      "Epoch 840/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.0985 - val_accuracy: 0.9783\n",
      "Epoch 841/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0585 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9658\n",
      "Epoch 842/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0595 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9658\n",
      "Epoch 843/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0617 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9689\n",
      "Epoch 844/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0996 - val_accuracy: 0.9814\n",
      "Epoch 845/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 846/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0601 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 847/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0580 - accuracy: 0.9816 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 848/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0564 - accuracy: 0.9770 - val_loss: 0.0976 - val_accuracy: 0.9783\n",
      "Epoch 849/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0590 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "Epoch 850/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0565 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "Epoch 851/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0560 - accuracy: 0.9786 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "Epoch 852/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "Epoch 853/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0568 - accuracy: 0.9801 - val_loss: 0.0987 - val_accuracy: 0.9814\n",
      "Epoch 854/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0572 - accuracy: 0.9786 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "Epoch 855/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0587 - accuracy: 0.9786 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "Epoch 856/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 0.0957 - val_accuracy: 0.9814\n",
      "Epoch 857/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9814\n",
      "Epoch 858/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0561 - accuracy: 0.9770 - val_loss: 0.0894 - val_accuracy: 0.9720\n",
      "Epoch 859/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0578 - accuracy: 0.9816 - val_loss: 0.0898 - val_accuracy: 0.9720\n",
      "Epoch 860/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0992 - val_accuracy: 0.9783\n",
      "Epoch 861/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0582 - accuracy: 0.9770 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "Epoch 862/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0548 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "Epoch 863/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0596 - accuracy: 0.9770 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 864/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0546 - accuracy: 0.9801 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "Epoch 865/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0610 - accuracy: 0.9801 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 866/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0525 - accuracy: 0.9801 - val_loss: 0.0934 - val_accuracy: 0.9689\n",
      "Epoch 867/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0638 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9689\n",
      "Epoch 868/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.1050 - val_accuracy: 0.9783\n",
      "Epoch 869/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0619 - accuracy: 0.9816 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "Epoch 870/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0595 - accuracy: 0.9786 - val_loss: 0.0910 - val_accuracy: 0.9689\n",
      "Epoch 871/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.0909 - val_accuracy: 0.9720\n",
      "Epoch 872/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0587 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9783\n",
      "Epoch 873/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0544 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 874/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0545 - accuracy: 0.9770 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "Epoch 875/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0531 - accuracy: 0.9801 - val_loss: 0.0899 - val_accuracy: 0.9720\n",
      "Epoch 876/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.0902 - val_accuracy: 0.9720\n",
      "Epoch 877/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0554 - accuracy: 0.9786 - val_loss: 0.0906 - val_accuracy: 0.9689\n",
      "Epoch 878/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0547 - accuracy: 0.9816 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "Epoch 879/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0550 - accuracy: 0.9801 - val_loss: 0.0981 - val_accuracy: 0.9752\n",
      "Epoch 880/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0553 - accuracy: 0.9786 - val_loss: 0.0905 - val_accuracy: 0.9720\n",
      "Epoch 881/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0545 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 882/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0586 - accuracy: 0.9816 - val_loss: 0.0925 - val_accuracy: 0.9689\n",
      "Epoch 883/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0548 - accuracy: 0.9801 - val_loss: 0.1103 - val_accuracy: 0.9752\n",
      "Epoch 884/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0635 - accuracy: 0.9801 - val_loss: 0.0912 - val_accuracy: 0.9689\n",
      "Epoch 885/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0565 - accuracy: 0.9816 - val_loss: 0.0960 - val_accuracy: 0.9720\n",
      "Epoch 886/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0638 - accuracy: 0.9770 - val_loss: 0.0915 - val_accuracy: 0.9720\n",
      "Epoch 887/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.1084 - val_accuracy: 0.9752\n",
      "Epoch 888/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0633 - accuracy: 0.9801 - val_loss: 0.0970 - val_accuracy: 0.9783\n",
      "Epoch 889/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0538 - accuracy: 0.9801 - val_loss: 0.0924 - val_accuracy: 0.9689\n",
      "Epoch 890/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0584 - accuracy: 0.9801 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "Epoch 891/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0922 - val_accuracy: 0.9783\n",
      "Epoch 892/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0529 - accuracy: 0.9786 - val_loss: 0.0979 - val_accuracy: 0.9814\n",
      "Epoch 893/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0560 - accuracy: 0.9755 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "Epoch 894/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0527 - accuracy: 0.9770 - val_loss: 0.0922 - val_accuracy: 0.9720\n",
      "Epoch 895/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.0920 - val_accuracy: 0.9689\n",
      "Epoch 896/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "Epoch 897/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0524 - accuracy: 0.9816 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "Epoch 898/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "Epoch 899/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0540 - accuracy: 0.9801 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 900/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0522 - accuracy: 0.9801 - val_loss: 0.0977 - val_accuracy: 0.9752\n",
      "Epoch 901/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0544 - accuracy: 0.9770 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 902/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0512 - accuracy: 0.9770 - val_loss: 0.0931 - val_accuracy: 0.9689\n",
      "Epoch 903/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0549 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "Epoch 904/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0569 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 905/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.0970 - val_accuracy: 0.9814\n",
      "Epoch 906/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0535 - accuracy: 0.9786 - val_loss: 0.0943 - val_accuracy: 0.9783\n",
      "Epoch 907/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0526 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "Epoch 908/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "Epoch 909/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.0928 - val_accuracy: 0.9752\n",
      "Epoch 910/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0505 - accuracy: 0.9801 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
      "Epoch 911/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0517 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "Epoch 912/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0939 - val_accuracy: 0.9752\n",
      "Epoch 913/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.0979 - val_accuracy: 0.9752\n",
      "Epoch 914/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0531 - accuracy: 0.9786 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 915/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0534 - accuracy: 0.9801 - val_loss: 0.0928 - val_accuracy: 0.9720\n",
      "Epoch 916/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.0965 - val_accuracy: 0.9752\n",
      "Epoch 917/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0520 - accuracy: 0.9770 - val_loss: 0.1000 - val_accuracy: 0.9783\n",
      "Epoch 918/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0524 - accuracy: 0.9786 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "Epoch 919/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 920/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "Epoch 921/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0520 - accuracy: 0.9801 - val_loss: 0.0918 - val_accuracy: 0.9783\n",
      "Epoch 922/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.0901 - val_accuracy: 0.9720\n",
      "Epoch 923/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0901 - val_accuracy: 0.9720\n",
      "Epoch 924/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0916 - val_accuracy: 0.9752\n",
      "Epoch 925/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0510 - accuracy: 0.9816 - val_loss: 0.0922 - val_accuracy: 0.9720\n",
      "Epoch 926/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0505 - accuracy: 0.9801 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 927/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0504 - accuracy: 0.9786 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "Epoch 928/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0520 - accuracy: 0.9832 - val_loss: 0.0911 - val_accuracy: 0.9752\n",
      "Epoch 929/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0510 - accuracy: 0.9847 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 930/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9786 - val_loss: 0.0996 - val_accuracy: 0.9783\n",
      "Epoch 931/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0518 - accuracy: 0.9770 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "Epoch 932/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0514 - accuracy: 0.9832 - val_loss: 0.0929 - val_accuracy: 0.9752\n",
      "Epoch 933/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0532 - accuracy: 0.9801 - val_loss: 0.0938 - val_accuracy: 0.9752\n",
      "Epoch 934/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0507 - accuracy: 0.9786 - val_loss: 0.0968 - val_accuracy: 0.9752\n",
      "Epoch 935/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0502 - accuracy: 0.9786 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "Epoch 936/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "Epoch 937/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 938/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0502 - accuracy: 0.9786 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "Epoch 939/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0490 - accuracy: 0.9801 - val_loss: 0.0927 - val_accuracy: 0.9752\n",
      "Epoch 940/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "Epoch 941/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 942/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0509 - accuracy: 0.9786 - val_loss: 0.0982 - val_accuracy: 0.9752\n",
      "Epoch 943/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0503 - accuracy: 0.9770 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 944/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 945/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0511 - accuracy: 0.9816 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "Epoch 946/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.0931 - val_accuracy: 0.9720\n",
      "Epoch 947/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0492 - accuracy: 0.9847 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 948/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 0.0942 - val_accuracy: 0.9814\n",
      "Epoch 949/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0488 - accuracy: 0.9786 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
      "Epoch 950/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0493 - accuracy: 0.9770 - val_loss: 0.0935 - val_accuracy: 0.9814\n",
      "Epoch 951/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.0924 - val_accuracy: 0.9752\n",
      "Epoch 952/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.0923 - val_accuracy: 0.9752\n",
      "Epoch 953/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.0928 - val_accuracy: 0.9783\n",
      "Epoch 954/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0489 - accuracy: 0.9816 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 955/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.0928 - val_accuracy: 0.9720\n",
      "Epoch 956/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0485 - accuracy: 0.9847 - val_loss: 0.0947 - val_accuracy: 0.9752\n",
      "Epoch 957/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 958/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.0936 - val_accuracy: 0.9752\n",
      "Epoch 959/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0498 - accuracy: 0.9832 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 960/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0482 - accuracy: 0.9786 - val_loss: 0.1087 - val_accuracy: 0.9752\n",
      "Epoch 961/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0561 - accuracy: 0.9786 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 962/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0501 - accuracy: 0.9801 - val_loss: 0.0937 - val_accuracy: 0.9752\n",
      "Epoch 963/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.0939 - val_accuracy: 0.9720\n",
      "Epoch 964/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9816 - val_loss: 0.0968 - val_accuracy: 0.9752\n",
      "Epoch 965/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0488 - accuracy: 0.9801 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "Epoch 966/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0481 - accuracy: 0.9801 - val_loss: 0.0944 - val_accuracy: 0.9752\n",
      "Epoch 967/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0477 - accuracy: 0.9801 - val_loss: 0.0951 - val_accuracy: 0.9720\n",
      "Epoch 968/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0473 - accuracy: 0.9801 - val_loss: 0.0966 - val_accuracy: 0.9752\n",
      "Epoch 969/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0480 - accuracy: 0.9786 - val_loss: 0.0963 - val_accuracy: 0.9752\n",
      "Epoch 970/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0482 - accuracy: 0.9786 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 971/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0482 - accuracy: 0.9832 - val_loss: 0.0962 - val_accuracy: 0.9752\n",
      "Epoch 972/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0481 - accuracy: 0.9786 - val_loss: 0.0961 - val_accuracy: 0.9752\n",
      "Epoch 973/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0471 - accuracy: 0.9786 - val_loss: 0.0924 - val_accuracy: 0.9752\n",
      "Epoch 974/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0516 - accuracy: 0.9832 - val_loss: 0.0921 - val_accuracy: 0.9752\n",
      "Epoch 975/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 976/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0476 - accuracy: 0.9770 - val_loss: 0.0920 - val_accuracy: 0.9752\n",
      "Epoch 977/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.0916 - val_accuracy: 0.9752\n",
      "Epoch 978/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0472 - accuracy: 0.9816 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "Epoch 979/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0479 - accuracy: 0.9786 - val_loss: 0.0954 - val_accuracy: 0.9752\n",
      "Epoch 980/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0473 - accuracy: 0.9786 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "Epoch 981/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0478 - accuracy: 0.9832 - val_loss: 0.0925 - val_accuracy: 0.9752\n",
      "Epoch 982/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0491 - accuracy: 0.9816 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 983/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0465 - accuracy: 0.9801 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 984/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0485 - accuracy: 0.9786 - val_loss: 0.0963 - val_accuracy: 0.9814\n",
      "Epoch 985/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0478 - accuracy: 0.9786 - val_loss: 0.0920 - val_accuracy: 0.9783\n",
      "Epoch 986/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0468 - accuracy: 0.9847 - val_loss: 0.0915 - val_accuracy: 0.9752\n",
      "Epoch 987/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0499 - accuracy: 0.9847 - val_loss: 0.0912 - val_accuracy: 0.9752\n",
      "Epoch 988/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0472 - accuracy: 0.9832 - val_loss: 0.0963 - val_accuracy: 0.9752\n",
      "Epoch 989/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0482 - accuracy: 0.9786 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 990/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0459 - accuracy: 0.9786 - val_loss: 0.0924 - val_accuracy: 0.9752\n",
      "Epoch 991/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0509 - accuracy: 0.9832 - val_loss: 0.0924 - val_accuracy: 0.9720\n",
      "Epoch 992/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0493 - accuracy: 0.9832 - val_loss: 0.0998 - val_accuracy: 0.9752\n",
      "Epoch 993/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 994/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0480 - accuracy: 0.9816 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "Epoch 995/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0461 - accuracy: 0.9832 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 996/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0466 - accuracy: 0.9801 - val_loss: 0.0970 - val_accuracy: 0.9752\n",
      "Epoch 997/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0467 - accuracy: 0.9786 - val_loss: 0.0944 - val_accuracy: 0.9752\n",
      "Epoch 998/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0475 - accuracy: 0.9847 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "Epoch 999/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.0978 - val_accuracy: 0.9752\n",
      "Epoch 1000/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0472 - accuracy: 0.9801 - val_loss: 0.0995 - val_accuracy: 0.9752\n",
      "Epoch 1001/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0475 - accuracy: 0.9786 - val_loss: 0.0958 - val_accuracy: 0.9752\n",
      "Epoch 1002/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0459 - accuracy: 0.9801 - val_loss: 0.0937 - val_accuracy: 0.9752\n",
      "Epoch 1003/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0464 - accuracy: 0.9816 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "Epoch 1004/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0460 - accuracy: 0.9816 - val_loss: 0.0942 - val_accuracy: 0.9752\n",
      "Epoch 1005/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0455 - accuracy: 0.9832 - val_loss: 0.0936 - val_accuracy: 0.9752\n",
      "Epoch 1006/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0457 - accuracy: 0.9816 - val_loss: 0.0934 - val_accuracy: 0.9752\n",
      "Epoch 1007/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0463 - accuracy: 0.9816 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1008/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0458 - accuracy: 0.9786 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 1009/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0462 - accuracy: 0.9786 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "Epoch 1010/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0464 - accuracy: 0.9847 - val_loss: 0.0930 - val_accuracy: 0.9752\n",
      "Epoch 1011/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 1012/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0454 - accuracy: 0.9816 - val_loss: 0.0984 - val_accuracy: 0.9752\n",
      "Epoch 1013/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0468 - accuracy: 0.9786 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 1014/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0451 - accuracy: 0.9801 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1015/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0459 - accuracy: 0.9832 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 1016/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0454 - accuracy: 0.9816 - val_loss: 0.0965 - val_accuracy: 0.9752\n",
      "Epoch 1017/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0460 - accuracy: 0.9816 - val_loss: 0.0995 - val_accuracy: 0.9720\n",
      "Epoch 1018/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0476 - accuracy: 0.9801 - val_loss: 0.0950 - val_accuracy: 0.9720\n",
      "Epoch 1019/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0456 - accuracy: 0.9832 - val_loss: 0.0939 - val_accuracy: 0.9752\n",
      "Epoch 1020/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0459 - accuracy: 0.9847 - val_loss: 0.0947 - val_accuracy: 0.9720\n",
      "Epoch 1021/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0452 - accuracy: 0.9832 - val_loss: 0.0975 - val_accuracy: 0.9752\n",
      "Epoch 1022/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0455 - accuracy: 0.9786 - val_loss: 0.0960 - val_accuracy: 0.9752\n",
      "Epoch 1023/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0447 - accuracy: 0.9816 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "Epoch 1024/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 0.9832 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1025/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0450 - accuracy: 0.9832 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1026/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0457 - accuracy: 0.9801 - val_loss: 0.0974 - val_accuracy: 0.9783\n",
      "Epoch 1027/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0453 - accuracy: 0.9801 - val_loss: 0.0936 - val_accuracy: 0.9752\n",
      "Epoch 1028/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0465 - accuracy: 0.9847 - val_loss: 0.0932 - val_accuracy: 0.9783\n",
      "Epoch 1029/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0473 - accuracy: 0.9847 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1030/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0446 - accuracy: 0.9801 - val_loss: 0.0930 - val_accuracy: 0.9783\n",
      "Epoch 1031/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0445 - accuracy: 0.9832 - val_loss: 0.0933 - val_accuracy: 0.9783\n",
      "Epoch 1032/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0449 - accuracy: 0.9801 - val_loss: 0.0925 - val_accuracy: 0.9783\n",
      "Epoch 1033/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0450 - accuracy: 0.9801 - val_loss: 0.0913 - val_accuracy: 0.9783\n",
      "Epoch 1034/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0445 - accuracy: 0.9862 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "Epoch 1035/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0458 - accuracy: 0.9832 - val_loss: 0.0928 - val_accuracy: 0.9783\n",
      "Epoch 1036/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0452 - accuracy: 0.9832 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "Epoch 1037/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0449 - accuracy: 0.9862 - val_loss: 0.0910 - val_accuracy: 0.9752\n",
      "Epoch 1038/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0459 - accuracy: 0.9862 - val_loss: 0.0923 - val_accuracy: 0.9783\n",
      "Epoch 1039/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0431 - accuracy: 0.9801 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1040/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0499 - accuracy: 0.9801 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1041/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0492 - accuracy: 0.9786 - val_loss: 0.0920 - val_accuracy: 0.9752\n",
      "Epoch 1042/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0441 - accuracy: 0.9847 - val_loss: 0.0919 - val_accuracy: 0.9752\n",
      "Epoch 1043/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0460 - accuracy: 0.9862 - val_loss: 0.0932 - val_accuracy: 0.9814\n",
      "Epoch 1044/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0441 - accuracy: 0.9832 - val_loss: 0.0986 - val_accuracy: 0.9814\n",
      "Epoch 1045/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0456 - accuracy: 0.9801 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1046/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0444 - accuracy: 0.9816 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "Epoch 1047/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0451 - accuracy: 0.9862 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 1048/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0468 - accuracy: 0.9816 - val_loss: 0.0980 - val_accuracy: 0.9783\n",
      "Epoch 1049/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0451 - accuracy: 0.9847 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 1050/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0443 - accuracy: 0.9847 - val_loss: 0.0933 - val_accuracy: 0.9783\n",
      "Epoch 1051/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 1052/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0459 - accuracy: 0.9801 - val_loss: 0.0969 - val_accuracy: 0.9814\n",
      "Epoch 1053/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0465 - accuracy: 0.9816 - val_loss: 0.0930 - val_accuracy: 0.9814\n",
      "Epoch 1054/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0459 - accuracy: 0.9816 - val_loss: 0.0930 - val_accuracy: 0.9814\n",
      "Epoch 1055/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0435 - accuracy: 0.9832 - val_loss: 0.0992 - val_accuracy: 0.9814\n",
      "Epoch 1056/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0460 - accuracy: 0.9816 - val_loss: 0.0977 - val_accuracy: 0.9783\n",
      "Epoch 1057/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0437 - accuracy: 0.9832 - val_loss: 0.0926 - val_accuracy: 0.9752\n",
      "Epoch 1058/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "Epoch 1059/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.0931 - val_accuracy: 0.9752\n",
      "Epoch 1060/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0444 - accuracy: 0.9816 - val_loss: 0.1029 - val_accuracy: 0.9752\n",
      "Epoch 1061/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0478 - accuracy: 0.9816 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 1062/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0471 - accuracy: 0.9832 - val_loss: 0.0926 - val_accuracy: 0.9752\n",
      "Epoch 1063/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 0.1000 - val_accuracy: 0.9720\n",
      "Epoch 1064/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0471 - accuracy: 0.9816 - val_loss: 0.1003 - val_accuracy: 0.9752\n",
      "Epoch 1065/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0468 - accuracy: 0.9801 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 1066/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0438 - accuracy: 0.9832 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 1067/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0442 - accuracy: 0.9832 - val_loss: 0.0953 - val_accuracy: 0.9752\n",
      "Epoch 1068/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.1003 - val_accuracy: 0.9783\n",
      "Epoch 1069/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0447 - accuracy: 0.9816 - val_loss: 0.0957 - val_accuracy: 0.9752\n",
      "Epoch 1070/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0427 - accuracy: 0.9847 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 1071/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0466 - accuracy: 0.9816 - val_loss: 0.0947 - val_accuracy: 0.9783\n",
      "Epoch 1072/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: 0.0987 - val_accuracy: 0.9783\n",
      "Epoch 1073/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0445 - accuracy: 0.9816 - val_loss: 0.1028 - val_accuracy: 0.9783\n",
      "Epoch 1074/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0466 - accuracy: 0.9786 - val_loss: 0.0958 - val_accuracy: 0.9783\n",
      "Epoch 1075/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0426 - accuracy: 0.9832 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "Epoch 1076/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0429 - accuracy: 0.9847 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1077/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0422 - accuracy: 0.9862 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 1078/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0456 - accuracy: 0.9816 - val_loss: 0.0938 - val_accuracy: 0.9752\n",
      "Epoch 1079/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0416 - accuracy: 0.9862 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1080/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0502 - accuracy: 0.9832 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 1081/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0450 - accuracy: 0.9832 - val_loss: 0.1029 - val_accuracy: 0.9814\n",
      "Epoch 1082/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0515 - accuracy: 0.9816 - val_loss: 0.0969 - val_accuracy: 0.9814\n",
      "Epoch 1083/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0442 - accuracy: 0.9786 - val_loss: 0.0957 - val_accuracy: 0.9752\n",
      "Epoch 1084/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0523 - accuracy: 0.9832 - val_loss: 0.0949 - val_accuracy: 0.9752\n",
      "Epoch 1085/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0490 - accuracy: 0.9832 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1086/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0432 - accuracy: 0.9832 - val_loss: 0.1046 - val_accuracy: 0.9752\n",
      "Epoch 1087/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0478 - accuracy: 0.9786 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 1088/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 0.0937 - val_accuracy: 0.9752\n",
      "Epoch 1089/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0457 - accuracy: 0.9832 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "Epoch 1090/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0421 - accuracy: 0.9877 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "Epoch 1091/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0469 - accuracy: 0.9801 - val_loss: 0.0995 - val_accuracy: 0.9814\n",
      "Epoch 1092/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0447 - accuracy: 0.9801 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 1093/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 0.0932 - val_accuracy: 0.9752\n",
      "Epoch 1094/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0426 - accuracy: 0.9847 - val_loss: 0.0946 - val_accuracy: 0.9783\n",
      "Epoch 1095/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0420 - accuracy: 0.9847 - val_loss: 0.0964 - val_accuracy: 0.9783\n",
      "Epoch 1096/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0430 - accuracy: 0.9847 - val_loss: 0.0956 - val_accuracy: 0.9783\n",
      "Epoch 1097/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0415 - accuracy: 0.9832 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "Epoch 1098/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0459 - accuracy: 0.9832 - val_loss: 0.0924 - val_accuracy: 0.9783\n",
      "Epoch 1099/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0420 - accuracy: 0.9832 - val_loss: 0.0987 - val_accuracy: 0.9783\n",
      "Epoch 1100/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0459 - accuracy: 0.9832 - val_loss: 0.0950 - val_accuracy: 0.9752\n",
      "Epoch 1101/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "Epoch 1102/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0450 - accuracy: 0.9847 - val_loss: 0.0905 - val_accuracy: 0.9783\n",
      "Epoch 1103/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.0917 - val_accuracy: 0.9814\n",
      "Epoch 1104/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0425 - accuracy: 0.9847 - val_loss: 0.0926 - val_accuracy: 0.9814\n",
      "Epoch 1105/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0421 - accuracy: 0.9832 - val_loss: 0.0937 - val_accuracy: 0.9783\n",
      "Epoch 1106/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0404 - accuracy: 0.9832 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1107/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 0.0976 - val_accuracy: 0.9752\n",
      "Epoch 1108/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0448 - accuracy: 0.9862 - val_loss: 0.0940 - val_accuracy: 0.9752\n",
      "Epoch 1109/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0417 - accuracy: 0.9877 - val_loss: 0.0951 - val_accuracy: 0.9783\n",
      "Epoch 1110/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1111/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0410 - accuracy: 0.9832 - val_loss: 0.0951 - val_accuracy: 0.9814\n",
      "Epoch 1112/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.0958 - val_accuracy: 0.9814\n",
      "Epoch 1113/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0417 - accuracy: 0.9832 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1114/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0418 - accuracy: 0.9832 - val_loss: 0.1026 - val_accuracy: 0.9783\n",
      "Epoch 1115/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0433 - accuracy: 0.9832 - val_loss: 0.1067 - val_accuracy: 0.9752\n",
      "Epoch 1116/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0458 - accuracy: 0.9816 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 1117/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0405 - accuracy: 0.9862 - val_loss: 0.0972 - val_accuracy: 0.9752\n",
      "Epoch 1118/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.0960 - val_accuracy: 0.9752\n",
      "Epoch 1119/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.1115 - val_accuracy: 0.9752\n",
      "Epoch 1120/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0499 - accuracy: 0.9816 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1121/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0440 - accuracy: 0.9801 - val_loss: 0.0945 - val_accuracy: 0.9783\n",
      "Epoch 1122/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0451 - accuracy: 0.9816 - val_loss: 0.0935 - val_accuracy: 0.9814\n",
      "Epoch 1123/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 1124/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0413 - accuracy: 0.9832 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 1125/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.0923 - val_accuracy: 0.9752\n",
      "Epoch 1126/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0414 - accuracy: 0.9847 - val_loss: 0.0928 - val_accuracy: 0.9752\n",
      "Epoch 1127/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0408 - accuracy: 0.9862 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "Epoch 1128/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1129/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0403 - accuracy: 0.9847 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 1130/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0410 - accuracy: 0.9832 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
      "Epoch 1131/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0405 - accuracy: 0.9847 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "Epoch 1132/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1133/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0418 - accuracy: 0.9847 - val_loss: 0.1022 - val_accuracy: 0.9783\n",
      "Epoch 1134/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.0936 - val_accuracy: 0.9783\n",
      "Epoch 1135/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0398 - accuracy: 0.9862 - val_loss: 0.0937 - val_accuracy: 0.9783\n",
      "Epoch 1136/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0443 - accuracy: 0.9832 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 1137/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
      "Epoch 1138/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0403 - accuracy: 0.9832 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1139/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.0930 - val_accuracy: 0.9752\n",
      "Epoch 1140/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 0.0931 - val_accuracy: 0.9752\n",
      "Epoch 1141/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0418 - accuracy: 0.9847 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
      "Epoch 1142/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0403 - accuracy: 0.9847 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1143/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0433 - accuracy: 0.9832 - val_loss: 0.0941 - val_accuracy: 0.9752\n",
      "Epoch 1144/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0945 - val_accuracy: 0.9720\n",
      "Epoch 1145/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0459 - accuracy: 0.9832 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1146/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.0986 - val_accuracy: 0.9814\n",
      "Epoch 1147/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0425 - accuracy: 0.9816 - val_loss: 0.1108 - val_accuracy: 0.9752\n",
      "Epoch 1148/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0487 - accuracy: 0.9816 - val_loss: 0.0933 - val_accuracy: 0.9783\n",
      "Epoch 1149/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0461 - accuracy: 0.9847 - val_loss: 0.0935 - val_accuracy: 0.9783\n",
      "Epoch 1150/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0419 - accuracy: 0.9832 - val_loss: 0.0988 - val_accuracy: 0.9814\n",
      "Epoch 1151/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0443 - accuracy: 0.9832 - val_loss: 0.1053 - val_accuracy: 0.9783\n",
      "Epoch 1152/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0484 - accuracy: 0.9816 - val_loss: 0.0917 - val_accuracy: 0.9752\n",
      "Epoch 1153/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0392 - accuracy: 0.9847 - val_loss: 0.0924 - val_accuracy: 0.9783\n",
      "Epoch 1154/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0438 - accuracy: 0.9832 - val_loss: 0.0921 - val_accuracy: 0.9814\n",
      "Epoch 1155/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0398 - accuracy: 0.9832 - val_loss: 0.1021 - val_accuracy: 0.9814\n",
      "Epoch 1156/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0450 - accuracy: 0.9832 - val_loss: 0.0941 - val_accuracy: 0.9814\n",
      "Epoch 1157/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0420 - accuracy: 0.9786 - val_loss: 0.0909 - val_accuracy: 0.9752\n",
      "Epoch 1158/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0402 - accuracy: 0.9877 - val_loss: 0.0922 - val_accuracy: 0.9783\n",
      "Epoch 1159/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0395 - accuracy: 0.9832 - val_loss: 0.0938 - val_accuracy: 0.9783\n",
      "Epoch 1160/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0397 - accuracy: 0.9862 - val_loss: 0.0926 - val_accuracy: 0.9752\n",
      "Epoch 1161/1500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 0.0933 - val_accuracy: 0.9783\n",
      "Epoch 1162/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0386 - accuracy: 0.9877 - val_loss: 0.0982 - val_accuracy: 0.9814\n",
      "Epoch 1163/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0409 - accuracy: 0.9847 - val_loss: 0.0991 - val_accuracy: 0.9814\n",
      "Epoch 1164/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0410 - accuracy: 0.9816 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 1165/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0387 - accuracy: 0.9832 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1166/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0405 - accuracy: 0.9847 - val_loss: 0.0951 - val_accuracy: 0.9783\n",
      "Epoch 1167/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0440 - accuracy: 0.9832 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1168/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0418 - accuracy: 0.9832 - val_loss: 0.1138 - val_accuracy: 0.9720\n",
      "Epoch 1169/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0494 - accuracy: 0.9832 - val_loss: 0.0937 - val_accuracy: 0.9752\n",
      "Epoch 1170/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0415 - accuracy: 0.9877 - val_loss: 0.0981 - val_accuracy: 0.9720\n",
      "Epoch 1171/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0493 - accuracy: 0.9832 - val_loss: 0.0944 - val_accuracy: 0.9752\n",
      "Epoch 1172/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0398 - accuracy: 0.9877 - val_loss: 0.1080 - val_accuracy: 0.9783\n",
      "Epoch 1173/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0467 - accuracy: 0.9832 - val_loss: 0.0958 - val_accuracy: 0.9783\n",
      "Epoch 1174/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 1175/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0441 - accuracy: 0.9832 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1176/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0417 - accuracy: 0.9847 - val_loss: 0.0975 - val_accuracy: 0.9814\n",
      "Epoch 1177/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0409 - accuracy: 0.9832 - val_loss: 0.1073 - val_accuracy: 0.9783\n",
      "Epoch 1178/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0454 - accuracy: 0.9816 - val_loss: 0.0944 - val_accuracy: 0.9752\n",
      "Epoch 1179/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0409 - accuracy: 0.9847 - val_loss: 0.0952 - val_accuracy: 0.9720\n",
      "Epoch 1180/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0437 - accuracy: 0.9832 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "Epoch 1181/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0390 - accuracy: 0.9877 - val_loss: 0.0999 - val_accuracy: 0.9814\n",
      "Epoch 1182/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0404 - accuracy: 0.9847 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1183/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0385 - accuracy: 0.9816 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1184/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0397 - accuracy: 0.9847 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1185/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1186/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0391 - accuracy: 0.9832 - val_loss: 0.0957 - val_accuracy: 0.9752\n",
      "Epoch 1187/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 1188/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0383 - accuracy: 0.9862 - val_loss: 0.0920 - val_accuracy: 0.9752\n",
      "Epoch 1189/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 0.0922 - val_accuracy: 0.9752\n",
      "Epoch 1190/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0385 - accuracy: 0.9877 - val_loss: 0.0957 - val_accuracy: 0.9783\n",
      "Epoch 1191/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0385 - accuracy: 0.9816 - val_loss: 0.0982 - val_accuracy: 0.9783\n",
      "Epoch 1192/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0394 - accuracy: 0.9847 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1193/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0385 - accuracy: 0.9832 - val_loss: 0.0957 - val_accuracy: 0.9783\n",
      "Epoch 1194/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.0954 - val_accuracy: 0.9783\n",
      "Epoch 1195/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0381 - accuracy: 0.9862 - val_loss: 0.0958 - val_accuracy: 0.9783\n",
      "Epoch 1196/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0379 - accuracy: 0.9862 - val_loss: 0.0954 - val_accuracy: 0.9783\n",
      "Epoch 1197/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.0952 - val_accuracy: 0.9752\n",
      "Epoch 1198/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0382 - accuracy: 0.9877 - val_loss: 0.0944 - val_accuracy: 0.9752\n",
      "Epoch 1199/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "Epoch 1200/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0948 - val_accuracy: 0.9783\n",
      "Epoch 1201/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0386 - accuracy: 0.9862 - val_loss: 0.0953 - val_accuracy: 0.9783\n",
      "Epoch 1202/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0385 - accuracy: 0.9862 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1203/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0372 - accuracy: 0.9847 - val_loss: 0.1004 - val_accuracy: 0.9783\n",
      "Epoch 1204/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0402 - accuracy: 0.9832 - val_loss: 0.0973 - val_accuracy: 0.9783\n",
      "Epoch 1205/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0397 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9752\n",
      "Epoch 1206/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0407 - accuracy: 0.9847 - val_loss: 0.0963 - val_accuracy: 0.9752\n",
      "Epoch 1207/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0372 - accuracy: 0.9847 - val_loss: 0.1020 - val_accuracy: 0.9814\n",
      "Epoch 1208/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0401 - accuracy: 0.9847 - val_loss: 0.1015 - val_accuracy: 0.9814\n",
      "Epoch 1209/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0401 - accuracy: 0.9862 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "Epoch 1210/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0378 - accuracy: 0.9877 - val_loss: 0.0947 - val_accuracy: 0.9752\n",
      "Epoch 1211/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.0991 - val_accuracy: 0.9752\n",
      "Epoch 1212/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0426 - accuracy: 0.9832 - val_loss: 0.0981 - val_accuracy: 0.9752\n",
      "Epoch 1213/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1214/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0398 - accuracy: 0.9847 - val_loss: 0.0953 - val_accuracy: 0.9783\n",
      "Epoch 1215/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.0963 - val_accuracy: 0.9814\n",
      "Epoch 1216/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 0.1004 - val_accuracy: 0.9845\n",
      "Epoch 1217/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0435 - accuracy: 0.9832 - val_loss: 0.0951 - val_accuracy: 0.9814\n",
      "Epoch 1218/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0380 - accuracy: 0.9847 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "Epoch 1219/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0418 - accuracy: 0.9832 - val_loss: 0.0942 - val_accuracy: 0.9752\n",
      "Epoch 1220/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0393 - accuracy: 0.9877 - val_loss: 0.1026 - val_accuracy: 0.9783\n",
      "Epoch 1221/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0421 - accuracy: 0.9847 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1222/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0380 - accuracy: 0.9893 - val_loss: 0.0952 - val_accuracy: 0.9752\n",
      "Epoch 1223/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0379 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1224/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1225/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0372 - accuracy: 0.9847 - val_loss: 0.0983 - val_accuracy: 0.9783\n",
      "Epoch 1226/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0371 - accuracy: 0.9862 - val_loss: 0.0977 - val_accuracy: 0.9783\n",
      "Epoch 1227/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0374 - accuracy: 0.9847 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1228/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0377 - accuracy: 0.9832 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1229/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0367 - accuracy: 0.9877 - val_loss: 0.0960 - val_accuracy: 0.9752\n",
      "Epoch 1230/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0376 - accuracy: 0.9862 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1231/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0369 - accuracy: 0.9893 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1232/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0380 - accuracy: 0.9862 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1233/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0366 - accuracy: 0.9847 - val_loss: 0.1007 - val_accuracy: 0.9814\n",
      "Epoch 1234/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0396 - accuracy: 0.9832 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1235/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0375 - accuracy: 0.9862 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "Epoch 1236/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0374 - accuracy: 0.9877 - val_loss: 0.0937 - val_accuracy: 0.9783\n",
      "Epoch 1237/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0394 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9814\n",
      "Epoch 1238/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0371 - accuracy: 0.9862 - val_loss: 0.0980 - val_accuracy: 0.9783\n",
      "Epoch 1239/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0376 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1240/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0372 - accuracy: 0.9862 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1241/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0365 - accuracy: 0.9862 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1242/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0367 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1243/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0366 - accuracy: 0.9862 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1244/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0368 - accuracy: 0.9847 - val_loss: 0.0956 - val_accuracy: 0.9783\n",
      "Epoch 1245/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0373 - accuracy: 0.9862 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "Epoch 1246/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1247/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0363 - accuracy: 0.9877 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1248/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0366 - accuracy: 0.9847 - val_loss: 0.0945 - val_accuracy: 0.9814\n",
      "Epoch 1249/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0366 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1250/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0391 - accuracy: 0.9847 - val_loss: 0.0942 - val_accuracy: 0.9814\n",
      "Epoch 1251/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0366 - accuracy: 0.9862 - val_loss: 0.1001 - val_accuracy: 0.9814\n",
      "Epoch 1252/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0385 - accuracy: 0.9862 - val_loss: 0.1017 - val_accuracy: 0.9814\n",
      "Epoch 1253/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0386 - accuracy: 0.9862 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1254/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0365 - accuracy: 0.9862 - val_loss: 0.0965 - val_accuracy: 0.9752\n",
      "Epoch 1255/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0373 - accuracy: 0.9877 - val_loss: 0.0979 - val_accuracy: 0.9783\n",
      "Epoch 1256/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0364 - accuracy: 0.9877 - val_loss: 0.0987 - val_accuracy: 0.9783\n",
      "Epoch 1257/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0361 - accuracy: 0.9862 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1258/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0364 - accuracy: 0.9862 - val_loss: 0.0984 - val_accuracy: 0.9752\n",
      "Epoch 1259/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0368 - accuracy: 0.9877 - val_loss: 0.0978 - val_accuracy: 0.9752\n",
      "Epoch 1260/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0363 - accuracy: 0.9847 - val_loss: 0.0986 - val_accuracy: 0.9783\n",
      "Epoch 1261/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0986 - val_accuracy: 0.9783\n",
      "Epoch 1262/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0359 - accuracy: 0.9847 - val_loss: 0.0982 - val_accuracy: 0.9814\n",
      "Epoch 1263/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0360 - accuracy: 0.9847 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 1264/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0364 - accuracy: 0.9862 - val_loss: 0.0953 - val_accuracy: 0.9783\n",
      "Epoch 1265/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1266/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0358 - accuracy: 0.9832 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "Epoch 1267/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0358 - accuracy: 0.9832 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 1268/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1269/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0353 - accuracy: 0.9832 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1270/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0355 - accuracy: 0.9847 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1271/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0351 - accuracy: 0.9847 - val_loss: 0.0965 - val_accuracy: 0.9814\n",
      "Epoch 1272/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0362 - accuracy: 0.9862 - val_loss: 0.0965 - val_accuracy: 0.9752\n",
      "Epoch 1273/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0364 - accuracy: 0.9847 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1274/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1275/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0359 - accuracy: 0.9877 - val_loss: 0.0957 - val_accuracy: 0.9752\n",
      "Epoch 1276/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0357 - accuracy: 0.9877 - val_loss: 0.0947 - val_accuracy: 0.9752\n",
      "Epoch 1277/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0358 - accuracy: 0.9862 - val_loss: 0.0947 - val_accuracy: 0.9783\n",
      "Epoch 1278/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0961 - val_accuracy: 0.9814\n",
      "Epoch 1279/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0355 - accuracy: 0.9847 - val_loss: 0.0977 - val_accuracy: 0.9814\n",
      "Epoch 1280/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0361 - accuracy: 0.9832 - val_loss: 0.0974 - val_accuracy: 0.9814\n",
      "Epoch 1281/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0355 - accuracy: 0.9832 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "Epoch 1282/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0352 - accuracy: 0.9862 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "Epoch 1283/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0359 - accuracy: 0.9877 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1284/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0344 - accuracy: 0.9893 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1285/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0362 - accuracy: 0.9847 - val_loss: 0.0954 - val_accuracy: 0.9783\n",
      "Epoch 1286/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0350 - accuracy: 0.9893 - val_loss: 0.0944 - val_accuracy: 0.9783\n",
      "Epoch 1287/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0367 - accuracy: 0.9877 - val_loss: 0.0940 - val_accuracy: 0.9783\n",
      "Epoch 1288/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1289/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0347 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1290/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0352 - accuracy: 0.9832 - val_loss: 0.0956 - val_accuracy: 0.9814\n",
      "Epoch 1291/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0349 - accuracy: 0.9862 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1292/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0345 - accuracy: 0.9862 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1293/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0339 - accuracy: 0.9877 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "Epoch 1294/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 1295/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0352 - accuracy: 0.9877 - val_loss: 0.0958 - val_accuracy: 0.9752\n",
      "Epoch 1296/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1297/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 0.0991 - val_accuracy: 0.9783\n",
      "Epoch 1298/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0354 - accuracy: 0.9847 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1299/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0335 - accuracy: 0.9893 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "Epoch 1300/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0358 - accuracy: 0.9862 - val_loss: 0.0954 - val_accuracy: 0.9752\n",
      "Epoch 1301/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0356 - accuracy: 0.9847 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1302/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0340 - accuracy: 0.9847 - val_loss: 0.0970 - val_accuracy: 0.9783\n",
      "Epoch 1303/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0338 - accuracy: 0.9847 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "Epoch 1304/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0340 - accuracy: 0.9877 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1305/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0352 - accuracy: 0.9877 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1306/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.0983 - val_accuracy: 0.9783\n",
      "Epoch 1307/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0362 - accuracy: 0.9862 - val_loss: 0.1029 - val_accuracy: 0.9814\n",
      "Epoch 1308/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "Epoch 1309/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "Epoch 1310/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1311/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0353 - accuracy: 0.9862 - val_loss: 0.0984 - val_accuracy: 0.9783\n",
      "Epoch 1312/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0339 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1313/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1314/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0357 - accuracy: 0.9877 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1315/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1316/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0352 - accuracy: 0.9832 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1317/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0350 - accuracy: 0.9877 - val_loss: 0.0959 - val_accuracy: 0.9752\n",
      "Epoch 1318/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0338 - accuracy: 0.9877 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1319/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0392 - accuracy: 0.9832 - val_loss: 0.0966 - val_accuracy: 0.9752\n",
      "Epoch 1320/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0323 - accuracy: 0.9862 - val_loss: 0.1114 - val_accuracy: 0.9783\n",
      "Epoch 1321/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.1025 - val_accuracy: 0.9783\n",
      "Epoch 1322/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0985 - val_accuracy: 0.9783\n",
      "Epoch 1323/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0415 - accuracy: 0.9816 - val_loss: 0.0993 - val_accuracy: 0.9783\n",
      "Epoch 1324/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0412 - accuracy: 0.9847 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1325/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0338 - accuracy: 0.9862 - val_loss: 0.1068 - val_accuracy: 0.9783\n",
      "Epoch 1326/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0387 - accuracy: 0.9847 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1327/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0353 - accuracy: 0.9877 - val_loss: 0.0964 - val_accuracy: 0.9783\n",
      "Epoch 1328/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0356 - accuracy: 0.9832 - val_loss: 0.0979 - val_accuracy: 0.9783\n",
      "Epoch 1329/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0334 - accuracy: 0.9877 - val_loss: 0.1063 - val_accuracy: 0.9783\n",
      "Epoch 1330/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0392 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1331/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0337 - accuracy: 0.9862 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1332/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0364 - accuracy: 0.9862 - val_loss: 0.0963 - val_accuracy: 0.9752\n",
      "Epoch 1333/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0333 - accuracy: 0.9877 - val_loss: 0.1027 - val_accuracy: 0.9783\n",
      "Epoch 1334/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0355 - accuracy: 0.9877 - val_loss: 0.1019 - val_accuracy: 0.9783\n",
      "Epoch 1335/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1336/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0355 - accuracy: 0.9832 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 1337/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0374 - accuracy: 0.9847 - val_loss: 0.0983 - val_accuracy: 0.9783\n",
      "Epoch 1338/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0336 - accuracy: 0.9847 - val_loss: 0.1031 - val_accuracy: 0.9814\n",
      "Epoch 1339/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0359 - accuracy: 0.9877 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 1340/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0340 - accuracy: 0.9893 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 1341/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0370 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9752\n",
      "Epoch 1342/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0336 - accuracy: 0.9862 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 1343/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0325 - accuracy: 0.9877 - val_loss: 0.0957 - val_accuracy: 0.9814\n",
      "Epoch 1344/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0343 - accuracy: 0.9862 - val_loss: 0.0974 - val_accuracy: 0.9814\n",
      "Epoch 1345/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0339 - accuracy: 0.9816 - val_loss: 0.0995 - val_accuracy: 0.9783\n",
      "Epoch 1346/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1347/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0327 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1348/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0365 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1349/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1350/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0329 - accuracy: 0.9877 - val_loss: 0.1017 - val_accuracy: 0.9783\n",
      "Epoch 1351/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0347 - accuracy: 0.9847 - val_loss: 0.0980 - val_accuracy: 0.9814\n",
      "Epoch 1352/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0339 - accuracy: 0.9877 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1353/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0368 - accuracy: 0.9847 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1354/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0341 - accuracy: 0.9862 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
      "Epoch 1355/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0329 - accuracy: 0.9862 - val_loss: 0.1018 - val_accuracy: 0.9783\n",
      "Epoch 1356/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0349 - accuracy: 0.9847 - val_loss: 0.0993 - val_accuracy: 0.9783\n",
      "Epoch 1357/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0328 - accuracy: 0.9862 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1358/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0321 - accuracy: 0.9862 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "Epoch 1359/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.0951 - val_accuracy: 0.9783\n",
      "Epoch 1360/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0323 - accuracy: 0.9862 - val_loss: 0.0953 - val_accuracy: 0.9783\n",
      "Epoch 1361/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0319 - accuracy: 0.9877 - val_loss: 0.0942 - val_accuracy: 0.9783\n",
      "Epoch 1362/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.0942 - val_accuracy: 0.9752\n",
      "Epoch 1363/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0947 - val_accuracy: 0.9783\n",
      "Epoch 1364/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1365/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.1015 - val_accuracy: 0.9752\n",
      "Epoch 1366/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0346 - accuracy: 0.9862 - val_loss: 0.1016 - val_accuracy: 0.9783\n",
      "Epoch 1367/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0331 - accuracy: 0.9862 - val_loss: 0.0974 - val_accuracy: 0.9783\n",
      "Epoch 1368/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0325 - accuracy: 0.9877 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 1369/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0346 - accuracy: 0.9862 - val_loss: 0.0999 - val_accuracy: 0.9783\n",
      "Epoch 1370/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0330 - accuracy: 0.9862 - val_loss: 0.1090 - val_accuracy: 0.9814\n",
      "Epoch 1371/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0380 - accuracy: 0.9847 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1372/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0320 - accuracy: 0.9893 - val_loss: 0.0973 - val_accuracy: 0.9814\n",
      "Epoch 1373/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0368 - accuracy: 0.9832 - val_loss: 0.0954 - val_accuracy: 0.9783\n",
      "Epoch 1374/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0330 - accuracy: 0.9847 - val_loss: 0.0985 - val_accuracy: 0.9783\n",
      "Epoch 1375/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0329 - accuracy: 0.9862 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1376/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0319 - accuracy: 0.9862 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "Epoch 1377/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0335 - accuracy: 0.9862 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "Epoch 1378/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0324 - accuracy: 0.9862 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1379/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0321 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1380/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0328 - accuracy: 0.9877 - val_loss: 0.0965 - val_accuracy: 0.9783\n",
      "Epoch 1381/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0316 - accuracy: 0.9877 - val_loss: 0.0977 - val_accuracy: 0.9783\n",
      "Epoch 1382/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0311 - accuracy: 0.9877 - val_loss: 0.0971 - val_accuracy: 0.9752\n",
      "Epoch 1383/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0323 - accuracy: 0.9877 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1384/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 0.1003 - val_accuracy: 0.9783\n",
      "Epoch 1385/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0335 - accuracy: 0.9862 - val_loss: 0.1019 - val_accuracy: 0.9783\n",
      "Epoch 1386/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0336 - accuracy: 0.9862 - val_loss: 0.0956 - val_accuracy: 0.9752\n",
      "Epoch 1387/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0320 - accuracy: 0.9877 - val_loss: 0.0956 - val_accuracy: 0.9783\n",
      "Epoch 1388/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0333 - accuracy: 0.9862 - val_loss: 0.0958 - val_accuracy: 0.9783\n",
      "Epoch 1389/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1390/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0321 - accuracy: 0.9847 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1391/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.0957 - val_accuracy: 0.9752\n",
      "Epoch 1392/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0319 - accuracy: 0.9877 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1393/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0314 - accuracy: 0.9877 - val_loss: 0.0973 - val_accuracy: 0.9783\n",
      "Epoch 1394/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0309 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1395/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0308 - accuracy: 0.9893 - val_loss: 0.0982 - val_accuracy: 0.9783\n",
      "Epoch 1396/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.0999 - val_accuracy: 0.9783\n",
      "Epoch 1397/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0314 - accuracy: 0.9847 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1398/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0307 - accuracy: 0.9908 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "Epoch 1399/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1400/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.1000 - val_accuracy: 0.9783\n",
      "Epoch 1401/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0324 - accuracy: 0.9862 - val_loss: 0.0958 - val_accuracy: 0.9783\n",
      "Epoch 1402/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.0948 - val_accuracy: 0.9814\n",
      "Epoch 1403/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0335 - accuracy: 0.9862 - val_loss: 0.0947 - val_accuracy: 0.9783\n",
      "Epoch 1404/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0324 - accuracy: 0.9877 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1405/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0336 - accuracy: 0.9847 - val_loss: 0.1037 - val_accuracy: 0.9783\n",
      "Epoch 1406/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0363 - accuracy: 0.9847 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "Epoch 1407/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0318 - accuracy: 0.9877 - val_loss: 0.0974 - val_accuracy: 0.9814\n",
      "Epoch 1408/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0362 - accuracy: 0.9847 - val_loss: 0.0964 - val_accuracy: 0.9814\n",
      "Epoch 1409/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0318 - accuracy: 0.9893 - val_loss: 0.1015 - val_accuracy: 0.9783\n",
      "Epoch 1410/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0325 - accuracy: 0.9862 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1411/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0311 - accuracy: 0.9877 - val_loss: 0.0973 - val_accuracy: 0.9783\n",
      "Epoch 1412/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0319 - accuracy: 0.9877 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1413/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0321 - accuracy: 0.9862 - val_loss: 0.0969 - val_accuracy: 0.9783\n",
      "Epoch 1414/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1415/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.0970 - val_accuracy: 0.9783\n",
      "Epoch 1416/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.0966 - val_accuracy: 0.9783\n",
      "Epoch 1417/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.0979 - val_accuracy: 0.9783\n",
      "Epoch 1418/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0976 - val_accuracy: 0.9783\n",
      "Epoch 1419/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0976 - val_accuracy: 0.9783\n",
      "Epoch 1420/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.0990 - val_accuracy: 0.9783\n",
      "Epoch 1421/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1422/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1423/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1424/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0300 - accuracy: 0.9877 - val_loss: 0.0953 - val_accuracy: 0.9783\n",
      "Epoch 1425/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.0947 - val_accuracy: 0.9814\n",
      "Epoch 1426/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.0945 - val_accuracy: 0.9814\n",
      "Epoch 1427/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1428/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0296 - accuracy: 0.9877 - val_loss: 0.1029 - val_accuracy: 0.9783\n",
      "Epoch 1429/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0340 - accuracy: 0.9862 - val_loss: 0.0974 - val_accuracy: 0.9783\n",
      "Epoch 1430/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 1431/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1432/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.1005 - val_accuracy: 0.9752\n",
      "Epoch 1433/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1434/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 0.0967 - val_accuracy: 0.9814\n",
      "Epoch 1435/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0338 - accuracy: 0.9877 - val_loss: 0.0984 - val_accuracy: 0.9783\n",
      "Epoch 1436/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0294 - accuracy: 0.9862 - val_loss: 0.1169 - val_accuracy: 0.9720\n",
      "Epoch 1437/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0429 - accuracy: 0.9847 - val_loss: 0.0967 - val_accuracy: 0.9783\n",
      "Epoch 1438/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.1042 - val_accuracy: 0.9814\n",
      "Epoch 1439/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0455 - accuracy: 0.9832 - val_loss: 0.0971 - val_accuracy: 0.9783\n",
      "Epoch 1440/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0376 - accuracy: 0.9862 - val_loss: 0.1078 - val_accuracy: 0.9783\n",
      "Epoch 1441/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "Epoch 1442/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.0969 - val_accuracy: 0.9814\n",
      "Epoch 1443/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0342 - accuracy: 0.9862 - val_loss: 0.0961 - val_accuracy: 0.9814\n",
      "Epoch 1444/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1445/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0310 - accuracy: 0.9877 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "Epoch 1446/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0340 - accuracy: 0.9862 - val_loss: 0.0968 - val_accuracy: 0.9783\n",
      "Epoch 1447/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1448/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1449/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1450/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1451/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0294 - accuracy: 0.9862 - val_loss: 0.0944 - val_accuracy: 0.9783\n",
      "Epoch 1452/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0310 - accuracy: 0.9877 - val_loss: 0.0938 - val_accuracy: 0.9783\n",
      "Epoch 1453/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "Epoch 1454/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0296 - accuracy: 0.9893 - val_loss: 0.0993 - val_accuracy: 0.9814\n",
      "Epoch 1455/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0314 - accuracy: 0.9893 - val_loss: 0.0972 - val_accuracy: 0.9783\n",
      "Epoch 1456/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0296 - accuracy: 0.9908 - val_loss: 0.0956 - val_accuracy: 0.9783\n",
      "Epoch 1457/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 0.0966 - val_accuracy: 0.9752\n",
      "Epoch 1458/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 0.1014 - val_accuracy: 0.9783\n",
      "Epoch 1459/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.1031 - val_accuracy: 0.9783\n",
      "Epoch 1460/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0314 - accuracy: 0.9862 - val_loss: 0.0993 - val_accuracy: 0.9814\n",
      "Epoch 1461/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0294 - accuracy: 0.9877 - val_loss: 0.0979 - val_accuracy: 0.9783\n",
      "Epoch 1462/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0299 - accuracy: 0.9877 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "Epoch 1463/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.0994 - val_accuracy: 0.9783\n",
      "Epoch 1464/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0294 - accuracy: 0.9862 - val_loss: 0.0993 - val_accuracy: 0.9783\n",
      "Epoch 1465/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0295 - accuracy: 0.9877 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "Epoch 1466/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.0954 - val_accuracy: 0.9783\n",
      "Epoch 1467/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0962 - val_accuracy: 0.9783\n",
      "Epoch 1468/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0297 - accuracy: 0.9877 - val_loss: 0.0960 - val_accuracy: 0.9783\n",
      "Epoch 1469/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 0.0964 - val_accuracy: 0.9783\n",
      "Epoch 1470/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0308 - accuracy: 0.9893 - val_loss: 0.0969 - val_accuracy: 0.9752\n",
      "Epoch 1471/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0291 - accuracy: 0.9893 - val_loss: 0.1007 - val_accuracy: 0.9752\n",
      "Epoch 1472/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.1025 - val_accuracy: 0.9783\n",
      "Epoch 1473/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0299 - accuracy: 0.9893 - val_loss: 0.0998 - val_accuracy: 0.9783\n",
      "Epoch 1474/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0316 - accuracy: 0.9877 - val_loss: 0.0990 - val_accuracy: 0.9814\n",
      "Epoch 1475/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0299 - accuracy: 0.9877 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
      "Epoch 1476/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "Epoch 1477/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0294 - accuracy: 0.9877 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 1478/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.0961 - val_accuracy: 0.9814\n",
      "Epoch 1479/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
      "Epoch 1480/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.1024 - val_accuracy: 0.9814\n",
      "Epoch 1481/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0307 - accuracy: 0.9877 - val_loss: 0.0987 - val_accuracy: 0.9783\n",
      "Epoch 1482/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.0983 - val_accuracy: 0.9783\n",
      "Epoch 1483/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.1002 - val_accuracy: 0.9783\n",
      "Epoch 1484/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 0.0997 - val_accuracy: 0.9783\n",
      "Epoch 1485/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0281 - accuracy: 0.9877 - val_loss: 0.0988 - val_accuracy: 0.9783\n",
      "Epoch 1486/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0303 - accuracy: 0.9877 - val_loss: 0.0986 - val_accuracy: 0.9783\n",
      "Epoch 1487/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.1016 - val_accuracy: 0.9783\n",
      "Epoch 1488/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0291 - accuracy: 0.9893 - val_loss: 0.0994 - val_accuracy: 0.9783\n",
      "Epoch 1489/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.0976 - val_accuracy: 0.9814\n",
      "Epoch 1490/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0319 - accuracy: 0.9877 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 1491/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1002 - val_accuracy: 0.9783\n",
      "Epoch 1492/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 0.1039 - val_accuracy: 0.9783\n",
      "Epoch 1493/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0339 - accuracy: 0.9877 - val_loss: 0.0975 - val_accuracy: 0.9783\n",
      "Epoch 1494/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0973 - val_accuracy: 0.9783\n",
      "Epoch 1495/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0301 - accuracy: 0.9877 - val_loss: 0.1032 - val_accuracy: 0.9783\n",
      "Epoch 1496/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0325 - accuracy: 0.9862 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "Epoch 1497/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.1000 - val_accuracy: 0.9814\n",
      "Epoch 1498/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0345 - accuracy: 0.9847 - val_loss: 0.0996 - val_accuracy: 0.9814\n",
      "Epoch 1499/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0327 - accuracy: 0.9877 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1500/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.1039 - val_accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 모델 저장조건 설정\n",
    "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss',\n",
    "                               verbose = 1, save_best_only=True)\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split = 0.33,\n",
    "                    epochs = 1500, batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9d4b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cc8569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.048168659210205,\n",
       "  1.5577014684677124,\n",
       "  1.0863947868347168,\n",
       "  0.7074677348136902,\n",
       "  0.7634031772613525,\n",
       "  0.7918376922607422,\n",
       "  0.5715736746788025,\n",
       "  0.5544500350952148,\n",
       "  0.583797037601471,\n",
       "  0.5876337885856628,\n",
       "  0.5595888495445251,\n",
       "  0.5127702951431274,\n",
       "  0.46334022283554077,\n",
       "  0.43525493144989014,\n",
       "  0.43260928988456726,\n",
       "  0.42808160185813904,\n",
       "  0.40878918766975403,\n",
       "  0.38989025354385376,\n",
       "  0.38130563497543335,\n",
       "  0.3763117790222168,\n",
       "  0.3709556460380554,\n",
       "  0.36053651571273804,\n",
       "  0.35014116764068604,\n",
       "  0.345323771238327,\n",
       "  0.34216615557670593,\n",
       "  0.33646252751350403,\n",
       "  0.32872119545936584,\n",
       "  0.32317525148391724,\n",
       "  0.31936195492744446,\n",
       "  0.3165075182914734,\n",
       "  0.3128441870212555,\n",
       "  0.30787864327430725,\n",
       "  0.3035833537578583,\n",
       "  0.30108368396759033,\n",
       "  0.29852694272994995,\n",
       "  0.2956794798374176,\n",
       "  0.29245525598526,\n",
       "  0.2910066545009613,\n",
       "  0.2877391576766968,\n",
       "  0.2852928936481476,\n",
       "  0.28310224413871765,\n",
       "  0.28099918365478516,\n",
       "  0.2786458134651184,\n",
       "  0.27644264698028564,\n",
       "  0.27480217814445496,\n",
       "  0.27314600348472595,\n",
       "  0.2708035111427307,\n",
       "  0.2688266336917877,\n",
       "  0.2660153806209564,\n",
       "  0.2635679244995117,\n",
       "  0.2617824375629425,\n",
       "  0.26091474294662476,\n",
       "  0.25919824838638306,\n",
       "  0.2584860026836395,\n",
       "  0.25809407234191895,\n",
       "  0.2569352388381958,\n",
       "  0.25570663809776306,\n",
       "  0.25495341420173645,\n",
       "  0.2539908289909363,\n",
       "  0.25351542234420776,\n",
       "  0.2524109482765198,\n",
       "  0.25161781907081604,\n",
       "  0.2510804831981659,\n",
       "  0.2507733404636383,\n",
       "  0.25048646330833435,\n",
       "  0.2503482699394226,\n",
       "  0.2498733550310135,\n",
       "  0.24863311648368835,\n",
       "  0.2472229301929474,\n",
       "  0.24683551490306854,\n",
       "  0.24623063206672668,\n",
       "  0.2453949749469757,\n",
       "  0.24450594186782837,\n",
       "  0.24373792111873627,\n",
       "  0.2438216358423233,\n",
       "  0.24357186257839203,\n",
       "  0.24236896634101868,\n",
       "  0.2419334352016449,\n",
       "  0.2405836433172226,\n",
       "  0.2398885190486908,\n",
       "  0.23961879312992096,\n",
       "  0.23966942727565765,\n",
       "  0.23893651366233826,\n",
       "  0.23776878416538239,\n",
       "  0.23714695870876312,\n",
       "  0.23718710243701935,\n",
       "  0.23769673705101013,\n",
       "  0.23785585165023804,\n",
       "  0.2365478277206421,\n",
       "  0.23468349874019623,\n",
       "  0.23414380848407745,\n",
       "  0.23489655554294586,\n",
       "  0.23509545624256134,\n",
       "  0.23355181515216827,\n",
       "  0.23341138660907745,\n",
       "  0.23213204741477966,\n",
       "  0.2316242754459381,\n",
       "  0.231103777885437,\n",
       "  0.23055718839168549,\n",
       "  0.23050947487354279,\n",
       "  0.2308000773191452,\n",
       "  0.2298983782529831,\n",
       "  0.2288425862789154,\n",
       "  0.2289223074913025,\n",
       "  0.22828850150108337,\n",
       "  0.22767263650894165,\n",
       "  0.22695651650428772,\n",
       "  0.22680631279945374,\n",
       "  0.22646650671958923,\n",
       "  0.2263931781053543,\n",
       "  0.22595922648906708,\n",
       "  0.225660040974617,\n",
       "  0.22541610896587372,\n",
       "  0.22499287128448486,\n",
       "  0.22399668395519257,\n",
       "  0.22317084670066833,\n",
       "  0.22347721457481384,\n",
       "  0.22328774631023407,\n",
       "  0.22254791855812073,\n",
       "  0.22176030278205872,\n",
       "  0.2211657464504242,\n",
       "  0.22072221338748932,\n",
       "  0.22033387422561646,\n",
       "  0.2197175920009613,\n",
       "  0.2193336933851242,\n",
       "  0.21889963746070862,\n",
       "  0.21840783953666687,\n",
       "  0.21832191944122314,\n",
       "  0.21769316494464874,\n",
       "  0.21768435835838318,\n",
       "  0.21711508929729462,\n",
       "  0.21662087738513947,\n",
       "  0.21603430807590485,\n",
       "  0.21552219986915588,\n",
       "  0.21505339443683624,\n",
       "  0.21473537385463715,\n",
       "  0.21397626399993896,\n",
       "  0.2139250487089157,\n",
       "  0.21451319754123688,\n",
       "  0.21425938606262207,\n",
       "  0.2127671092748642,\n",
       "  0.21169282495975494,\n",
       "  0.21358168125152588,\n",
       "  0.213529571890831,\n",
       "  0.21165697276592255,\n",
       "  0.21062590181827545,\n",
       "  0.210319384932518,\n",
       "  0.21057067811489105,\n",
       "  0.21017813682556152,\n",
       "  0.21013684570789337,\n",
       "  0.2095467746257782,\n",
       "  0.208759143948555,\n",
       "  0.2085798680782318,\n",
       "  0.207786425948143,\n",
       "  0.20770443975925446,\n",
       "  0.20611482858657837,\n",
       "  0.2067146748304367,\n",
       "  0.20770156383514404,\n",
       "  0.206758514046669,\n",
       "  0.2047031819820404,\n",
       "  0.20444515347480774,\n",
       "  0.20456953346729279,\n",
       "  0.2037496119737625,\n",
       "  0.20260299742221832,\n",
       "  0.20244678854942322,\n",
       "  0.20243486762046814,\n",
       "  0.2014036774635315,\n",
       "  0.20235098898410797,\n",
       "  0.2018166035413742,\n",
       "  0.2008071094751358,\n",
       "  0.20049789547920227,\n",
       "  0.19927683472633362,\n",
       "  0.2001543790102005,\n",
       "  0.20054811239242554,\n",
       "  0.1983315497636795,\n",
       "  0.19781465828418732,\n",
       "  0.19856654107570648,\n",
       "  0.197762131690979,\n",
       "  0.19595932960510254,\n",
       "  0.19688163697719574,\n",
       "  0.196851447224617,\n",
       "  0.1952553689479828,\n",
       "  0.19498063623905182,\n",
       "  0.1962333619594574,\n",
       "  0.1950380951166153,\n",
       "  0.19428908824920654,\n",
       "  0.19431449472904205,\n",
       "  0.19226659834384918,\n",
       "  0.19339829683303833,\n",
       "  0.19475990533828735,\n",
       "  0.19255587458610535,\n",
       "  0.19230444729328156,\n",
       "  0.19387273490428925,\n",
       "  0.191873237490654,\n",
       "  0.1891498863697052,\n",
       "  0.19077804684638977,\n",
       "  0.18920423090457916,\n",
       "  0.18811362981796265,\n",
       "  0.19025669991970062,\n",
       "  0.18934111297130585,\n",
       "  0.18714188039302826,\n",
       "  0.18695832788944244,\n",
       "  0.18689343333244324,\n",
       "  0.18663544952869415,\n",
       "  0.18561546504497528,\n",
       "  0.18508028984069824,\n",
       "  0.18408124148845673,\n",
       "  0.18465660512447357,\n",
       "  0.1846216320991516,\n",
       "  0.18393005430698395,\n",
       "  0.1825530230998993,\n",
       "  0.1825198084115982,\n",
       "  0.1827264279127121,\n",
       "  0.1823267638683319,\n",
       "  0.18041186034679413,\n",
       "  0.18151234090328217,\n",
       "  0.18547604978084564,\n",
       "  0.1813952624797821,\n",
       "  0.17925448715686798,\n",
       "  0.18187642097473145,\n",
       "  0.1822807639837265,\n",
       "  0.17967091500759125,\n",
       "  0.1785908341407776,\n",
       "  0.18095584213733673,\n",
       "  0.17698466777801514,\n",
       "  0.17749713361263275,\n",
       "  0.18059879541397095,\n",
       "  0.17869332432746887,\n",
       "  0.1757483184337616,\n",
       "  0.17750899493694305,\n",
       "  0.17676179111003876,\n",
       "  0.17371612787246704,\n",
       "  0.17518940567970276,\n",
       "  0.1749328076839447,\n",
       "  0.17360135912895203,\n",
       "  0.17403040826320648,\n",
       "  0.17292949557304382,\n",
       "  0.17153631150722504,\n",
       "  0.17084473371505737,\n",
       "  0.17075516283512115,\n",
       "  0.1700717955827713,\n",
       "  0.1699855923652649,\n",
       "  0.17064307630062103,\n",
       "  0.17025268077850342,\n",
       "  0.16847282648086548,\n",
       "  0.16787467896938324,\n",
       "  0.16873453557491302,\n",
       "  0.16682851314544678,\n",
       "  0.16745057702064514,\n",
       "  0.16792458295822144,\n",
       "  0.16589148342609406,\n",
       "  0.16511110961437225,\n",
       "  0.16476336121559143,\n",
       "  0.16477161645889282,\n",
       "  0.16451233625411987,\n",
       "  0.16332745552062988,\n",
       "  0.1626829206943512,\n",
       "  0.16227693855762482,\n",
       "  0.16161473095417023,\n",
       "  0.1619674265384674,\n",
       "  0.16070321202278137,\n",
       "  0.16046306490898132,\n",
       "  0.16077257692813873,\n",
       "  0.15980306267738342,\n",
       "  0.15910027921199799,\n",
       "  0.15848110616207123,\n",
       "  0.15808077156543732,\n",
       "  0.1577213853597641,\n",
       "  0.1570289134979248,\n",
       "  0.1582403928041458,\n",
       "  0.1578030288219452,\n",
       "  0.15622390806674957,\n",
       "  0.15500909090042114,\n",
       "  0.15458698570728302,\n",
       "  0.15443293750286102,\n",
       "  0.153855100274086,\n",
       "  0.15318506956100464,\n",
       "  0.15299931168556213,\n",
       "  0.154524564743042,\n",
       "  0.15278203785419464,\n",
       "  0.15420247614383698,\n",
       "  0.15300463140010834,\n",
       "  0.15229986608028412,\n",
       "  0.15236812829971313,\n",
       "  0.15117500722408295,\n",
       "  0.1501794159412384,\n",
       "  0.14876507222652435,\n",
       "  0.1499234437942505,\n",
       "  0.1489042192697525,\n",
       "  0.14889773726463318,\n",
       "  0.1473211944103241,\n",
       "  0.1487427055835724,\n",
       "  0.14792396128177643,\n",
       "  0.14713743329048157,\n",
       "  0.1467680037021637,\n",
       "  0.14541679620742798,\n",
       "  0.1458977907896042,\n",
       "  0.1443873941898346,\n",
       "  0.14598602056503296,\n",
       "  0.14574986696243286,\n",
       "  0.1463530957698822,\n",
       "  0.1443876028060913,\n",
       "  0.14381849765777588,\n",
       "  0.14312681555747986,\n",
       "  0.14303548634052277,\n",
       "  0.14273664355278015,\n",
       "  0.14239637553691864,\n",
       "  0.14130017161369324,\n",
       "  0.14297783374786377,\n",
       "  0.13996310532093048,\n",
       "  0.1432027369737625,\n",
       "  0.14190977811813354,\n",
       "  0.1419539600610733,\n",
       "  0.14264023303985596,\n",
       "  0.14214375615119934,\n",
       "  0.1426236629486084,\n",
       "  0.13795433938503265,\n",
       "  0.13900671899318695,\n",
       "  0.13992789387702942,\n",
       "  0.1385905146598816,\n",
       "  0.13943374156951904,\n",
       "  0.13763229548931122,\n",
       "  0.13595490157604218,\n",
       "  0.13709194958209991,\n",
       "  0.13585634529590607,\n",
       "  0.13663853704929352,\n",
       "  0.13606861233711243,\n",
       "  0.13501614332199097,\n",
       "  0.13618533313274384,\n",
       "  0.13408711552619934,\n",
       "  0.13602705299854279,\n",
       "  0.1348927617073059,\n",
       "  0.13385891914367676,\n",
       "  0.13485319912433624,\n",
       "  0.13215988874435425,\n",
       "  0.13263985514640808,\n",
       "  0.13207054138183594,\n",
       "  0.1320592164993286,\n",
       "  0.1336296796798706,\n",
       "  0.13110125064849854,\n",
       "  0.13133881986141205,\n",
       "  0.13022418320178986,\n",
       "  0.13077352941036224,\n",
       "  0.12901903688907623,\n",
       "  0.12981285154819489,\n",
       "  0.13051976263523102,\n",
       "  0.12846778333187103,\n",
       "  0.13193517923355103,\n",
       "  0.12813475728034973,\n",
       "  0.13025300204753876,\n",
       "  0.12788699567317963,\n",
       "  0.13401207327842712,\n",
       "  0.1336168348789215,\n",
       "  0.13076844811439514,\n",
       "  0.13262449204921722,\n",
       "  0.12805671989917755,\n",
       "  0.1271466463804245,\n",
       "  0.12430455535650253,\n",
       "  0.12656784057617188,\n",
       "  0.126347616314888,\n",
       "  0.12437334656715393,\n",
       "  0.12587414681911469,\n",
       "  0.12429710477590561,\n",
       "  0.12401879578828812,\n",
       "  0.12294783443212509,\n",
       "  0.12902316451072693,\n",
       "  0.12261815369129181,\n",
       "  0.1255679577589035,\n",
       "  0.12479832023382187,\n",
       "  0.12594090402126312,\n",
       "  0.12653842568397522,\n",
       "  0.12379924952983856,\n",
       "  0.12135592103004456,\n",
       "  0.12166459858417511,\n",
       "  0.12010032683610916,\n",
       "  0.12072698771953583,\n",
       "  0.12190914899110794,\n",
       "  0.12124919146299362,\n",
       "  0.12105774134397507,\n",
       "  0.11942999064922333,\n",
       "  0.1185934841632843,\n",
       "  0.11963871121406555,\n",
       "  0.11913277208805084,\n",
       "  0.1202424094080925,\n",
       "  0.11878944933414459,\n",
       "  0.1185099184513092,\n",
       "  0.12184291332960129,\n",
       "  0.11843652278184891,\n",
       "  0.1182926744222641,\n",
       "  0.1181066483259201,\n",
       "  0.11684215813875198,\n",
       "  0.11626522243022919,\n",
       "  0.11603030562400818,\n",
       "  0.11570083349943161,\n",
       "  0.11539541184902191,\n",
       "  0.11496864259243011,\n",
       "  0.11487061530351639,\n",
       "  0.11693235486745834,\n",
       "  0.11559207737445831,\n",
       "  0.11584287136793137,\n",
       "  0.11403392255306244,\n",
       "  0.11633050441741943,\n",
       "  0.11446838825941086,\n",
       "  0.11448957771062851,\n",
       "  0.11506325751543045,\n",
       "  0.11452087759971619,\n",
       "  0.11256846785545349,\n",
       "  0.1133788675069809,\n",
       "  0.11235006898641586,\n",
       "  0.11557105928659439,\n",
       "  0.11407843232154846,\n",
       "  0.11367407441139221,\n",
       "  0.11606188118457794,\n",
       "  0.11272186040878296,\n",
       "  0.11775028705596924,\n",
       "  0.111700139939785,\n",
       "  0.11108443886041641,\n",
       "  0.11268441379070282,\n",
       "  0.11338279396295547,\n",
       "  0.1110808402299881,\n",
       "  0.10956156253814697,\n",
       "  0.1096588745713234,\n",
       "  0.11133476346731186,\n",
       "  0.11143680661916733,\n",
       "  0.11062891036272049,\n",
       "  0.10958882421255112,\n",
       "  0.10937564074993134,\n",
       "  0.10827963799238205,\n",
       "  0.11038269102573395,\n",
       "  0.11167027056217194,\n",
       "  0.10978616774082184,\n",
       "  0.10843978822231293,\n",
       "  0.10792230814695358,\n",
       "  0.10745072364807129,\n",
       "  0.1074298694729805,\n",
       "  0.10653185099363327,\n",
       "  0.10904797911643982,\n",
       "  0.10763876885175705,\n",
       "  0.10781279951334,\n",
       "  0.10667674243450165,\n",
       "  0.10642819851636887,\n",
       "  0.10575023293495178,\n",
       "  0.10566886514425278,\n",
       "  0.10556505620479584,\n",
       "  0.10489015281200409,\n",
       "  0.10770738869905472,\n",
       "  0.10775788128376007,\n",
       "  0.10497945547103882,\n",
       "  0.10610451549291611,\n",
       "  0.10412027686834335,\n",
       "  0.10467316955327988,\n",
       "  0.10471039265394211,\n",
       "  0.10391777753829956,\n",
       "  0.10425571352243423,\n",
       "  0.10336463898420334,\n",
       "  0.10441309958696365,\n",
       "  0.10482567548751831,\n",
       "  0.10354775190353394,\n",
       "  0.10460621863603592,\n",
       "  0.10389167070388794,\n",
       "  0.10445807129144669,\n",
       "  0.10326549410820007,\n",
       "  0.10349790006875992,\n",
       "  0.1018156036734581,\n",
       "  0.10161440074443817,\n",
       "  0.10271501541137695,\n",
       "  0.10238254815340042,\n",
       "  0.10172473639249802,\n",
       "  0.1020001769065857,\n",
       "  0.10123930871486664,\n",
       "  0.10120166093111038,\n",
       "  0.10114333033561707,\n",
       "  0.10178710520267487,\n",
       "  0.10109866410493851,\n",
       "  0.10021995007991791,\n",
       "  0.10026741772890091,\n",
       "  0.10021838545799255,\n",
       "  0.09997472167015076,\n",
       "  0.10243386030197144,\n",
       "  0.10062681883573532,\n",
       "  0.10138361155986786,\n",
       "  0.09966599196195602,\n",
       "  0.100533127784729,\n",
       "  0.09986908733844757,\n",
       "  0.09925447404384613,\n",
       "  0.09876947849988937,\n",
       "  0.09883670508861542,\n",
       "  0.09919741004705429,\n",
       "  0.09788986295461655,\n",
       "  0.09904599189758301,\n",
       "  0.10156059265136719,\n",
       "  0.09887230396270752,\n",
       "  0.09975608438253403,\n",
       "  0.0971660166978836,\n",
       "  0.09991872310638428,\n",
       "  0.09800387173891068,\n",
       "  0.09831174463033676,\n",
       "  0.09854000061750412,\n",
       "  0.09651077538728714,\n",
       "  0.09614166617393494,\n",
       "  0.09803213179111481,\n",
       "  0.09644949436187744,\n",
       "  0.09695100784301758,\n",
       "  0.0972723737359047,\n",
       "  0.09628710895776749,\n",
       "  0.09886135905981064,\n",
       "  0.0990944430232048,\n",
       "  0.09568667411804199,\n",
       "  0.09499844163656235,\n",
       "  0.09596478193998337,\n",
       "  0.09790755063295364,\n",
       "  0.09766318649053574,\n",
       "  0.09621725231409073,\n",
       "  0.09419725835323334,\n",
       "  0.09541627764701843,\n",
       "  0.0954555943608284,\n",
       "  0.09669698774814606,\n",
       "  0.09566580504179001,\n",
       "  0.09536834806203842,\n",
       "  0.09284371137619019,\n",
       "  0.09369838982820511,\n",
       "  0.09398028254508972,\n",
       "  0.09475766122341156,\n",
       "  0.09231801331043243,\n",
       "  0.09331675618886948,\n",
       "  0.09292053431272507,\n",
       "  0.09254196286201477,\n",
       "  0.09213641285896301,\n",
       "  0.09238729625940323,\n",
       "  0.09244922548532486,\n",
       "  0.09190895408391953,\n",
       "  0.09248465299606323,\n",
       "  0.09166324883699417,\n",
       "  0.09188330918550491,\n",
       "  0.09159869700670242,\n",
       "  0.09099817276000977,\n",
       "  0.09132452309131622,\n",
       "  0.09154681116342545,\n",
       "  0.09181740134954453,\n",
       "  0.09181729704141617,\n",
       "  0.09212860465049744,\n",
       "  0.09205614030361176,\n",
       "  0.09048707038164139,\n",
       "  0.09368782490491867,\n",
       "  0.09049075841903687,\n",
       "  0.09153767675161362,\n",
       "  0.08875962346792221,\n",
       "  0.0936175212264061,\n",
       "  0.09092342853546143,\n",
       "  0.09334595501422882,\n",
       "  0.09108012914657593,\n",
       "  0.09157595038414001,\n",
       "  0.09235572814941406,\n",
       "  0.08940772712230682,\n",
       "  0.08980771899223328,\n",
       "  0.0895954892039299,\n",
       "  0.0895737037062645,\n",
       "  0.0880754142999649,\n",
       "  0.08924699574708939,\n",
       "  0.08857926726341248,\n",
       "  0.08990122377872467,\n",
       "  0.08729632943868637,\n",
       "  0.088251493871212,\n",
       "  0.0871332511305809,\n",
       "  0.08753453940153122,\n",
       "  0.0877595841884613,\n",
       "  0.08643192052841187,\n",
       "  0.08682991564273834,\n",
       "  0.08656799048185349,\n",
       "  0.08759435266256332,\n",
       "  0.0879749208688736,\n",
       "  0.08599168807268143,\n",
       "  0.08660852164030075,\n",
       "  0.08702167123556137,\n",
       "  0.08583062887191772,\n",
       "  0.08621640503406525,\n",
       "  0.08633816242218018,\n",
       "  0.08719100803136826,\n",
       "  0.0877404436469078,\n",
       "  0.08776005357503891,\n",
       "  0.08423206955194473,\n",
       "  0.08702952414751053,\n",
       "  0.08398081362247467,\n",
       "  0.09037186950445175,\n",
       "  0.08421938121318817,\n",
       "  0.09215913712978363,\n",
       "  0.08824221044778824,\n",
       "  0.09091901779174805,\n",
       "  0.08869923651218414,\n",
       "  0.08591242879629135,\n",
       "  0.08803503215312958,\n",
       "  0.08554425835609436,\n",
       "  0.09155277907848358,\n",
       "  0.08347232639789581,\n",
       "  0.08667242527008057,\n",
       "  0.08504576236009598,\n",
       "  0.08519034832715988,\n",
       "  0.08338025957345963,\n",
       "  0.08407142758369446,\n",
       "  0.08486293256282806,\n",
       "  0.08321359008550644,\n",
       "  0.08584161102771759,\n",
       "  0.08136163651943207,\n",
       "  0.08955081552267075,\n",
       "  0.08309996873140335,\n",
       "  0.08231393247842789,\n",
       "  0.08089661598205566,\n",
       "  0.08138211816549301,\n",
       "  0.08173219859600067,\n",
       "  0.08095654845237732,\n",
       "  0.08079729974269867,\n",
       "  0.08027328550815582,\n",
       "  0.07983961701393127,\n",
       "  0.07983497530221939,\n",
       "  0.0793134942650795,\n",
       "  0.08009043335914612,\n",
       "  0.08011771738529205,\n",
       "  0.08101173490285873,\n",
       "  0.08123984187841415,\n",
       "  0.07933077961206436,\n",
       "  0.08087308704853058,\n",
       "  0.08018700033426285,\n",
       "  0.08213365077972412,\n",
       "  0.07941722124814987,\n",
       "  0.07846303284168243,\n",
       "  0.07811133563518524,\n",
       "  0.07820005714893341,\n",
       "  0.07833243161439896,\n",
       "  0.07737629860639572,\n",
       "  0.07814808934926987,\n",
       "  0.07806400954723358,\n",
       "  0.07855883985757828,\n",
       "  0.0780220478773117,\n",
       "  0.07746171951293945,\n",
       "  0.07653168588876724,\n",
       "  0.08022016286849976,\n",
       "  0.07689904421567917,\n",
       "  0.0790676474571228,\n",
       "  0.08349549770355225,\n",
       "  0.07487659901380539,\n",
       "  0.08934274315834045,\n",
       "  0.0766575038433075,\n",
       "  0.08615183085203171,\n",
       "  0.08364420384168625,\n",
       "  0.07657069712877274,\n",
       "  0.08547894656658173,\n",
       "  0.07650041580200195,\n",
       "  0.07562162727117538,\n",
       "  0.07475755363702774,\n",
       "  0.07696353644132614,\n",
       "  0.0762791857123375,\n",
       "  0.07534494251012802,\n",
       "  0.07509838789701462,\n",
       "  0.07486649602651596,\n",
       "  0.07427863776683807,\n",
       "  0.07553721964359283,\n",
       "  0.07510143518447876,\n",
       "  0.07487983256578445,\n",
       "  0.07562319189310074,\n",
       "  0.07598663121461868,\n",
       "  0.0751500129699707,\n",
       "  0.07438947260379791,\n",
       "  0.07393737137317657,\n",
       "  0.07296762615442276,\n",
       "  0.07354875653982162,\n",
       "  0.07346075773239136,\n",
       "  0.07297170162200928,\n",
       "  0.07366076111793518,\n",
       "  0.07236567884683609,\n",
       "  0.07224901765584946,\n",
       "  0.0729701966047287,\n",
       "  0.07243182510137558,\n",
       "  0.07222326844930649,\n",
       "  0.07267158478498459,\n",
       "  0.07160760462284088,\n",
       "  0.0725485235452652,\n",
       "  0.071321040391922,\n",
       "  0.07155193388462067,\n",
       "  0.07076402753591537,\n",
       "  0.07147005200386047,\n",
       "  0.07175282388925552,\n",
       "  0.07111729681491852,\n",
       "  0.07074728608131409,\n",
       "  0.07266823947429657,\n",
       "  0.07313300669193268,\n",
       "  0.07135292887687683,\n",
       "  0.07740622758865356,\n",
       "  0.07172530889511108,\n",
       "  0.07783961296081543,\n",
       "  0.06875152140855789,\n",
       "  0.07921760529279709,\n",
       "  0.07305251806974411,\n",
       "  0.07743577659130096,\n",
       "  0.06899550557136536,\n",
       "  0.07858839631080627,\n",
       "  0.07410432398319244,\n",
       "  0.07271482795476913,\n",
       "  0.07180406153202057,\n",
       "  0.07327037304639816,\n",
       "  0.06962597370147705,\n",
       "  0.0729779377579689,\n",
       "  0.07388809323310852,\n",
       "  0.06871785968542099,\n",
       "  0.06892033666372299,\n",
       "  0.06897896528244019,\n",
       "  0.06809429824352264,\n",
       "  0.0689954161643982,\n",
       "  0.06836650520563126,\n",
       "  0.06758143752813339,\n",
       "  0.06902031600475311,\n",
       "  0.07020857185125351,\n",
       "  0.07011684030294418,\n",
       "  0.07009539753198624,\n",
       "  0.06817161291837692,\n",
       "  0.06801630556583405,\n",
       "  0.06750421971082687,\n",
       "  0.0668819472193718,\n",
       "  0.06747366487979889,\n",
       "  0.0674021765589714,\n",
       "  0.06677133589982986,\n",
       "  0.06699478626251221,\n",
       "  0.06851138919591904,\n",
       "  0.06673450767993927,\n",
       "  0.06706419587135315,\n",
       "  0.06618188321590424,\n",
       "  0.06624797731637955,\n",
       "  0.06655667722225189,\n",
       "  0.06760483235120773,\n",
       "  0.06667900085449219,\n",
       "  0.06633492559194565,\n",
       "  0.06716281920671463,\n",
       "  0.06570292264223099,\n",
       "  0.06631352007389069,\n",
       "  0.0666939839720726,\n",
       "  0.0655076876282692,\n",
       "  0.06579580157995224,\n",
       "  0.06529694050550461,\n",
       "  0.06512220948934555,\n",
       "  0.06487630307674408,\n",
       "  0.06579761952161789,\n",
       "  0.06636475771665573,\n",
       "  0.06485907733440399,\n",
       "  0.06562162935733795,\n",
       "  0.06491047143936157,\n",
       "  0.06459078192710876,\n",
       "  0.06480728089809418,\n",
       "  0.06514988839626312,\n",
       "  0.06483546644449234,\n",
       "  0.06519724428653717,\n",
       "  0.06542367488145828,\n",
       "  0.0650036484003067,\n",
       "  0.064352847635746,\n",
       "  0.0630931407213211,\n",
       "  0.06916163116693497,\n",
       "  0.0687670186161995,\n",
       "  0.06591016054153442,\n",
       "  0.06454082578420639,\n",
       "  0.06562373787164688,\n",
       "  0.06267538666725159,\n",
       "  0.06291387230157852,\n",
       "  0.06221500784158707,\n",
       "  0.0629817396402359,\n",
       "  0.06354032456874847,\n",
       "  0.062189869582653046,\n",
       "  0.0634714663028717,\n",
       "  0.06373744457960129,\n",
       "  0.06323649734258652,\n",
       "  0.06654603034257889,\n",
       "  0.06339570134878159,\n",
       "  0.06520333141088486,\n",
       "  0.06497038155794144,\n",
       "  0.06312666088342667,\n",
       "  0.06742092967033386,\n",
       "  0.06555802375078201,\n",
       "  0.0657295510172844,\n",
       "  0.0637713223695755,\n",
       "  0.07065647095441818,\n",
       "  0.06493494659662247,\n",
       "  0.06713414937257767,\n",
       "  0.06256259232759476,\n",
       "  0.06436999887228012,\n",
       "  0.06162848323583603,\n",
       "  0.06306643038988113,\n",
       "  0.06488098949193954,\n",
       "  0.0645458847284317,\n",
       "  0.06034347042441368,\n",
       "  0.06470826268196106,\n",
       "  0.0604129284620285,\n",
       "  0.06293551623821259,\n",
       "  0.06474048644304276,\n",
       "  0.06100086495280266,\n",
       "  0.06399177759885788,\n",
       "  0.05890658497810364,\n",
       "  0.06901849061250687,\n",
       "  0.06481635570526123,\n",
       "  0.06116078421473503,\n",
       "  0.060260795056819916,\n",
       "  0.06057243421673775,\n",
       "  0.060748931020498276,\n",
       "  0.05875904858112335,\n",
       "  0.06043432280421257,\n",
       "  0.06146877631545067,\n",
       "  0.05937861278653145,\n",
       "  0.059240467846393585,\n",
       "  0.059066638350486755,\n",
       "  0.058226678520441055,\n",
       "  0.061308518052101135,\n",
       "  0.059615328907966614,\n",
       "  0.05978060141205788,\n",
       "  0.05871535837650299,\n",
       "  0.059120405465364456,\n",
       "  0.059938766062259674,\n",
       "  0.058243487030267715,\n",
       "  0.05760236084461212,\n",
       "  0.05906281620264053,\n",
       "  0.06028300151228905,\n",
       "  0.05957857891917229,\n",
       "  0.059225283563137054,\n",
       "  0.05791753903031349,\n",
       "  0.05768844485282898,\n",
       "  0.05782617628574371,\n",
       "  0.05856728553771973,\n",
       "  0.05757647007703781,\n",
       "  0.058265190571546555,\n",
       "  0.056459393352270126,\n",
       "  0.05905964970588684,\n",
       "  0.058435771614313126,\n",
       "  0.05788244679570198,\n",
       "  0.05952019989490509,\n",
       "  0.05628051236271858,\n",
       "  0.05711209401488304,\n",
       "  0.05831962823867798,\n",
       "  0.05782536044716835,\n",
       "  0.056330423802137375,\n",
       "  0.05625953525304794,\n",
       "  0.05884250998497009,\n",
       "  0.058715932071208954,\n",
       "  0.06110859289765358,\n",
       "  0.0567329078912735,\n",
       "  0.05887302756309509,\n",
       "  0.05851063132286072,\n",
       "  0.05946391075849533,\n",
       "  0.061654213815927505,\n",
       "  0.05769004672765732,\n",
       "  0.05831781402230263,\n",
       "  0.06012696027755737,\n",
       "  0.05802477151155472,\n",
       "  0.056365545839071274,\n",
       "  0.059037692844867706,\n",
       "  0.05652424320578575,\n",
       "  0.05599400773644447,\n",
       "  0.05564037337899208,\n",
       "  0.05676554515957832,\n",
       "  0.057155247777700424,\n",
       "  0.058731090277433395,\n",
       "  0.056969910860061646,\n",
       "  0.05826343595981598,\n",
       "  0.05610720440745354,\n",
       "  0.0577518530189991,\n",
       "  0.05560515448451042,\n",
       "  0.05824175104498863,\n",
       "  0.054819174110889435,\n",
       "  0.05959112197160721,\n",
       "  0.05457171052694321,\n",
       "  0.0609721876680851,\n",
       "  0.052540093660354614,\n",
       "  0.06381641328334808,\n",
       "  0.056691307574510574,\n",
       "  0.06191205233335495,\n",
       "  0.0595126673579216,\n",
       "  0.057159580290317535,\n",
       "  0.05870045721530914,\n",
       "  0.05441475287079811,\n",
       "  0.05449604243040085,\n",
       "  0.053124237805604935,\n",
       "  0.054816439747810364,\n",
       "  0.05538487806916237,\n",
       "  0.05471278354525566,\n",
       "  0.05504441261291504,\n",
       "  0.05528442561626434,\n",
       "  0.054524678736925125,\n",
       "  0.05862967669963837,\n",
       "  0.054837506264448166,\n",
       "  0.06350941956043243,\n",
       "  0.05652853101491928,\n",
       "  0.06381197273731232,\n",
       "  0.05257727578282356,\n",
       "  0.06330939382314682,\n",
       "  0.05379100516438484,\n",
       "  0.058409132063388824,\n",
       "  0.057835496962070465,\n",
       "  0.052853774279356,\n",
       "  0.05600427836179733,\n",
       "  0.052721139043569565,\n",
       "  0.0517885647714138,\n",
       "  0.05211764946579933,\n",
       "  0.052358753979206085,\n",
       "  0.053133774548769,\n",
       "  0.05404271185398102,\n",
       "  0.05215032771229744,\n",
       "  0.05442599579691887,\n",
       "  0.0512477308511734,\n",
       "  0.054869409650564194,\n",
       "  0.05685487389564514,\n",
       "  0.05182717740535736,\n",
       "  0.05347723141312599,\n",
       "  0.05264393240213394,\n",
       "  0.051440123468637466,\n",
       "  0.05141681060194969,\n",
       "  0.050525158643722534,\n",
       "  0.051665280014276505,\n",
       "  0.051416512578725815,\n",
       "  0.050338536500930786,\n",
       "  0.053131040185689926,\n",
       "  0.053351160138845444,\n",
       "  0.05137312412261963,\n",
       "  0.05200165510177612,\n",
       "  0.05236075818538666,\n",
       "  0.051423586905002594,\n",
       "  0.05481947958469391,\n",
       "  0.052045684307813644,\n",
       "  0.05025576800107956,\n",
       "  0.05084361881017685,\n",
       "  0.05074689909815788,\n",
       "  0.051004957407712936,\n",
       "  0.0505119189620018,\n",
       "  0.05038797855377197,\n",
       "  0.05195680633187294,\n",
       "  0.05103535205125809,\n",
       "  0.05139072611927986,\n",
       "  0.05175922065973282,\n",
       "  0.05144068971276283,\n",
       "  0.053154416382312775,\n",
       "  0.05070995166897774,\n",
       "  0.05015849694609642,\n",
       "  0.050826262682676315,\n",
       "  0.05027894675731659,\n",
       "  0.05019822716712952,\n",
       "  0.04903478920459747,\n",
       "  0.05030728876590729,\n",
       "  0.049337055534124374,\n",
       "  0.05088629201054573,\n",
       "  0.050254810601472855,\n",
       "  0.05071615055203438,\n",
       "  0.051138196140527725,\n",
       "  0.05032816156744957,\n",
       "  0.04918723925948143,\n",
       "  0.04848184064030647,\n",
       "  0.04884123057126999,\n",
       "  0.049257948994636536,\n",
       "  0.049141496419906616,\n",
       "  0.04992273822426796,\n",
       "  0.04975433275103569,\n",
       "  0.04886358976364136,\n",
       "  0.04831656068563461,\n",
       "  0.04854798689484596,\n",
       "  0.04914945363998413,\n",
       "  0.04841313883662224,\n",
       "  0.04984749108552933,\n",
       "  0.0481538400053978,\n",
       "  0.05606357753276825,\n",
       "  0.05013151466846466,\n",
       "  0.05124029144644737,\n",
       "  0.0484333261847496,\n",
       "  0.04877981171011925,\n",
       "  0.04809707775712013,\n",
       "  0.047681573778390884,\n",
       "  0.047341302037239075,\n",
       "  0.048008594661951065,\n",
       "  0.04819560796022415,\n",
       "  0.048209331929683685,\n",
       "  0.048068251460790634,\n",
       "  0.047100454568862915,\n",
       "  0.051587898284196854,\n",
       "  0.052043650299310684,\n",
       "  0.047591473907232285,\n",
       "  0.048391420394182205,\n",
       "  0.04722204804420471,\n",
       "  0.04790651798248291,\n",
       "  0.04725686460733414,\n",
       "  0.047823745757341385,\n",
       "  0.04905039444565773,\n",
       "  0.046485137194395065,\n",
       "  0.048546697944402695,\n",
       "  0.04782488942146301,\n",
       "  0.046811543405056,\n",
       "  0.04991845041513443,\n",
       "  0.04715534672141075,\n",
       "  0.048213887959718704,\n",
       "  0.045937489718198776,\n",
       "  0.050851721316576004,\n",
       "  0.04931534454226494,\n",
       "  0.04925968125462532,\n",
       "  0.048047106713056564,\n",
       "  0.04614005610346794,\n",
       "  0.04662720113992691,\n",
       "  0.04668326675891876,\n",
       "  0.047498881816864014,\n",
       "  0.046969227492809296,\n",
       "  0.047183260321617126,\n",
       "  ...],\n",
       " 'accuracy': [0.7258805632591248,\n",
       "  0.7258805632591248,\n",
       "  0.7258805632591248,\n",
       "  0.7243491411209106,\n",
       "  0.586523711681366,\n",
       "  0.5267993807792664,\n",
       "  0.713629424571991,\n",
       "  0.7228177785873413,\n",
       "  0.7243491411209106,\n",
       "  0.7243491411209106,\n",
       "  0.7274119257926941,\n",
       "  0.7320061326026917,\n",
       "  0.7442572712898254,\n",
       "  0.7810107469558716,\n",
       "  0.7901991009712219,\n",
       "  0.8192955851554871,\n",
       "  0.8346095085144043,\n",
       "  0.8422664403915405,\n",
       "  0.8545176386833191,\n",
       "  0.8575804233551025,\n",
       "  0.8591117858886719,\n",
       "  0.8759571313858032,\n",
       "  0.8851454854011536,\n",
       "  0.9019908308982849,\n",
       "  0.9019908308982849,\n",
       "  0.9035221934318542,\n",
       "  0.9050536155700684,\n",
       "  0.9065849781036377,\n",
       "  0.9065849781036377,\n",
       "  0.9065849781036377,\n",
       "  0.9065849781036377,\n",
       "  0.9081164002418518,\n",
       "  0.9096477627754211,\n",
       "  0.9111791849136353,\n",
       "  0.9142419695854187,\n",
       "  0.9142419695854187,\n",
       "  0.915773332118988,\n",
       "  0.9127105474472046,\n",
       "  0.9127105474472046,\n",
       "  0.9127105474472046,\n",
       "  0.9127105474472046,\n",
       "  0.9142419695854187,\n",
       "  0.9142419695854187,\n",
       "  0.9173047542572021,\n",
       "  0.915773332118988,\n",
       "  0.915773332118988,\n",
       "  0.915773332118988,\n",
       "  0.915773332118988,\n",
       "  0.9173047542572021,\n",
       "  0.9188361167907715,\n",
       "  0.9188361167907715,\n",
       "  0.9218989014625549,\n",
       "  0.923430323600769,\n",
       "  0.9203675389289856,\n",
       "  0.9203675389289856,\n",
       "  0.9188361167907715,\n",
       "  0.9203675389289856,\n",
       "  0.9218989014625549,\n",
       "  0.923430323600769,\n",
       "  0.9203675389289856,\n",
       "  0.9203675389289856,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.9203675389289856,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.9249616861343384,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.923430323600769,\n",
       "  0.923430323600769,\n",
       "  0.9249616861343384,\n",
       "  0.9249616861343384,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.923430323600769,\n",
       "  0.9264931082725525,\n",
       "  0.9249616861343384,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.9249616861343384,\n",
       "  0.9264931082725525,\n",
       "  0.923430323600769,\n",
       "  0.9218989014625549,\n",
       "  0.9218989014625549,\n",
       "  0.9264931082725525,\n",
       "  0.9249616861343384,\n",
       "  0.9249616861343384,\n",
       "  0.9249616861343384,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9264931082725525,\n",
       "  0.9264931082725525,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9203675389289856,\n",
       "  0.9218989014625549,\n",
       "  0.9295558929443359,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9295558929443359,\n",
       "  0.9295558929443359,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.93108731508255,\n",
       "  0.93108731508255,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9341500997543335,\n",
       "  0.9280245304107666,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9280245304107666,\n",
       "  0.93108731508255,\n",
       "  0.9356814622879028,\n",
       "  0.93108731508255,\n",
       "  0.9264931082725525,\n",
       "  0.923430323600769,\n",
       "  0.9280245304107666,\n",
       "  0.9295558929443359,\n",
       "  0.9356814622879028,\n",
       "  0.9356814622879028,\n",
       "  0.9326186776161194,\n",
       "  0.9326186776161194,\n",
       "  0.93108731508255,\n",
       "  0.9341500997543335,\n",
       "  0.9372128844261169,\n",
       "  0.9341500997543335,\n",
       "  0.9326186776161194,\n",
       "  0.9341500997543335,\n",
       "  0.9341500997543335,\n",
       "  0.9372128844261169,\n",
       "  0.9372128844261169,\n",
       "  0.9372128844261169,\n",
       "  0.9372128844261169,\n",
       "  0.9326186776161194,\n",
       "  0.93108731508255,\n",
       "  0.9356814622879028,\n",
       "  0.9372128844261169,\n",
       "  0.9387442469596863,\n",
       "  0.9402756690979004,\n",
       "  0.9387442469596863,\n",
       "  0.9418070316314697,\n",
       "  0.9387442469596863,\n",
       "  0.9433384537696838,\n",
       "  0.9402756690979004,\n",
       "  0.9387442469596863,\n",
       "  0.9356814622879028,\n",
       "  0.9356814622879028,\n",
       "  0.9402756690979004,\n",
       "  0.9418070316314697,\n",
       "  0.9448698163032532,\n",
       "  0.9464012384414673,\n",
       "  0.9402756690979004,\n",
       "  0.9326186776161194,\n",
       "  0.9341500997543335,\n",
       "  0.9433384537696838,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9464012384414673,\n",
       "  0.9402756690979004,\n",
       "  0.9387442469596863,\n",
       "  0.9448698163032532,\n",
       "  0.9494640231132507,\n",
       "  0.9479326009750366,\n",
       "  0.9464012384414673,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9479326009750366,\n",
       "  0.9494640231132507,\n",
       "  0.9494640231132507,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9494640231132507,\n",
       "  0.9448698163032532,\n",
       "  0.9433384537696838,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9494640231132507,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9494640231132507,\n",
       "  0.9494640231132507,\n",
       "  0.9540581703186035,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9540581703186035,\n",
       "  0.9540581703186035,\n",
       "  0.9494640231132507,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9540581703186035,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9540581703186035,\n",
       "  0.9540581703186035,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9540581703186035,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9509953856468201,\n",
       "  0.9540581703186035,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9525268077850342,\n",
       "  0.9509953856468201,\n",
       "  0.9617151618003845,\n",
       "  0.964777946472168,\n",
       "  0.9540581703186035,\n",
       "  0.9509953856468201,\n",
       "  0.9525268077850342,\n",
       "  0.9601837396621704,\n",
       "  0.9617151618003845,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9540581703186035,\n",
       "  0.9632465839385986,\n",
       "  0.9555895924568176,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9601837396621704,\n",
       "  0.964777946472168,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9617151618003845,\n",
       "  0.9663093686103821,\n",
       "  0.9601837396621704,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.964777946472168,\n",
       "  0.9663093686103821,\n",
       "  0.9586523771286011,\n",
       "  0.9525268077850342,\n",
       "  0.9601837396621704,\n",
       "  0.964777946472168,\n",
       "  0.9632465839385986,\n",
       "  0.9555895924568176,\n",
       "  0.9555895924568176,\n",
       "  0.9632465839385986,\n",
       "  0.9739663004875183,\n",
       "  0.9678407311439514,\n",
       "  0.9555895924568176,\n",
       "  0.9540581703186035,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9525268077850342,\n",
       "  0.9525268077850342,\n",
       "  0.9617151618003845,\n",
       "  0.9678407311439514,\n",
       "  0.9632465839385986,\n",
       "  0.9555895924568176,\n",
       "  0.957120954990387,\n",
       "  0.964777946472168,\n",
       "  0.972434937953949,\n",
       "  0.9663093686103821,\n",
       "  0.9601837396621704,\n",
       "  0.9693721532821655,\n",
       "  0.9739663004875183,\n",
       "  0.9678407311439514,\n",
       "  0.9555895924568176,\n",
       "  0.9540581703186035,\n",
       "  0.964777946472168,\n",
       "  0.9678407311439514,\n",
       "  0.9601837396621704,\n",
       "  0.9632465839385986,\n",
       "  0.9709035158157349,\n",
       "  0.9693721532821655,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.9617151618003845,\n",
       "  0.9617151618003845,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9632465839385986,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.964777946472168,\n",
       "  0.9617151618003845,\n",
       "  0.964777946472168,\n",
       "  0.9709035158157349,\n",
       "  0.9663093686103821,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.964777946472168,\n",
       "  0.9601837396621704,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9739663004875183,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9663093686103821,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.964777946472168,\n",
       "  0.964777946472168,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9663093686103821,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9754977226257324,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9678407311439514,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9709035158157349,\n",
       "  0.9678407311439514,\n",
       "  0.9709035158157349,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.9693721532821655,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.972434937953949,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9693721532821655,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9678407311439514,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9663093686103821,\n",
       "  0.9678407311439514,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9663093686103821,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9709035158157349,\n",
       "  0.9709035158157349,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9693721532821655,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9693721532821655,\n",
       "  0.9678407311439514,\n",
       "  0.9770290851593018,\n",
       "  0.972434937953949,\n",
       "  0.9739663004875183,\n",
       "  0.9678407311439514,\n",
       "  0.9678407311439514,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9709035158157349,\n",
       "  0.9785605072975159,\n",
       "  0.9709035158157349,\n",
       "  0.9770290851593018,\n",
       "  0.9693721532821655,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9693721532821655,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.972434937953949,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9739663004875183,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9739663004875183,\n",
       "  0.972434937953949,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9785605072975159,\n",
       "  0.9739663004875183,\n",
       "  0.9800918698310852,\n",
       "  0.9754977226257324,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9739663004875183,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9770290851593018,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9754977226257324,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9770290851593018,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9770290851593018,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9831546545028687,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9862174391746521,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9770290851593018,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9800918698310852,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9770290851593018,\n",
       "  0.9846860766410828,\n",
       "  0.9816232919692993,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9846860766410828,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9785605072975159,\n",
       "  0.9785605072975159,\n",
       "  0.9831546545028687,\n",
       "  0.9831546545028687,\n",
       "  0.9816232919692993,\n",
       "  0.9816232919692993,\n",
       "  0.9831546545028687,\n",
       "  0.9800918698310852,\n",
       "  0.9785605072975159,\n",
       "  0.9846860766410828,\n",
       "  0.9831546545028687,\n",
       "  0.9800918698310852,\n",
       "  ...],\n",
       " 'val_loss': [1.3696472644805908,\n",
       "  0.9767626523971558,\n",
       "  0.6555690169334412,\n",
       "  0.7204952239990234,\n",
       "  0.8498615026473999,\n",
       "  0.5612354874610901,\n",
       "  0.49682360887527466,\n",
       "  0.5095617175102234,\n",
       "  0.5103656649589539,\n",
       "  0.4879467785358429,\n",
       "  0.44969385862350464,\n",
       "  0.4129253029823303,\n",
       "  0.40077152848243713,\n",
       "  0.41555237770080566,\n",
       "  0.4201304316520691,\n",
       "  0.395426481962204,\n",
       "  0.3659825623035431,\n",
       "  0.34737318754196167,\n",
       "  0.33685651421546936,\n",
       "  0.32892346382141113,\n",
       "  0.3215826153755188,\n",
       "  0.31745269894599915,\n",
       "  0.3187258243560791,\n",
       "  0.3209141492843628,\n",
       "  0.315132737159729,\n",
       "  0.3033420145511627,\n",
       "  0.29165586829185486,\n",
       "  0.2835754156112671,\n",
       "  0.2780337929725647,\n",
       "  0.27360987663269043,\n",
       "  0.2703114449977875,\n",
       "  0.2690448760986328,\n",
       "  0.2696975767612457,\n",
       "  0.2690184712409973,\n",
       "  0.26551756262779236,\n",
       "  0.2614074945449829,\n",
       "  0.2553754150867462,\n",
       "  0.2506581246852875,\n",
       "  0.24829357862472534,\n",
       "  0.24593473970890045,\n",
       "  0.2439837008714676,\n",
       "  0.24229110777378082,\n",
       "  0.24184595048427582,\n",
       "  0.24107865989208221,\n",
       "  0.23952805995941162,\n",
       "  0.23721233010292053,\n",
       "  0.2358115315437317,\n",
       "  0.23332150280475616,\n",
       "  0.22928057610988617,\n",
       "  0.22568796575069427,\n",
       "  0.22319142520427704,\n",
       "  0.22254984080791473,\n",
       "  0.22376450896263123,\n",
       "  0.22485283017158508,\n",
       "  0.22445963323116302,\n",
       "  0.22244930267333984,\n",
       "  0.22077621519565582,\n",
       "  0.22042948007583618,\n",
       "  0.2214084416627884,\n",
       "  0.22122040390968323,\n",
       "  0.21934553980827332,\n",
       "  0.21754491329193115,\n",
       "  0.21621856093406677,\n",
       "  0.21527594327926636,\n",
       "  0.21453826129436493,\n",
       "  0.21448810398578644,\n",
       "  0.21529749035835266,\n",
       "  0.21790802478790283,\n",
       "  0.2209775745868683,\n",
       "  0.222259521484375,\n",
       "  0.22109675407409668,\n",
       "  0.21913154423236847,\n",
       "  0.2169015109539032,\n",
       "  0.21516819298267365,\n",
       "  0.2143751084804535,\n",
       "  0.21464505791664124,\n",
       "  0.2145758718252182,\n",
       "  0.21547019481658936,\n",
       "  0.2157813310623169,\n",
       "  0.2166181057691574,\n",
       "  0.21759988367557526,\n",
       "  0.21887914836406708,\n",
       "  0.21568873524665833,\n",
       "  0.21177014708518982,\n",
       "  0.209358811378479,\n",
       "  0.20822463929653168,\n",
       "  0.2077326774597168,\n",
       "  0.2079700380563736,\n",
       "  0.2090081423521042,\n",
       "  0.2121487557888031,\n",
       "  0.2167552262544632,\n",
       "  0.21869049966335297,\n",
       "  0.21616671979427338,\n",
       "  0.21072973310947418,\n",
       "  0.20757515728473663,\n",
       "  0.20742805302143097,\n",
       "  0.20815123617649078,\n",
       "  0.20972482860088348,\n",
       "  0.21060813963413239,\n",
       "  0.21170634031295776,\n",
       "  0.21239006519317627,\n",
       "  0.20954529941082,\n",
       "  0.20679914951324463,\n",
       "  0.2057310789823532,\n",
       "  0.2068394422531128,\n",
       "  0.20587503910064697,\n",
       "  0.20400787889957428,\n",
       "  0.20333132147789001,\n",
       "  0.2044645994901657,\n",
       "  0.20586463809013367,\n",
       "  0.20643478631973267,\n",
       "  0.2075384110212326,\n",
       "  0.2087554633617401,\n",
       "  0.20660175383090973,\n",
       "  0.20309609174728394,\n",
       "  0.20064376294612885,\n",
       "  0.1996869146823883,\n",
       "  0.19996020197868347,\n",
       "  0.20026938617229462,\n",
       "  0.2001899629831314,\n",
       "  0.19982600212097168,\n",
       "  0.19973643124103546,\n",
       "  0.20083747804164886,\n",
       "  0.20176026225090027,\n",
       "  0.20246373116970062,\n",
       "  0.2019677311182022,\n",
       "  0.20048627257347107,\n",
       "  0.19941656291484833,\n",
       "  0.20046871900558472,\n",
       "  0.20091865956783295,\n",
       "  0.1987878531217575,\n",
       "  0.19665342569351196,\n",
       "  0.19642071425914764,\n",
       "  0.19710484147071838,\n",
       "  0.19751714169979095,\n",
       "  0.1966899186372757,\n",
       "  0.19495224952697754,\n",
       "  0.1940128356218338,\n",
       "  0.19391050934791565,\n",
       "  0.19459928572177887,\n",
       "  0.19844116270542145,\n",
       "  0.20185627043247223,\n",
       "  0.20148584246635437,\n",
       "  0.19670674204826355,\n",
       "  0.19374243915081024,\n",
       "  0.19230614602565765,\n",
       "  0.19188715517520905,\n",
       "  0.1934303343296051,\n",
       "  0.19716747105121613,\n",
       "  0.1989694982767105,\n",
       "  0.19912189245224,\n",
       "  0.19913382828235626,\n",
       "  0.19771330058574677,\n",
       "  0.19829590618610382,\n",
       "  0.1977873593568802,\n",
       "  0.19185170531272888,\n",
       "  0.18833018839359283,\n",
       "  0.18801136314868927,\n",
       "  0.1894409954547882,\n",
       "  0.1938997358083725,\n",
       "  0.1973186880350113,\n",
       "  0.19553405046463013,\n",
       "  0.19208437204360962,\n",
       "  0.19007273018360138,\n",
       "  0.18917569518089294,\n",
       "  0.1899200975894928,\n",
       "  0.19360868632793427,\n",
       "  0.19658109545707703,\n",
       "  0.1922936588525772,\n",
       "  0.18959932029247284,\n",
       "  0.1899515986442566,\n",
       "  0.19413286447525024,\n",
       "  0.19729356467723846,\n",
       "  0.1917826384305954,\n",
       "  0.18582046031951904,\n",
       "  0.18304656445980072,\n",
       "  0.18299329280853271,\n",
       "  0.18562428653240204,\n",
       "  0.19030970335006714,\n",
       "  0.19387732446193695,\n",
       "  0.18879349529743195,\n",
       "  0.18341955542564392,\n",
       "  0.1811390221118927,\n",
       "  0.1815664917230606,\n",
       "  0.18596680462360382,\n",
       "  0.19039855897426605,\n",
       "  0.1867479681968689,\n",
       "  0.18059039115905762,\n",
       "  0.17913156747817993,\n",
       "  0.17970900237560272,\n",
       "  0.1850879192352295,\n",
       "  0.193657785654068,\n",
       "  0.1894856095314026,\n",
       "  0.18179872632026672,\n",
       "  0.17777809500694275,\n",
       "  0.17685651779174805,\n",
       "  0.1810479313135147,\n",
       "  0.18730488419532776,\n",
       "  0.18486420810222626,\n",
       "  0.1779618114233017,\n",
       "  0.17570245265960693,\n",
       "  0.17478349804878235,\n",
       "  0.17608413100242615,\n",
       "  0.1801508665084839,\n",
       "  0.17930999398231506,\n",
       "  0.17647849023342133,\n",
       "  0.1730833351612091,\n",
       "  0.17193399369716644,\n",
       "  0.17304229736328125,\n",
       "  0.17509910464286804,\n",
       "  0.1744382530450821,\n",
       "  0.1726016104221344,\n",
       "  0.17128364741802216,\n",
       "  0.17257235944271088,\n",
       "  0.1793891042470932,\n",
       "  0.18833182752132416,\n",
       "  0.18163809180259705,\n",
       "  0.17035456001758575,\n",
       "  0.166893869638443,\n",
       "  0.16606831550598145,\n",
       "  0.1663394719362259,\n",
       "  0.17191757261753082,\n",
       "  0.18164145946502686,\n",
       "  0.17609891295433044,\n",
       "  0.16633987426757812,\n",
       "  0.16494566202163696,\n",
       "  0.16577577590942383,\n",
       "  0.17011627554893494,\n",
       "  0.17695443332195282,\n",
       "  0.177937850356102,\n",
       "  0.1685543805360794,\n",
       "  0.1629071980714798,\n",
       "  0.16202738881111145,\n",
       "  0.1647358536720276,\n",
       "  0.17315445840358734,\n",
       "  0.1717405915260315,\n",
       "  0.1651880145072937,\n",
       "  0.16271963715553284,\n",
       "  0.16373787820339203,\n",
       "  0.16463184356689453,\n",
       "  0.16227997839450836,\n",
       "  0.16020944714546204,\n",
       "  0.16005843877792358,\n",
       "  0.16223837435245514,\n",
       "  0.1630716770887375,\n",
       "  0.1655566394329071,\n",
       "  0.16480527818202972,\n",
       "  0.15802785754203796,\n",
       "  0.1563500165939331,\n",
       "  0.15750792622566223,\n",
       "  0.1578657329082489,\n",
       "  0.15686354041099548,\n",
       "  0.1554965078830719,\n",
       "  0.1551201492547989,\n",
       "  0.1559108942747116,\n",
       "  0.15723417699337006,\n",
       "  0.15844175219535828,\n",
       "  0.15844124555587769,\n",
       "  0.16049492359161377,\n",
       "  0.15855583548545837,\n",
       "  0.1531948745250702,\n",
       "  0.15159395337104797,\n",
       "  0.15259408950805664,\n",
       "  0.15533481538295746,\n",
       "  0.15589015185832977,\n",
       "  0.1527755707502365,\n",
       "  0.15099617838859558,\n",
       "  0.15280850231647491,\n",
       "  0.1586705595254898,\n",
       "  0.1583261489868164,\n",
       "  0.1533329337835312,\n",
       "  0.15145941078662872,\n",
       "  0.15161912143230438,\n",
       "  0.15239521861076355,\n",
       "  0.151132732629776,\n",
       "  0.14924347400665283,\n",
       "  0.14905868470668793,\n",
       "  0.1530168354511261,\n",
       "  0.1546044796705246,\n",
       "  0.14586509764194489,\n",
       "  0.144334077835083,\n",
       "  0.14981645345687866,\n",
       "  0.1574389785528183,\n",
       "  0.14749367535114288,\n",
       "  0.14402830600738525,\n",
       "  0.14749453961849213,\n",
       "  0.15282326936721802,\n",
       "  0.14830315113067627,\n",
       "  0.14317575097084045,\n",
       "  0.14430449903011322,\n",
       "  0.15056800842285156,\n",
       "  0.15013165771961212,\n",
       "  0.14298494160175323,\n",
       "  0.1425515115261078,\n",
       "  0.14502863585948944,\n",
       "  0.1474621295928955,\n",
       "  0.1450881063938141,\n",
       "  0.13999710977077484,\n",
       "  0.13943380117416382,\n",
       "  0.14357882738113403,\n",
       "  0.14771738648414612,\n",
       "  0.14051757752895355,\n",
       "  0.13908174633979797,\n",
       "  0.14264298975467682,\n",
       "  0.1445617824792862,\n",
       "  0.14103631675243378,\n",
       "  0.14154955744743347,\n",
       "  0.14681097865104675,\n",
       "  0.1439993679523468,\n",
       "  0.13673634827136993,\n",
       "  0.13639689981937408,\n",
       "  0.14361713826656342,\n",
       "  0.1532130092382431,\n",
       "  0.13819684088230133,\n",
       "  0.13523679971694946,\n",
       "  0.13733182847499847,\n",
       "  0.14452698826789856,\n",
       "  0.14820680022239685,\n",
       "  0.13938571512699127,\n",
       "  0.13501188158988953,\n",
       "  0.13508203625679016,\n",
       "  0.13797520101070404,\n",
       "  0.14210368692874908,\n",
       "  0.13738834857940674,\n",
       "  0.13326102495193481,\n",
       "  0.13301414251327515,\n",
       "  0.1331569105386734,\n",
       "  0.13761258125305176,\n",
       "  0.1398896425962448,\n",
       "  0.13200537860393524,\n",
       "  0.13112501800060272,\n",
       "  0.13722389936447144,\n",
       "  0.14291028678417206,\n",
       "  0.13521988689899445,\n",
       "  0.13154380023479462,\n",
       "  0.13139179348945618,\n",
       "  0.13676859438419342,\n",
       "  0.1428864747285843,\n",
       "  0.13449066877365112,\n",
       "  0.13046151399612427,\n",
       "  0.13165685534477234,\n",
       "  0.13720278441905975,\n",
       "  0.1338382214307785,\n",
       "  0.12873853743076324,\n",
       "  0.1283348649740219,\n",
       "  0.1306208372116089,\n",
       "  0.14073750376701355,\n",
       "  0.1370249092578888,\n",
       "  0.12640729546546936,\n",
       "  0.12617993354797363,\n",
       "  0.13848967850208282,\n",
       "  0.1461113542318344,\n",
       "  0.12689514458179474,\n",
       "  0.12603098154067993,\n",
       "  0.12695610523223877,\n",
       "  0.13607223331928253,\n",
       "  0.12865687906742096,\n",
       "  0.12487930059432983,\n",
       "  0.12449214607477188,\n",
       "  0.1282719522714615,\n",
       "  0.13457202911376953,\n",
       "  0.12849782407283783,\n",
       "  0.12473051995038986,\n",
       "  0.12925885617733002,\n",
       "  0.1424064040184021,\n",
       "  0.1300520896911621,\n",
       "  0.12319844216108322,\n",
       "  0.12355498969554901,\n",
       "  0.12981408834457397,\n",
       "  0.13610121607780457,\n",
       "  0.12573933601379395,\n",
       "  0.12500807642936707,\n",
       "  0.13140346109867096,\n",
       "  0.12784378230571747,\n",
       "  0.1233598068356514,\n",
       "  0.12257381528615952,\n",
       "  0.12451159209012985,\n",
       "  0.12760823965072632,\n",
       "  0.1253298819065094,\n",
       "  0.12332230061292648,\n",
       "  0.12435220927000046,\n",
       "  0.12664362788200378,\n",
       "  0.12243813276290894,\n",
       "  0.12115882337093353,\n",
       "  0.1277163326740265,\n",
       "  0.13662202656269073,\n",
       "  0.12745119631290436,\n",
       "  0.12144377827644348,\n",
       "  0.12108025699853897,\n",
       "  0.12324202060699463,\n",
       "  0.12416422367095947,\n",
       "  0.12173472344875336,\n",
       "  0.12238465994596481,\n",
       "  0.12405069917440414,\n",
       "  0.12265322357416153,\n",
       "  0.12319757789373398,\n",
       "  0.12662114202976227,\n",
       "  0.12416099011898041,\n",
       "  0.1193876788020134,\n",
       "  0.11972011625766754,\n",
       "  0.12780213356018066,\n",
       "  0.12436415255069733,\n",
       "  0.11859825253486633,\n",
       "  0.11819833517074585,\n",
       "  0.11941711604595184,\n",
       "  0.12223748117685318,\n",
       "  0.1193934977054596,\n",
       "  0.12014123797416687,\n",
       "  0.12880206108093262,\n",
       "  0.13019412755966187,\n",
       "  0.11758904904127121,\n",
       "  0.11704680323600769,\n",
       "  0.1198994442820549,\n",
       "  0.13194496929645538,\n",
       "  0.12236784398555756,\n",
       "  0.1172327771782875,\n",
       "  0.11973210424184799,\n",
       "  0.1236308217048645,\n",
       "  0.12122798711061478,\n",
       "  0.11866672337055206,\n",
       "  0.11698110401630402,\n",
       "  0.11644596606492996,\n",
       "  0.11840145289897919,\n",
       "  0.12081742286682129,\n",
       "  0.11858097463846207,\n",
       "  0.11785022914409637,\n",
       "  0.118394635617733,\n",
       "  0.12312101572751999,\n",
       "  0.12233539670705795,\n",
       "  0.11719958484172821,\n",
       "  0.11708574742078781,\n",
       "  0.11669053137302399,\n",
       "  0.1179332360625267,\n",
       "  0.11983676999807358,\n",
       "  0.11534634232521057,\n",
       "  0.11350338906049728,\n",
       "  0.1136864572763443,\n",
       "  0.1166226863861084,\n",
       "  0.11716815084218979,\n",
       "  0.11389893293380737,\n",
       "  0.11498138308525085,\n",
       "  0.11753072589635849,\n",
       "  0.11747829616069794,\n",
       "  0.11514595150947571,\n",
       "  0.11279185861349106,\n",
       "  0.11271610856056213,\n",
       "  0.1154220774769783,\n",
       "  0.12004850059747696,\n",
       "  0.11636015772819519,\n",
       "  0.1132064163684845,\n",
       "  0.11325360089540482,\n",
       "  0.1158425435423851,\n",
       "  0.11883962154388428,\n",
       "  0.1151965856552124,\n",
       "  0.11252995580434799,\n",
       "  0.11269143223762512,\n",
       "  0.11300692707300186,\n",
       "  0.11108618974685669,\n",
       "  0.11173657327890396,\n",
       "  0.11908017098903656,\n",
       "  0.11343685537576675,\n",
       "  0.11063537746667862,\n",
       "  0.11104800552129745,\n",
       "  0.11302918195724487,\n",
       "  0.11450795084238052,\n",
       "  0.1138690710067749,\n",
       "  0.11301372200250626,\n",
       "  0.11147037148475647,\n",
       "  0.11221638321876526,\n",
       "  0.11579401046037674,\n",
       "  0.11379974335432053,\n",
       "  0.11101704835891724,\n",
       "  0.11047090590000153,\n",
       "  0.11212190240621567,\n",
       "  0.11411715298891068,\n",
       "  0.11440182477235794,\n",
       "  0.11003437638282776,\n",
       "  0.10885985940694809,\n",
       "  0.10907413810491562,\n",
       "  0.11472302675247192,\n",
       "  0.11474081128835678,\n",
       "  0.10848499089479446,\n",
       "  0.10811229795217514,\n",
       "  0.11212161928415298,\n",
       "  0.11113757640123367,\n",
       "  0.10800289362668991,\n",
       "  0.10922323167324066,\n",
       "  0.11140163242816925,\n",
       "  0.1080087348818779,\n",
       "  0.10798374563455582,\n",
       "  0.10861918330192566,\n",
       "  0.11580415070056915,\n",
       "  0.10942274332046509,\n",
       "  0.10657227784395218,\n",
       "  0.10642354935407639,\n",
       "  0.11046838760375977,\n",
       "  0.11237316578626633,\n",
       "  0.10784607380628586,\n",
       "  0.107381172478199,\n",
       "  0.10643026977777481,\n",
       "  0.10690604150295258,\n",
       "  0.11258438974618912,\n",
       "  0.11562994122505188,\n",
       "  0.11031732708215714,\n",
       "  0.11025357991456985,\n",
       "  0.11183594167232513,\n",
       "  0.10966036468744278,\n",
       "  0.1069880872964859,\n",
       "  0.1071377769112587,\n",
       "  0.10655577480792999,\n",
       "  0.10707203298807144,\n",
       "  0.10813749581575394,\n",
       "  0.1095249354839325,\n",
       "  0.11175204068422318,\n",
       "  0.10822699218988419,\n",
       "  0.10656043887138367,\n",
       "  0.10881708562374115,\n",
       "  0.11197880655527115,\n",
       "  0.10751166194677353,\n",
       "  0.10569028556346893,\n",
       "  0.10761716961860657,\n",
       "  0.110967256128788,\n",
       "  0.10718782991170883,\n",
       "  0.10504033416509628,\n",
       "  0.10553755611181259,\n",
       "  0.10843833535909653,\n",
       "  0.10689552873373032,\n",
       "  0.10557835549116135,\n",
       "  0.10730797797441483,\n",
       "  0.10633324086666107,\n",
       "  0.10395684093236923,\n",
       "  0.10449278354644775,\n",
       "  0.10598084330558777,\n",
       "  0.10555154830217361,\n",
       "  0.1041046753525734,\n",
       "  0.10603868961334229,\n",
       "  0.10538168996572495,\n",
       "  0.10343814641237259,\n",
       "  0.1037364974617958,\n",
       "  0.10672444850206375,\n",
       "  0.11235694587230682,\n",
       "  0.10359147936105728,\n",
       "  0.10309965908527374,\n",
       "  0.10405813157558441,\n",
       "  0.111945740878582,\n",
       "  0.10563109070062637,\n",
       "  0.10257120430469513,\n",
       "  0.10240663588047028,\n",
       "  0.11072957515716553,\n",
       "  0.11178188771009445,\n",
       "  0.10211832076311111,\n",
       "  0.10257548838853836,\n",
       "  0.10533972829580307,\n",
       "  0.10928033292293549,\n",
       "  0.10297785699367523,\n",
       "  0.10172877460718155,\n",
       "  0.1046106219291687,\n",
       "  0.10890303552150726,\n",
       "  0.1028415635228157,\n",
       "  0.10145267844200134,\n",
       "  0.10305242240428925,\n",
       "  0.10820768028497696,\n",
       "  0.10550135374069214,\n",
       "  0.10245515406131744,\n",
       "  0.10186546295881271,\n",
       "  0.102781742811203,\n",
       "  0.10607711225748062,\n",
       "  0.10583160817623138,\n",
       "  0.1023482009768486,\n",
       "  0.10171426832675934,\n",
       "  0.10351777821779251,\n",
       "  0.10613341629505157,\n",
       "  0.10695794224739075,\n",
       "  0.10244055837392807,\n",
       "  0.09999092668294907,\n",
       "  0.10297036170959473,\n",
       "  0.10896068066358566,\n",
       "  0.10045318305492401,\n",
       "  0.09978374093770981,\n",
       "  0.10262129455804825,\n",
       "  0.10860151797533035,\n",
       "  0.10524199903011322,\n",
       "  0.10025744885206223,\n",
       "  0.1002025157213211,\n",
       "  0.11710735410451889,\n",
       "  0.11400134116411209,\n",
       "  0.10043440759181976,\n",
       "  0.10079014301300049,\n",
       "  0.10740289837121964,\n",
       "  0.1157665029168129,\n",
       "  0.09996673464775085,\n",
       "  0.10165180265903473,\n",
       "  0.09992196410894394,\n",
       "  0.10911811143159866,\n",
       "  0.10994014143943787,\n",
       "  0.09928221255540848,\n",
       "  0.09894342720508575,\n",
       "  0.10519842803478241,\n",
       "  0.10986815392971039,\n",
       "  0.09902173280715942,\n",
       "  0.09878642857074738,\n",
       "  0.10062111169099808,\n",
       "  0.11699122935533524,\n",
       "  0.10446947813034058,\n",
       "  0.09805118292570114,\n",
       "  0.09856468439102173,\n",
       "  0.10356586426496506,\n",
       "  0.10248422622680664,\n",
       "  0.10010004788637161,\n",
       "  0.10022265464067459,\n",
       "  0.09992150217294693,\n",
       "  0.0997779592871666,\n",
       "  0.09906582534313202,\n",
       "  0.09975434839725494,\n",
       "  0.1028032973408699,\n",
       "  0.10310836136341095,\n",
       "  0.1012316644191742,\n",
       "  0.10224023461341858,\n",
       "  0.10212689638137817,\n",
       "  0.09786387532949448,\n",
       "  0.09945152699947357,\n",
       "  0.10680505633354187,\n",
       "  0.10115011036396027,\n",
       "  0.0995483249425888,\n",
       "  0.09994151443243027,\n",
       "  0.10063633322715759,\n",
       "  0.1018269956111908,\n",
       "  0.09952536970376968,\n",
       "  0.09766293317079544,\n",
       "  0.09842365235090256,\n",
       "  0.10264898091554642,\n",
       "  0.098471999168396,\n",
       "  0.09669182449579239,\n",
       "  0.09957904368638992,\n",
       "  0.10702540725469589,\n",
       "  0.10311130434274673,\n",
       "  0.09678105264902115,\n",
       "  0.09834570437669754,\n",
       "  0.09756369888782501,\n",
       "  0.11992717534303665,\n",
       "  0.10289329290390015,\n",
       "  0.09867053478956223,\n",
       "  0.09841519594192505,\n",
       "  0.0996033176779747,\n",
       "  0.11549627780914307,\n",
       "  0.09911230206489563,\n",
       "  0.09585519134998322,\n",
       "  0.09758888930082321,\n",
       "  0.10232003778219223,\n",
       "  0.10140439122915268,\n",
       "  0.0994560495018959,\n",
       "  0.09937504678964615,\n",
       "  0.10016635805368423,\n",
       "  0.09647118300199509,\n",
       "  0.09531938284635544,\n",
       "  0.0957477018237114,\n",
       "  0.09677518904209137,\n",
       "  0.09854407608509064,\n",
       "  0.09986359626054764,\n",
       "  0.09908562153577805,\n",
       "  0.09576281905174255,\n",
       "  0.09524569660425186,\n",
       "  0.09662026911973953,\n",
       "  0.09953552484512329,\n",
       "  0.10031501203775406,\n",
       "  0.09661039710044861,\n",
       "  0.09504979848861694,\n",
       "  0.09597373753786087,\n",
       "  0.09763068705797195,\n",
       "  0.10048998892307281,\n",
       "  0.09954237937927246,\n",
       "  0.09612447023391724,\n",
       "  0.09534720331430435,\n",
       "  0.09684101492166519,\n",
       "  0.09598258882761002,\n",
       "  0.09733275324106216,\n",
       "  0.09951896965503693,\n",
       "  0.0956173986196518,\n",
       "  0.09403058141469955,\n",
       "  0.09521180391311646,\n",
       "  0.09594264626502991,\n",
       "  0.09474903345108032,\n",
       "  0.10047468543052673,\n",
       "  0.10433398187160492,\n",
       "  0.09327647089958191,\n",
       "  0.09472876042127609,\n",
       "  0.0941024124622345,\n",
       "  0.11199027299880981,\n",
       "  0.09510178864002228,\n",
       "  0.09497474133968353,\n",
       "  0.09335363656282425,\n",
       "  0.10996013879776001,\n",
       "  0.09703685343265533,\n",
       "  0.09482301771640778,\n",
       "  0.09357857704162598,\n",
       "  0.10151178389787674,\n",
       "  0.10351875424385071,\n",
       "  0.09255220741033554,\n",
       "  0.09250283986330032,\n",
       "  0.1035289615392685,\n",
       "  0.10284517705440521,\n",
       "  0.09433016180992126,\n",
       "  0.09406042098999023,\n",
       "  0.09375736117362976,\n",
       "  0.09460682421922684,\n",
       "  0.09724044799804688,\n",
       "  0.09831336885690689,\n",
       "  0.0968969315290451,\n",
       "  0.09412942081689835,\n",
       "  0.09499649703502655,\n",
       "  0.09825511276721954,\n",
       "  0.09736704081296921,\n",
       "  0.09745991975069046,\n",
       "  0.0966075137257576,\n",
       "  0.09378737211227417,\n",
       "  0.09443186968564987,\n",
       "  0.09638389945030212,\n",
       "  0.09567179530858994,\n",
       "  0.09410625696182251,\n",
       "  0.0970940813422203,\n",
       "  0.09877677261829376,\n",
       "  0.09680279344320297,\n",
       "  0.0965084657073021,\n",
       "  0.09434405714273453,\n",
       "  0.09377513080835342,\n",
       "  0.093748077750206,\n",
       "  0.09630432724952698,\n",
       "  0.096316397190094,\n",
       "  0.09268264472484589,\n",
       "  0.09255079925060272,\n",
       "  0.09327654540538788,\n",
       "  0.09575760364532471,\n",
       "  0.09752701967954636,\n",
       "  0.09423615038394928,\n",
       "  0.0922882929444313,\n",
       "  0.09269147366285324,\n",
       "  0.092985138297081,\n",
       "  0.0927228257060051,\n",
       "  0.09553194046020508,\n",
       "  0.09734004735946655,\n",
       "  0.09329106658697128,\n",
       "  0.09157624840736389,\n",
       "  0.09219281375408173,\n",
       "  0.09405086934566498,\n",
       "  0.09649353474378586,\n",
       "  0.09917281568050385,\n",
       "  0.09427407383918762,\n",
       "  0.09391504526138306,\n",
       "  0.09668631106615067,\n",
       "  0.09553149342536926,\n",
       "  0.09295906871557236,\n",
       "  0.0931520164012909,\n",
       "  0.10082170367240906,\n",
       "  0.09754548966884613,\n",
       "  0.09152403473854065,\n",
       "  0.09267833828926086,\n",
       "  0.09737865626811981,\n",
       "  0.09318380057811737,\n",
       "  0.09162815660238266,\n",
       "  0.0919356495141983,\n",
       "  0.09535062313079834,\n",
       "  0.09496250748634338,\n",
       "  0.09297128766775131,\n",
       "  0.0968070775270462,\n",
       "  0.09820075333118439,\n",
       "  0.09172987937927246,\n",
       "  0.09213238209486008,\n",
       "  0.09228482097387314,\n",
       "  0.09937592595815659,\n",
       "  0.10001450031995773,\n",
       "  0.09253711998462677,\n",
       "  0.09326612949371338,\n",
       "  0.09489491581916809,\n",
       "  0.10401035100221634,\n",
       "  0.09220077842473984,\n",
       "  0.09504794329404831,\n",
       "  0.09294699877500534,\n",
       "  0.09776611626148224,\n",
       "  0.09496444463729858,\n",
       "  0.0911049097776413,\n",
       "  0.09100614488124847,\n",
       "  0.09678251296281815,\n",
       "  0.09956368058919907,\n",
       "  0.09189748764038086,\n",
       "  0.09171996265649796,\n",
       "  0.10119976103305817,\n",
       "  0.09692602604627609,\n",
       "  0.0912122055888176,\n",
       "  0.09206439554691315,\n",
       "  0.0919569879770279,\n",
       "  0.1013827696442604,\n",
       "  0.09081771969795227,\n",
       "  0.09406674653291702,\n",
       "  0.09218674898147583,\n",
       "  0.09550457447767258,\n",
       "  0.09539169818162918,\n",
       "  0.0911337360739708,\n",
       "  0.09094826877117157,\n",
       "  0.09116671979427338,\n",
       "  0.0958901196718216,\n",
       "  0.09871640056371689,\n",
       "  0.09192977100610733,\n",
       "  0.09163075685501099,\n",
       "  0.09387611597776413,\n",
       "  0.09389494359493256,\n",
       "  0.09111549705266953,\n",
       "  0.09108822047710419,\n",
       "  0.09639572352170944,\n",
       "  0.09322512149810791,\n",
       "  0.09078055620193481,\n",
       "  0.09097178280353546,\n",
       "  0.09110187739133835,\n",
       "  0.0934523269534111,\n",
       "  0.0948488712310791,\n",
       "  0.09522580355405807,\n",
       "  0.09353845566511154,\n",
       "  0.09141278266906738,\n",
       "  0.09125878661870956,\n",
       "  0.09354005753993988,\n",
       "  0.09329445660114288,\n",
       "  0.09155751764774323,\n",
       "  0.09192771464586258,\n",
       "  0.09630154073238373,\n",
       "  0.09436308592557907,\n",
       "  0.09200681000947952,\n",
       "  0.09194976836442947,\n",
       "  0.09541960060596466,\n",
       "  0.09766317903995514,\n",
       "  0.09288346022367477,\n",
       "  0.09195602685213089,\n",
       "  0.09415896981954575,\n",
       "  0.09519945830106735,\n",
       "  0.09185291081666946,\n",
       "  0.09132374823093414,\n",
       "  0.09531722217798233,\n",
       "  0.0942799523472786,\n",
       "  0.09159639477729797,\n",
       "  0.09099017083644867,\n",
       "  0.09864812344312668,\n",
       "  0.09849907457828522,\n",
       "  0.09206277132034302,\n",
       "  0.09285852313041687,\n",
       "  0.09157329797744751,\n",
       "  0.09961291402578354,\n",
       "  0.09128960222005844,\n",
       "  0.09125496447086334,\n",
       "  0.09552354365587234,\n",
       "  0.09761057794094086,\n",
       "  0.09500783681869507,\n",
       "  0.09273605048656464,\n",
       "  0.09181471168994904,\n",
       "  0.09446730464696884,\n",
       "  0.09865113347768784,\n",
       "  0.09140238910913467,\n",
       "  0.09067822992801666,\n",
       "  0.09565747529268265,\n",
       "  0.09381743520498276,\n",
       "  0.0894000232219696,\n",
       "  0.0898270234465599,\n",
       "  0.09917537868022919,\n",
       "  0.09070692956447601,\n",
       "  0.09207668155431747,\n",
       "  0.09127602726221085,\n",
       "  0.10429800301790237,\n",
       "  0.09519249200820923,\n",
       "  0.09337350726127625,\n",
       "  0.09254811704158783,\n",
       "  0.10504848510026932,\n",
       "  0.10343774408102036,\n",
       "  0.09098637849092484,\n",
       "  0.09090118110179901,\n",
       "  0.09270080178976059,\n",
       "  0.09294872730970383,\n",
       "  0.09078451246023178,\n",
       "  0.0899263471364975,\n",
       "  0.09024661779403687,\n",
       "  0.09062509983778,\n",
       "  0.09322649240493774,\n",
       "  0.09809137135744095,\n",
       "  0.09049830585718155,\n",
       "  0.09234187006950378,\n",
       "  0.09254803508520126,\n",
       "  0.11025676131248474,\n",
       "  0.09123490750789642,\n",
       "  0.09601116180419922,\n",
       "  0.09151924401521683,\n",
       "  0.10835129767656326,\n",
       "  0.09695415943861008,\n",
       "  0.09243617206811905,\n",
       "  0.09219430387020111,\n",
       "  0.09223999828100204,\n",
       "  0.09785380959510803,\n",
       "  0.09351707249879837,\n",
       "  0.09223092347383499,\n",
       "  0.0919932872056961,\n",
       "  0.09182411432266235,\n",
       "  0.09179139137268066,\n",
       "  0.09207862615585327,\n",
       "  0.09234699606895447,\n",
       "  0.09774241596460342,\n",
       "  0.09636618942022324,\n",
       "  0.09305790811777115,\n",
       "  0.09390337020158768,\n",
       "  0.09234928339719772,\n",
       "  0.09703703969717026,\n",
       "  0.09427965432405472,\n",
       "  0.09133220463991165,\n",
       "  0.09205788373947144,\n",
       "  0.0927962064743042,\n",
       "  0.09178172796964645,\n",
       "  0.09208279103040695,\n",
       "  0.09393132477998734,\n",
       "  0.09787824004888535,\n",
       "  0.09423641860485077,\n",
       "  0.09278874844312668,\n",
       "  0.09653426706790924,\n",
       "  0.09996447712182999,\n",
       "  0.09212362766265869,\n",
       "  0.09230005741119385,\n",
       "  0.09109663218259811,\n",
       "  0.09182659536600113,\n",
       "  0.09008205682039261,\n",
       "  0.0900542214512825,\n",
       "  0.09157314896583557,\n",
       "  0.0921822115778923,\n",
       "  0.09332741796970367,\n",
       "  0.09093562513589859,\n",
       "  0.09112507849931717,\n",
       "  0.09626905620098114,\n",
       "  0.09962668269872665,\n",
       "  0.0924687385559082,\n",
       "  0.0928717702627182,\n",
       "  0.09376867115497589,\n",
       "  0.0967583954334259,\n",
       "  0.09247096627950668,\n",
       "  0.09245238453149796,\n",
       "  0.09639198333024979,\n",
       "  0.09445510059595108,\n",
       "  0.09269385784864426,\n",
       "  0.0931333377957344,\n",
       "  0.09806247800588608,\n",
       "  0.09819160401821136,\n",
       "  0.09455714374780655,\n",
       "  0.09422533214092255,\n",
       "  0.09505592286586761,\n",
       "  0.09309983253479004,\n",
       "  0.09333259612321854,\n",
       "  0.09418633580207825,\n",
       "  0.09491008520126343,\n",
       "  0.09348136931657791,\n",
       "  0.09241312742233276,\n",
       "  0.09226348996162415,\n",
       "  0.09283759444952011,\n",
       "  0.09291402995586395,\n",
       "  0.09282214939594269,\n",
       "  0.09466903656721115,\n",
       "  0.0960952565073967,\n",
       "  0.09355801343917847,\n",
       "  0.09594950079917908,\n",
       "  0.1086932122707367,\n",
       "  0.09588729590177536,\n",
       "  0.09370395541191101,\n",
       "  0.09385179728269577,\n",
       "  0.09677860885858536,\n",
       "  0.09555287659168243,\n",
       "  0.09440561383962631,\n",
       "  0.09514205902814865,\n",
       "  0.09658355265855789,\n",
       "  0.09629875421524048,\n",
       "  0.09516525268554688,\n",
       "  0.09616707265377045,\n",
       "  0.09611613303422928,\n",
       "  0.0924374982714653,\n",
       "  0.09214454889297485,\n",
       "  0.09550480544567108,\n",
       "  0.09203184396028519,\n",
       "  0.09162158519029617,\n",
       "  0.09564967453479767,\n",
       "  0.09540915489196777,\n",
       "  0.09234930574893951,\n",
       "  0.09252158552408218,\n",
       "  0.09349203109741211,\n",
       "  0.09749875962734222,\n",
       "  0.09634990245103836,\n",
       "  0.09203195571899414,\n",
       "  0.0914759486913681,\n",
       "  0.09116220474243164,\n",
       "  0.09628109633922577,\n",
       "  0.09353789687156677,\n",
       "  0.09244805574417114,\n",
       "  0.09239649027585983,\n",
       "  0.09980282187461853,\n",
       "  0.09476352483034134,\n",
       "  0.09307385236024857,\n",
       "  0.0959843173623085,\n",
       "  0.09695366770029068,\n",
       "  0.0943722203373909,\n",
       "  0.09432606399059296,\n",
       "  0.0977972149848938,\n",
       "  0.09947294741868973,\n",
       "  ...],\n",
       " 'val_accuracy': [0.7546584010124207,\n",
       "  0.7546584010124207,\n",
       "  0.7546584010124207,\n",
       "  0.6428571343421936,\n",
       "  0.44720497727394104,\n",
       "  0.7236024737358093,\n",
       "  0.7515528202056885,\n",
       "  0.7577639818191528,\n",
       "  0.7577639818191528,\n",
       "  0.7577639818191528,\n",
       "  0.760869562625885,\n",
       "  0.760869562625885,\n",
       "  0.7763975262641907,\n",
       "  0.7701863646507263,\n",
       "  0.7857142686843872,\n",
       "  0.8105590343475342,\n",
       "  0.8229813575744629,\n",
       "  0.8478260636329651,\n",
       "  0.8540372848510742,\n",
       "  0.8602484464645386,\n",
       "  0.8633540272712708,\n",
       "  0.8788819909095764,\n",
       "  0.8850931525230408,\n",
       "  0.888198733329773,\n",
       "  0.8944099545478821,\n",
       "  0.8944099545478821,\n",
       "  0.8944099545478821,\n",
       "  0.8913043737411499,\n",
       "  0.8913043737411499,\n",
       "  0.8975155353546143,\n",
       "  0.8975155353546143,\n",
       "  0.9037266969680786,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9068322777748108,\n",
       "  0.9037266969680786,\n",
       "  0.9068322777748108,\n",
       "  0.9068322777748108,\n",
       "  0.9037266969680786,\n",
       "  0.9068322777748108,\n",
       "  0.9037266969680786,\n",
       "  0.9037266969680786,\n",
       "  0.9037266969680786,\n",
       "  0.9037266969680786,\n",
       "  0.9037266969680786,\n",
       "  0.9068322777748108,\n",
       "  0.9068322777748108,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9068322777748108,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9068322777748108,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.909937858581543,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9130434989929199,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9130434989929199,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9192546606063843,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9161490797996521,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9192546606063843,\n",
       "  0.9285714030265808,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9223602414131165,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9254658222198486,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9254658222198486,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9223602414131165,\n",
       "  0.9254658222198486,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9409937858581543,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9378882050514221,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9596273303031921,\n",
       "  0.9472049474716187,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9503105878829956,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9534161686897278,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9534161686897278,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9534161686897278,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9627329111099243,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  ...]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99d729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68f4973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3696472644805908,\n",
       " 0.9767626523971558,\n",
       " 0.6555690169334412,\n",
       " 0.7204952239990234,\n",
       " 0.8498615026473999,\n",
       " 0.5612354874610901,\n",
       " 0.49682360887527466,\n",
       " 0.5095617175102234,\n",
       " 0.5103656649589539,\n",
       " 0.4879467785358429,\n",
       " 0.44969385862350464,\n",
       " 0.4129253029823303,\n",
       " 0.40077152848243713,\n",
       " 0.41555237770080566,\n",
       " 0.4201304316520691,\n",
       " 0.395426481962204,\n",
       " 0.3659825623035431,\n",
       " 0.34737318754196167,\n",
       " 0.33685651421546936,\n",
       " 0.32892346382141113,\n",
       " 0.3215826153755188,\n",
       " 0.31745269894599915,\n",
       " 0.3187258243560791,\n",
       " 0.3209141492843628,\n",
       " 0.315132737159729,\n",
       " 0.3033420145511627,\n",
       " 0.29165586829185486,\n",
       " 0.2835754156112671,\n",
       " 0.2780337929725647,\n",
       " 0.27360987663269043,\n",
       " 0.2703114449977875,\n",
       " 0.2690448760986328,\n",
       " 0.2696975767612457,\n",
       " 0.2690184712409973,\n",
       " 0.26551756262779236,\n",
       " 0.2614074945449829,\n",
       " 0.2553754150867462,\n",
       " 0.2506581246852875,\n",
       " 0.24829357862472534,\n",
       " 0.24593473970890045,\n",
       " 0.2439837008714676,\n",
       " 0.24229110777378082,\n",
       " 0.24184595048427582,\n",
       " 0.24107865989208221,\n",
       " 0.23952805995941162,\n",
       " 0.23721233010292053,\n",
       " 0.2358115315437317,\n",
       " 0.23332150280475616,\n",
       " 0.22928057610988617,\n",
       " 0.22568796575069427,\n",
       " 0.22319142520427704,\n",
       " 0.22254984080791473,\n",
       " 0.22376450896263123,\n",
       " 0.22485283017158508,\n",
       " 0.22445963323116302,\n",
       " 0.22244930267333984,\n",
       " 0.22077621519565582,\n",
       " 0.22042948007583618,\n",
       " 0.2214084416627884,\n",
       " 0.22122040390968323,\n",
       " 0.21934553980827332,\n",
       " 0.21754491329193115,\n",
       " 0.21621856093406677,\n",
       " 0.21527594327926636,\n",
       " 0.21453826129436493,\n",
       " 0.21448810398578644,\n",
       " 0.21529749035835266,\n",
       " 0.21790802478790283,\n",
       " 0.2209775745868683,\n",
       " 0.222259521484375,\n",
       " 0.22109675407409668,\n",
       " 0.21913154423236847,\n",
       " 0.2169015109539032,\n",
       " 0.21516819298267365,\n",
       " 0.2143751084804535,\n",
       " 0.21464505791664124,\n",
       " 0.2145758718252182,\n",
       " 0.21547019481658936,\n",
       " 0.2157813310623169,\n",
       " 0.2166181057691574,\n",
       " 0.21759988367557526,\n",
       " 0.21887914836406708,\n",
       " 0.21568873524665833,\n",
       " 0.21177014708518982,\n",
       " 0.209358811378479,\n",
       " 0.20822463929653168,\n",
       " 0.2077326774597168,\n",
       " 0.2079700380563736,\n",
       " 0.2090081423521042,\n",
       " 0.2121487557888031,\n",
       " 0.2167552262544632,\n",
       " 0.21869049966335297,\n",
       " 0.21616671979427338,\n",
       " 0.21072973310947418,\n",
       " 0.20757515728473663,\n",
       " 0.20742805302143097,\n",
       " 0.20815123617649078,\n",
       " 0.20972482860088348,\n",
       " 0.21060813963413239,\n",
       " 0.21170634031295776,\n",
       " 0.21239006519317627,\n",
       " 0.20954529941082,\n",
       " 0.20679914951324463,\n",
       " 0.2057310789823532,\n",
       " 0.2068394422531128,\n",
       " 0.20587503910064697,\n",
       " 0.20400787889957428,\n",
       " 0.20333132147789001,\n",
       " 0.2044645994901657,\n",
       " 0.20586463809013367,\n",
       " 0.20643478631973267,\n",
       " 0.2075384110212326,\n",
       " 0.2087554633617401,\n",
       " 0.20660175383090973,\n",
       " 0.20309609174728394,\n",
       " 0.20064376294612885,\n",
       " 0.1996869146823883,\n",
       " 0.19996020197868347,\n",
       " 0.20026938617229462,\n",
       " 0.2001899629831314,\n",
       " 0.19982600212097168,\n",
       " 0.19973643124103546,\n",
       " 0.20083747804164886,\n",
       " 0.20176026225090027,\n",
       " 0.20246373116970062,\n",
       " 0.2019677311182022,\n",
       " 0.20048627257347107,\n",
       " 0.19941656291484833,\n",
       " 0.20046871900558472,\n",
       " 0.20091865956783295,\n",
       " 0.1987878531217575,\n",
       " 0.19665342569351196,\n",
       " 0.19642071425914764,\n",
       " 0.19710484147071838,\n",
       " 0.19751714169979095,\n",
       " 0.1966899186372757,\n",
       " 0.19495224952697754,\n",
       " 0.1940128356218338,\n",
       " 0.19391050934791565,\n",
       " 0.19459928572177887,\n",
       " 0.19844116270542145,\n",
       " 0.20185627043247223,\n",
       " 0.20148584246635437,\n",
       " 0.19670674204826355,\n",
       " 0.19374243915081024,\n",
       " 0.19230614602565765,\n",
       " 0.19188715517520905,\n",
       " 0.1934303343296051,\n",
       " 0.19716747105121613,\n",
       " 0.1989694982767105,\n",
       " 0.19912189245224,\n",
       " 0.19913382828235626,\n",
       " 0.19771330058574677,\n",
       " 0.19829590618610382,\n",
       " 0.1977873593568802,\n",
       " 0.19185170531272888,\n",
       " 0.18833018839359283,\n",
       " 0.18801136314868927,\n",
       " 0.1894409954547882,\n",
       " 0.1938997358083725,\n",
       " 0.1973186880350113,\n",
       " 0.19553405046463013,\n",
       " 0.19208437204360962,\n",
       " 0.19007273018360138,\n",
       " 0.18917569518089294,\n",
       " 0.1899200975894928,\n",
       " 0.19360868632793427,\n",
       " 0.19658109545707703,\n",
       " 0.1922936588525772,\n",
       " 0.18959932029247284,\n",
       " 0.1899515986442566,\n",
       " 0.19413286447525024,\n",
       " 0.19729356467723846,\n",
       " 0.1917826384305954,\n",
       " 0.18582046031951904,\n",
       " 0.18304656445980072,\n",
       " 0.18299329280853271,\n",
       " 0.18562428653240204,\n",
       " 0.19030970335006714,\n",
       " 0.19387732446193695,\n",
       " 0.18879349529743195,\n",
       " 0.18341955542564392,\n",
       " 0.1811390221118927,\n",
       " 0.1815664917230606,\n",
       " 0.18596680462360382,\n",
       " 0.19039855897426605,\n",
       " 0.1867479681968689,\n",
       " 0.18059039115905762,\n",
       " 0.17913156747817993,\n",
       " 0.17970900237560272,\n",
       " 0.1850879192352295,\n",
       " 0.193657785654068,\n",
       " 0.1894856095314026,\n",
       " 0.18179872632026672,\n",
       " 0.17777809500694275,\n",
       " 0.17685651779174805,\n",
       " 0.1810479313135147,\n",
       " 0.18730488419532776,\n",
       " 0.18486420810222626,\n",
       " 0.1779618114233017,\n",
       " 0.17570245265960693,\n",
       " 0.17478349804878235,\n",
       " 0.17608413100242615,\n",
       " 0.1801508665084839,\n",
       " 0.17930999398231506,\n",
       " 0.17647849023342133,\n",
       " 0.1730833351612091,\n",
       " 0.17193399369716644,\n",
       " 0.17304229736328125,\n",
       " 0.17509910464286804,\n",
       " 0.1744382530450821,\n",
       " 0.1726016104221344,\n",
       " 0.17128364741802216,\n",
       " 0.17257235944271088,\n",
       " 0.1793891042470932,\n",
       " 0.18833182752132416,\n",
       " 0.18163809180259705,\n",
       " 0.17035456001758575,\n",
       " 0.166893869638443,\n",
       " 0.16606831550598145,\n",
       " 0.1663394719362259,\n",
       " 0.17191757261753082,\n",
       " 0.18164145946502686,\n",
       " 0.17609891295433044,\n",
       " 0.16633987426757812,\n",
       " 0.16494566202163696,\n",
       " 0.16577577590942383,\n",
       " 0.17011627554893494,\n",
       " 0.17695443332195282,\n",
       " 0.177937850356102,\n",
       " 0.1685543805360794,\n",
       " 0.1629071980714798,\n",
       " 0.16202738881111145,\n",
       " 0.1647358536720276,\n",
       " 0.17315445840358734,\n",
       " 0.1717405915260315,\n",
       " 0.1651880145072937,\n",
       " 0.16271963715553284,\n",
       " 0.16373787820339203,\n",
       " 0.16463184356689453,\n",
       " 0.16227997839450836,\n",
       " 0.16020944714546204,\n",
       " 0.16005843877792358,\n",
       " 0.16223837435245514,\n",
       " 0.1630716770887375,\n",
       " 0.1655566394329071,\n",
       " 0.16480527818202972,\n",
       " 0.15802785754203796,\n",
       " 0.1563500165939331,\n",
       " 0.15750792622566223,\n",
       " 0.1578657329082489,\n",
       " 0.15686354041099548,\n",
       " 0.1554965078830719,\n",
       " 0.1551201492547989,\n",
       " 0.1559108942747116,\n",
       " 0.15723417699337006,\n",
       " 0.15844175219535828,\n",
       " 0.15844124555587769,\n",
       " 0.16049492359161377,\n",
       " 0.15855583548545837,\n",
       " 0.1531948745250702,\n",
       " 0.15159395337104797,\n",
       " 0.15259408950805664,\n",
       " 0.15533481538295746,\n",
       " 0.15589015185832977,\n",
       " 0.1527755707502365,\n",
       " 0.15099617838859558,\n",
       " 0.15280850231647491,\n",
       " 0.1586705595254898,\n",
       " 0.1583261489868164,\n",
       " 0.1533329337835312,\n",
       " 0.15145941078662872,\n",
       " 0.15161912143230438,\n",
       " 0.15239521861076355,\n",
       " 0.151132732629776,\n",
       " 0.14924347400665283,\n",
       " 0.14905868470668793,\n",
       " 0.1530168354511261,\n",
       " 0.1546044796705246,\n",
       " 0.14586509764194489,\n",
       " 0.144334077835083,\n",
       " 0.14981645345687866,\n",
       " 0.1574389785528183,\n",
       " 0.14749367535114288,\n",
       " 0.14402830600738525,\n",
       " 0.14749453961849213,\n",
       " 0.15282326936721802,\n",
       " 0.14830315113067627,\n",
       " 0.14317575097084045,\n",
       " 0.14430449903011322,\n",
       " 0.15056800842285156,\n",
       " 0.15013165771961212,\n",
       " 0.14298494160175323,\n",
       " 0.1425515115261078,\n",
       " 0.14502863585948944,\n",
       " 0.1474621295928955,\n",
       " 0.1450881063938141,\n",
       " 0.13999710977077484,\n",
       " 0.13943380117416382,\n",
       " 0.14357882738113403,\n",
       " 0.14771738648414612,\n",
       " 0.14051757752895355,\n",
       " 0.13908174633979797,\n",
       " 0.14264298975467682,\n",
       " 0.1445617824792862,\n",
       " 0.14103631675243378,\n",
       " 0.14154955744743347,\n",
       " 0.14681097865104675,\n",
       " 0.1439993679523468,\n",
       " 0.13673634827136993,\n",
       " 0.13639689981937408,\n",
       " 0.14361713826656342,\n",
       " 0.1532130092382431,\n",
       " 0.13819684088230133,\n",
       " 0.13523679971694946,\n",
       " 0.13733182847499847,\n",
       " 0.14452698826789856,\n",
       " 0.14820680022239685,\n",
       " 0.13938571512699127,\n",
       " 0.13501188158988953,\n",
       " 0.13508203625679016,\n",
       " 0.13797520101070404,\n",
       " 0.14210368692874908,\n",
       " 0.13738834857940674,\n",
       " 0.13326102495193481,\n",
       " 0.13301414251327515,\n",
       " 0.1331569105386734,\n",
       " 0.13761258125305176,\n",
       " 0.1398896425962448,\n",
       " 0.13200537860393524,\n",
       " 0.13112501800060272,\n",
       " 0.13722389936447144,\n",
       " 0.14291028678417206,\n",
       " 0.13521988689899445,\n",
       " 0.13154380023479462,\n",
       " 0.13139179348945618,\n",
       " 0.13676859438419342,\n",
       " 0.1428864747285843,\n",
       " 0.13449066877365112,\n",
       " 0.13046151399612427,\n",
       " 0.13165685534477234,\n",
       " 0.13720278441905975,\n",
       " 0.1338382214307785,\n",
       " 0.12873853743076324,\n",
       " 0.1283348649740219,\n",
       " 0.1306208372116089,\n",
       " 0.14073750376701355,\n",
       " 0.1370249092578888,\n",
       " 0.12640729546546936,\n",
       " 0.12617993354797363,\n",
       " 0.13848967850208282,\n",
       " 0.1461113542318344,\n",
       " 0.12689514458179474,\n",
       " 0.12603098154067993,\n",
       " 0.12695610523223877,\n",
       " 0.13607223331928253,\n",
       " 0.12865687906742096,\n",
       " 0.12487930059432983,\n",
       " 0.12449214607477188,\n",
       " 0.1282719522714615,\n",
       " 0.13457202911376953,\n",
       " 0.12849782407283783,\n",
       " 0.12473051995038986,\n",
       " 0.12925885617733002,\n",
       " 0.1424064040184021,\n",
       " 0.1300520896911621,\n",
       " 0.12319844216108322,\n",
       " 0.12355498969554901,\n",
       " 0.12981408834457397,\n",
       " 0.13610121607780457,\n",
       " 0.12573933601379395,\n",
       " 0.12500807642936707,\n",
       " 0.13140346109867096,\n",
       " 0.12784378230571747,\n",
       " 0.1233598068356514,\n",
       " 0.12257381528615952,\n",
       " 0.12451159209012985,\n",
       " 0.12760823965072632,\n",
       " 0.1253298819065094,\n",
       " 0.12332230061292648,\n",
       " 0.12435220927000046,\n",
       " 0.12664362788200378,\n",
       " 0.12243813276290894,\n",
       " 0.12115882337093353,\n",
       " 0.1277163326740265,\n",
       " 0.13662202656269073,\n",
       " 0.12745119631290436,\n",
       " 0.12144377827644348,\n",
       " 0.12108025699853897,\n",
       " 0.12324202060699463,\n",
       " 0.12416422367095947,\n",
       " 0.12173472344875336,\n",
       " 0.12238465994596481,\n",
       " 0.12405069917440414,\n",
       " 0.12265322357416153,\n",
       " 0.12319757789373398,\n",
       " 0.12662114202976227,\n",
       " 0.12416099011898041,\n",
       " 0.1193876788020134,\n",
       " 0.11972011625766754,\n",
       " 0.12780213356018066,\n",
       " 0.12436415255069733,\n",
       " 0.11859825253486633,\n",
       " 0.11819833517074585,\n",
       " 0.11941711604595184,\n",
       " 0.12223748117685318,\n",
       " 0.1193934977054596,\n",
       " 0.12014123797416687,\n",
       " 0.12880206108093262,\n",
       " 0.13019412755966187,\n",
       " 0.11758904904127121,\n",
       " 0.11704680323600769,\n",
       " 0.1198994442820549,\n",
       " 0.13194496929645538,\n",
       " 0.12236784398555756,\n",
       " 0.1172327771782875,\n",
       " 0.11973210424184799,\n",
       " 0.1236308217048645,\n",
       " 0.12122798711061478,\n",
       " 0.11866672337055206,\n",
       " 0.11698110401630402,\n",
       " 0.11644596606492996,\n",
       " 0.11840145289897919,\n",
       " 0.12081742286682129,\n",
       " 0.11858097463846207,\n",
       " 0.11785022914409637,\n",
       " 0.118394635617733,\n",
       " 0.12312101572751999,\n",
       " 0.12233539670705795,\n",
       " 0.11719958484172821,\n",
       " 0.11708574742078781,\n",
       " 0.11669053137302399,\n",
       " 0.1179332360625267,\n",
       " 0.11983676999807358,\n",
       " 0.11534634232521057,\n",
       " 0.11350338906049728,\n",
       " 0.1136864572763443,\n",
       " 0.1166226863861084,\n",
       " 0.11716815084218979,\n",
       " 0.11389893293380737,\n",
       " 0.11498138308525085,\n",
       " 0.11753072589635849,\n",
       " 0.11747829616069794,\n",
       " 0.11514595150947571,\n",
       " 0.11279185861349106,\n",
       " 0.11271610856056213,\n",
       " 0.1154220774769783,\n",
       " 0.12004850059747696,\n",
       " 0.11636015772819519,\n",
       " 0.1132064163684845,\n",
       " 0.11325360089540482,\n",
       " 0.1158425435423851,\n",
       " 0.11883962154388428,\n",
       " 0.1151965856552124,\n",
       " 0.11252995580434799,\n",
       " 0.11269143223762512,\n",
       " 0.11300692707300186,\n",
       " 0.11108618974685669,\n",
       " 0.11173657327890396,\n",
       " 0.11908017098903656,\n",
       " 0.11343685537576675,\n",
       " 0.11063537746667862,\n",
       " 0.11104800552129745,\n",
       " 0.11302918195724487,\n",
       " 0.11450795084238052,\n",
       " 0.1138690710067749,\n",
       " 0.11301372200250626,\n",
       " 0.11147037148475647,\n",
       " 0.11221638321876526,\n",
       " 0.11579401046037674,\n",
       " 0.11379974335432053,\n",
       " 0.11101704835891724,\n",
       " 0.11047090590000153,\n",
       " 0.11212190240621567,\n",
       " 0.11411715298891068,\n",
       " 0.11440182477235794,\n",
       " 0.11003437638282776,\n",
       " 0.10885985940694809,\n",
       " 0.10907413810491562,\n",
       " 0.11472302675247192,\n",
       " 0.11474081128835678,\n",
       " 0.10848499089479446,\n",
       " 0.10811229795217514,\n",
       " 0.11212161928415298,\n",
       " 0.11113757640123367,\n",
       " 0.10800289362668991,\n",
       " 0.10922323167324066,\n",
       " 0.11140163242816925,\n",
       " 0.1080087348818779,\n",
       " 0.10798374563455582,\n",
       " 0.10861918330192566,\n",
       " 0.11580415070056915,\n",
       " 0.10942274332046509,\n",
       " 0.10657227784395218,\n",
       " 0.10642354935407639,\n",
       " 0.11046838760375977,\n",
       " 0.11237316578626633,\n",
       " 0.10784607380628586,\n",
       " 0.107381172478199,\n",
       " 0.10643026977777481,\n",
       " 0.10690604150295258,\n",
       " 0.11258438974618912,\n",
       " 0.11562994122505188,\n",
       " 0.11031732708215714,\n",
       " 0.11025357991456985,\n",
       " 0.11183594167232513,\n",
       " 0.10966036468744278,\n",
       " 0.1069880872964859,\n",
       " 0.1071377769112587,\n",
       " 0.10655577480792999,\n",
       " 0.10707203298807144,\n",
       " 0.10813749581575394,\n",
       " 0.1095249354839325,\n",
       " 0.11175204068422318,\n",
       " 0.10822699218988419,\n",
       " 0.10656043887138367,\n",
       " 0.10881708562374115,\n",
       " 0.11197880655527115,\n",
       " 0.10751166194677353,\n",
       " 0.10569028556346893,\n",
       " 0.10761716961860657,\n",
       " 0.110967256128788,\n",
       " 0.10718782991170883,\n",
       " 0.10504033416509628,\n",
       " 0.10553755611181259,\n",
       " 0.10843833535909653,\n",
       " 0.10689552873373032,\n",
       " 0.10557835549116135,\n",
       " 0.10730797797441483,\n",
       " 0.10633324086666107,\n",
       " 0.10395684093236923,\n",
       " 0.10449278354644775,\n",
       " 0.10598084330558777,\n",
       " 0.10555154830217361,\n",
       " 0.1041046753525734,\n",
       " 0.10603868961334229,\n",
       " 0.10538168996572495,\n",
       " 0.10343814641237259,\n",
       " 0.1037364974617958,\n",
       " 0.10672444850206375,\n",
       " 0.11235694587230682,\n",
       " 0.10359147936105728,\n",
       " 0.10309965908527374,\n",
       " 0.10405813157558441,\n",
       " 0.111945740878582,\n",
       " 0.10563109070062637,\n",
       " 0.10257120430469513,\n",
       " 0.10240663588047028,\n",
       " 0.11072957515716553,\n",
       " 0.11178188771009445,\n",
       " 0.10211832076311111,\n",
       " 0.10257548838853836,\n",
       " 0.10533972829580307,\n",
       " 0.10928033292293549,\n",
       " 0.10297785699367523,\n",
       " 0.10172877460718155,\n",
       " 0.1046106219291687,\n",
       " 0.10890303552150726,\n",
       " 0.1028415635228157,\n",
       " 0.10145267844200134,\n",
       " 0.10305242240428925,\n",
       " 0.10820768028497696,\n",
       " 0.10550135374069214,\n",
       " 0.10245515406131744,\n",
       " 0.10186546295881271,\n",
       " 0.102781742811203,\n",
       " 0.10607711225748062,\n",
       " 0.10583160817623138,\n",
       " 0.1023482009768486,\n",
       " 0.10171426832675934,\n",
       " 0.10351777821779251,\n",
       " 0.10613341629505157,\n",
       " 0.10695794224739075,\n",
       " 0.10244055837392807,\n",
       " 0.09999092668294907,\n",
       " 0.10297036170959473,\n",
       " 0.10896068066358566,\n",
       " 0.10045318305492401,\n",
       " 0.09978374093770981,\n",
       " 0.10262129455804825,\n",
       " 0.10860151797533035,\n",
       " 0.10524199903011322,\n",
       " 0.10025744885206223,\n",
       " 0.1002025157213211,\n",
       " 0.11710735410451889,\n",
       " 0.11400134116411209,\n",
       " 0.10043440759181976,\n",
       " 0.10079014301300049,\n",
       " 0.10740289837121964,\n",
       " 0.1157665029168129,\n",
       " 0.09996673464775085,\n",
       " 0.10165180265903473,\n",
       " 0.09992196410894394,\n",
       " 0.10911811143159866,\n",
       " 0.10994014143943787,\n",
       " 0.09928221255540848,\n",
       " 0.09894342720508575,\n",
       " 0.10519842803478241,\n",
       " 0.10986815392971039,\n",
       " 0.09902173280715942,\n",
       " 0.09878642857074738,\n",
       " 0.10062111169099808,\n",
       " 0.11699122935533524,\n",
       " 0.10446947813034058,\n",
       " 0.09805118292570114,\n",
       " 0.09856468439102173,\n",
       " 0.10356586426496506,\n",
       " 0.10248422622680664,\n",
       " 0.10010004788637161,\n",
       " 0.10022265464067459,\n",
       " 0.09992150217294693,\n",
       " 0.0997779592871666,\n",
       " 0.09906582534313202,\n",
       " 0.09975434839725494,\n",
       " 0.1028032973408699,\n",
       " 0.10310836136341095,\n",
       " 0.1012316644191742,\n",
       " 0.10224023461341858,\n",
       " 0.10212689638137817,\n",
       " 0.09786387532949448,\n",
       " 0.09945152699947357,\n",
       " 0.10680505633354187,\n",
       " 0.10115011036396027,\n",
       " 0.0995483249425888,\n",
       " 0.09994151443243027,\n",
       " 0.10063633322715759,\n",
       " 0.1018269956111908,\n",
       " 0.09952536970376968,\n",
       " 0.09766293317079544,\n",
       " 0.09842365235090256,\n",
       " 0.10264898091554642,\n",
       " 0.098471999168396,\n",
       " 0.09669182449579239,\n",
       " 0.09957904368638992,\n",
       " 0.10702540725469589,\n",
       " 0.10311130434274673,\n",
       " 0.09678105264902115,\n",
       " 0.09834570437669754,\n",
       " 0.09756369888782501,\n",
       " 0.11992717534303665,\n",
       " 0.10289329290390015,\n",
       " 0.09867053478956223,\n",
       " 0.09841519594192505,\n",
       " 0.0996033176779747,\n",
       " 0.11549627780914307,\n",
       " 0.09911230206489563,\n",
       " 0.09585519134998322,\n",
       " 0.09758888930082321,\n",
       " 0.10232003778219223,\n",
       " 0.10140439122915268,\n",
       " 0.0994560495018959,\n",
       " 0.09937504678964615,\n",
       " 0.10016635805368423,\n",
       " 0.09647118300199509,\n",
       " 0.09531938284635544,\n",
       " 0.0957477018237114,\n",
       " 0.09677518904209137,\n",
       " 0.09854407608509064,\n",
       " 0.09986359626054764,\n",
       " 0.09908562153577805,\n",
       " 0.09576281905174255,\n",
       " 0.09524569660425186,\n",
       " 0.09662026911973953,\n",
       " 0.09953552484512329,\n",
       " 0.10031501203775406,\n",
       " 0.09661039710044861,\n",
       " 0.09504979848861694,\n",
       " 0.09597373753786087,\n",
       " 0.09763068705797195,\n",
       " 0.10048998892307281,\n",
       " 0.09954237937927246,\n",
       " 0.09612447023391724,\n",
       " 0.09534720331430435,\n",
       " 0.09684101492166519,\n",
       " 0.09598258882761002,\n",
       " 0.09733275324106216,\n",
       " 0.09951896965503693,\n",
       " 0.0956173986196518,\n",
       " 0.09403058141469955,\n",
       " 0.09521180391311646,\n",
       " 0.09594264626502991,\n",
       " 0.09474903345108032,\n",
       " 0.10047468543052673,\n",
       " 0.10433398187160492,\n",
       " 0.09327647089958191,\n",
       " 0.09472876042127609,\n",
       " 0.0941024124622345,\n",
       " 0.11199027299880981,\n",
       " 0.09510178864002228,\n",
       " 0.09497474133968353,\n",
       " 0.09335363656282425,\n",
       " 0.10996013879776001,\n",
       " 0.09703685343265533,\n",
       " 0.09482301771640778,\n",
       " 0.09357857704162598,\n",
       " 0.10151178389787674,\n",
       " 0.10351875424385071,\n",
       " 0.09255220741033554,\n",
       " 0.09250283986330032,\n",
       " 0.1035289615392685,\n",
       " 0.10284517705440521,\n",
       " 0.09433016180992126,\n",
       " 0.09406042098999023,\n",
       " 0.09375736117362976,\n",
       " 0.09460682421922684,\n",
       " 0.09724044799804688,\n",
       " 0.09831336885690689,\n",
       " 0.0968969315290451,\n",
       " 0.09412942081689835,\n",
       " 0.09499649703502655,\n",
       " 0.09825511276721954,\n",
       " 0.09736704081296921,\n",
       " 0.09745991975069046,\n",
       " 0.0966075137257576,\n",
       " 0.09378737211227417,\n",
       " 0.09443186968564987,\n",
       " 0.09638389945030212,\n",
       " 0.09567179530858994,\n",
       " 0.09410625696182251,\n",
       " 0.0970940813422203,\n",
       " 0.09877677261829376,\n",
       " 0.09680279344320297,\n",
       " 0.0965084657073021,\n",
       " 0.09434405714273453,\n",
       " 0.09377513080835342,\n",
       " 0.093748077750206,\n",
       " 0.09630432724952698,\n",
       " 0.096316397190094,\n",
       " 0.09268264472484589,\n",
       " 0.09255079925060272,\n",
       " 0.09327654540538788,\n",
       " 0.09575760364532471,\n",
       " 0.09752701967954636,\n",
       " 0.09423615038394928,\n",
       " 0.0922882929444313,\n",
       " 0.09269147366285324,\n",
       " 0.092985138297081,\n",
       " 0.0927228257060051,\n",
       " 0.09553194046020508,\n",
       " 0.09734004735946655,\n",
       " 0.09329106658697128,\n",
       " 0.09157624840736389,\n",
       " 0.09219281375408173,\n",
       " 0.09405086934566498,\n",
       " 0.09649353474378586,\n",
       " 0.09917281568050385,\n",
       " 0.09427407383918762,\n",
       " 0.09391504526138306,\n",
       " 0.09668631106615067,\n",
       " 0.09553149342536926,\n",
       " 0.09295906871557236,\n",
       " 0.0931520164012909,\n",
       " 0.10082170367240906,\n",
       " 0.09754548966884613,\n",
       " 0.09152403473854065,\n",
       " 0.09267833828926086,\n",
       " 0.09737865626811981,\n",
       " 0.09318380057811737,\n",
       " 0.09162815660238266,\n",
       " 0.0919356495141983,\n",
       " 0.09535062313079834,\n",
       " 0.09496250748634338,\n",
       " 0.09297128766775131,\n",
       " 0.0968070775270462,\n",
       " 0.09820075333118439,\n",
       " 0.09172987937927246,\n",
       " 0.09213238209486008,\n",
       " 0.09228482097387314,\n",
       " 0.09937592595815659,\n",
       " 0.10001450031995773,\n",
       " 0.09253711998462677,\n",
       " 0.09326612949371338,\n",
       " 0.09489491581916809,\n",
       " 0.10401035100221634,\n",
       " 0.09220077842473984,\n",
       " 0.09504794329404831,\n",
       " 0.09294699877500534,\n",
       " 0.09776611626148224,\n",
       " 0.09496444463729858,\n",
       " 0.0911049097776413,\n",
       " 0.09100614488124847,\n",
       " 0.09678251296281815,\n",
       " 0.09956368058919907,\n",
       " 0.09189748764038086,\n",
       " 0.09171996265649796,\n",
       " 0.10119976103305817,\n",
       " 0.09692602604627609,\n",
       " 0.0912122055888176,\n",
       " 0.09206439554691315,\n",
       " 0.0919569879770279,\n",
       " 0.1013827696442604,\n",
       " 0.09081771969795227,\n",
       " 0.09406674653291702,\n",
       " 0.09218674898147583,\n",
       " 0.09550457447767258,\n",
       " 0.09539169818162918,\n",
       " 0.0911337360739708,\n",
       " 0.09094826877117157,\n",
       " 0.09116671979427338,\n",
       " 0.0958901196718216,\n",
       " 0.09871640056371689,\n",
       " 0.09192977100610733,\n",
       " 0.09163075685501099,\n",
       " 0.09387611597776413,\n",
       " 0.09389494359493256,\n",
       " 0.09111549705266953,\n",
       " 0.09108822047710419,\n",
       " 0.09639572352170944,\n",
       " 0.09322512149810791,\n",
       " 0.09078055620193481,\n",
       " 0.09097178280353546,\n",
       " 0.09110187739133835,\n",
       " 0.0934523269534111,\n",
       " 0.0948488712310791,\n",
       " 0.09522580355405807,\n",
       " 0.09353845566511154,\n",
       " 0.09141278266906738,\n",
       " 0.09125878661870956,\n",
       " 0.09354005753993988,\n",
       " 0.09329445660114288,\n",
       " 0.09155751764774323,\n",
       " 0.09192771464586258,\n",
       " 0.09630154073238373,\n",
       " 0.09436308592557907,\n",
       " 0.09200681000947952,\n",
       " 0.09194976836442947,\n",
       " 0.09541960060596466,\n",
       " 0.09766317903995514,\n",
       " 0.09288346022367477,\n",
       " 0.09195602685213089,\n",
       " 0.09415896981954575,\n",
       " 0.09519945830106735,\n",
       " 0.09185291081666946,\n",
       " 0.09132374823093414,\n",
       " 0.09531722217798233,\n",
       " 0.0942799523472786,\n",
       " 0.09159639477729797,\n",
       " 0.09099017083644867,\n",
       " 0.09864812344312668,\n",
       " 0.09849907457828522,\n",
       " 0.09206277132034302,\n",
       " 0.09285852313041687,\n",
       " 0.09157329797744751,\n",
       " 0.09961291402578354,\n",
       " 0.09128960222005844,\n",
       " 0.09125496447086334,\n",
       " 0.09552354365587234,\n",
       " 0.09761057794094086,\n",
       " 0.09500783681869507,\n",
       " 0.09273605048656464,\n",
       " 0.09181471168994904,\n",
       " 0.09446730464696884,\n",
       " 0.09865113347768784,\n",
       " 0.09140238910913467,\n",
       " 0.09067822992801666,\n",
       " 0.09565747529268265,\n",
       " 0.09381743520498276,\n",
       " 0.0894000232219696,\n",
       " 0.0898270234465599,\n",
       " 0.09917537868022919,\n",
       " 0.09070692956447601,\n",
       " 0.09207668155431747,\n",
       " 0.09127602726221085,\n",
       " 0.10429800301790237,\n",
       " 0.09519249200820923,\n",
       " 0.09337350726127625,\n",
       " 0.09254811704158783,\n",
       " 0.10504848510026932,\n",
       " 0.10343774408102036,\n",
       " 0.09098637849092484,\n",
       " 0.09090118110179901,\n",
       " 0.09270080178976059,\n",
       " 0.09294872730970383,\n",
       " 0.09078451246023178,\n",
       " 0.0899263471364975,\n",
       " 0.09024661779403687,\n",
       " 0.09062509983778,\n",
       " 0.09322649240493774,\n",
       " 0.09809137135744095,\n",
       " 0.09049830585718155,\n",
       " 0.09234187006950378,\n",
       " 0.09254803508520126,\n",
       " 0.11025676131248474,\n",
       " 0.09123490750789642,\n",
       " 0.09601116180419922,\n",
       " 0.09151924401521683,\n",
       " 0.10835129767656326,\n",
       " 0.09695415943861008,\n",
       " 0.09243617206811905,\n",
       " 0.09219430387020111,\n",
       " 0.09223999828100204,\n",
       " 0.09785380959510803,\n",
       " 0.09351707249879837,\n",
       " 0.09223092347383499,\n",
       " 0.0919932872056961,\n",
       " 0.09182411432266235,\n",
       " 0.09179139137268066,\n",
       " 0.09207862615585327,\n",
       " 0.09234699606895447,\n",
       " 0.09774241596460342,\n",
       " 0.09636618942022324,\n",
       " 0.09305790811777115,\n",
       " 0.09390337020158768,\n",
       " 0.09234928339719772,\n",
       " 0.09703703969717026,\n",
       " 0.09427965432405472,\n",
       " 0.09133220463991165,\n",
       " 0.09205788373947144,\n",
       " 0.0927962064743042,\n",
       " 0.09178172796964645,\n",
       " 0.09208279103040695,\n",
       " 0.09393132477998734,\n",
       " 0.09787824004888535,\n",
       " 0.09423641860485077,\n",
       " 0.09278874844312668,\n",
       " 0.09653426706790924,\n",
       " 0.09996447712182999,\n",
       " 0.09212362766265869,\n",
       " 0.09230005741119385,\n",
       " 0.09109663218259811,\n",
       " 0.09182659536600113,\n",
       " 0.09008205682039261,\n",
       " 0.0900542214512825,\n",
       " 0.09157314896583557,\n",
       " 0.0921822115778923,\n",
       " 0.09332741796970367,\n",
       " 0.09093562513589859,\n",
       " 0.09112507849931717,\n",
       " 0.09626905620098114,\n",
       " 0.09962668269872665,\n",
       " 0.0924687385559082,\n",
       " 0.0928717702627182,\n",
       " 0.09376867115497589,\n",
       " 0.0967583954334259,\n",
       " 0.09247096627950668,\n",
       " 0.09245238453149796,\n",
       " 0.09639198333024979,\n",
       " 0.09445510059595108,\n",
       " 0.09269385784864426,\n",
       " 0.0931333377957344,\n",
       " 0.09806247800588608,\n",
       " 0.09819160401821136,\n",
       " 0.09455714374780655,\n",
       " 0.09422533214092255,\n",
       " 0.09505592286586761,\n",
       " 0.09309983253479004,\n",
       " 0.09333259612321854,\n",
       " 0.09418633580207825,\n",
       " 0.09491008520126343,\n",
       " 0.09348136931657791,\n",
       " 0.09241312742233276,\n",
       " 0.09226348996162415,\n",
       " 0.09283759444952011,\n",
       " 0.09291402995586395,\n",
       " 0.09282214939594269,\n",
       " 0.09466903656721115,\n",
       " 0.0960952565073967,\n",
       " 0.09355801343917847,\n",
       " 0.09594950079917908,\n",
       " 0.1086932122707367,\n",
       " 0.09588729590177536,\n",
       " 0.09370395541191101,\n",
       " 0.09385179728269577,\n",
       " 0.09677860885858536,\n",
       " 0.09555287659168243,\n",
       " 0.09440561383962631,\n",
       " 0.09514205902814865,\n",
       " 0.09658355265855789,\n",
       " 0.09629875421524048,\n",
       " 0.09516525268554688,\n",
       " 0.09616707265377045,\n",
       " 0.09611613303422928,\n",
       " 0.0924374982714653,\n",
       " 0.09214454889297485,\n",
       " 0.09550480544567108,\n",
       " 0.09203184396028519,\n",
       " 0.09162158519029617,\n",
       " 0.09564967453479767,\n",
       " 0.09540915489196777,\n",
       " 0.09234930574893951,\n",
       " 0.09252158552408218,\n",
       " 0.09349203109741211,\n",
       " 0.09749875962734222,\n",
       " 0.09634990245103836,\n",
       " 0.09203195571899414,\n",
       " 0.0914759486913681,\n",
       " 0.09116220474243164,\n",
       " 0.09628109633922577,\n",
       " 0.09353789687156677,\n",
       " 0.09244805574417114,\n",
       " 0.09239649027585983,\n",
       " 0.09980282187461853,\n",
       " 0.09476352483034134,\n",
       " 0.09307385236024857,\n",
       " 0.0959843173623085,\n",
       " 0.09695366770029068,\n",
       " 0.0943722203373909,\n",
       " 0.09432606399059296,\n",
       " 0.0977972149848938,\n",
       " 0.09947294741868973,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15fe2218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트 셋으로 실험 결과의 오차 값 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습 셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize = 3)\n",
    "plt.plot(x_len, y_acc, \"o\", c='blue', markersize = 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56848fe7",
   "metadata": {},
   "source": [
    "- 학습이 진행될수록 학습셋의 정확도는 올라가지만 과적합 때문에 테스트셋의 실험 결과는 점점 나빠지게 됨<br><br>\n",
    "\n",
    "- EarlyStopping()함수 : 테스트셋 오차가 줄지 않으면 학습을 멈추게 하는 keras함수<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa8d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "df_pre = pd.read_csv('../dataset/wine.csv', header = None)\n",
    "df = df_pre.sample(frac=0.15)\n",
    "X = df.values[:,0:12]\n",
    "Y = df.values[:,12]\n",
    "Y\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "train_test_split(X, Y, test_size = 0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9931658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(30, input_dim = 12, activation='relu'))\n",
    "model_2.add(Dense(12, activation = 'relu'))\n",
    "model_2.add(Dense(8, activation = 'relu'))\n",
    "model_2.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e7e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "2/2 [==============================] - 1s 140ms/step - loss: 2.0482 - accuracy: 0.7259 - val_loss: 1.3696 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36965, saving model to ./model_100\\01-1.3696_train.hdf5\n",
      "Epoch 2/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 1.5577 - accuracy: 0.7259 - val_loss: 0.9768 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36965 to 0.97676, saving model to ./model_100\\02-0.9768_train.hdf5\n",
      "Epoch 3/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 1.0864 - accuracy: 0.7259 - val_loss: 0.6556 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97676 to 0.65557, saving model to ./model_100\\03-0.6556_train.hdf5\n",
      "Epoch 4/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.7075 - accuracy: 0.7243 - val_loss: 0.7205 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65557\n",
      "Epoch 5/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.7634 - accuracy: 0.5865 - val_loss: 0.8499 - val_accuracy: 0.4472\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.65557\n",
      "Epoch 6/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7918 - accuracy: 0.5268 - val_loss: 0.5612 - val_accuracy: 0.7236\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.65557 to 0.56124, saving model to ./model_100\\06-0.5612_train.hdf5\n",
      "Epoch 7/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5716 - accuracy: 0.7136 - val_loss: 0.4968 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56124 to 0.49682, saving model to ./model_100\\07-0.4968_train.hdf5\n",
      "Epoch 8/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.5545 - accuracy: 0.7228 - val_loss: 0.5096 - val_accuracy: 0.7578\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49682\n",
      "Epoch 9/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.5838 - accuracy: 0.7243 - val_loss: 0.5104 - val_accuracy: 0.7578\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49682\n",
      "Epoch 10/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5876 - accuracy: 0.7243 - val_loss: 0.4879 - val_accuracy: 0.7578\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49682 to 0.48795, saving model to ./model_100\\10-0.4879_train.hdf5\n",
      "Epoch 11/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5596 - accuracy: 0.7274 - val_loss: 0.4497 - val_accuracy: 0.7609\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48795 to 0.44969, saving model to ./model_100\\11-0.4497_train.hdf5\n",
      "Epoch 12/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.5128 - accuracy: 0.7320 - val_loss: 0.4129 - val_accuracy: 0.7609\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.44969 to 0.41293, saving model to ./model_100\\12-0.4129_train.hdf5\n",
      "Epoch 13/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4633 - accuracy: 0.7443 - val_loss: 0.4008 - val_accuracy: 0.7764\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.41293 to 0.40077, saving model to ./model_100\\13-0.4008_train.hdf5\n",
      "Epoch 14/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4353 - accuracy: 0.7810 - val_loss: 0.4156 - val_accuracy: 0.7702\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.40077\n",
      "Epoch 15/1500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4326 - accuracy: 0.7902 - val_loss: 0.4201 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.40077\n",
      "Epoch 16/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4281 - accuracy: 0.8193 - val_loss: 0.3954 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.40077 to 0.39543, saving model to ./model_100\\16-0.3954_train.hdf5\n",
      "Epoch 17/1500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4088 - accuracy: 0.8346 - val_loss: 0.3660 - val_accuracy: 0.8230\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.39543 to 0.36598, saving model to ./model_100\\17-0.3660_train.hdf5\n",
      "Epoch 18/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3899 - accuracy: 0.8423 - val_loss: 0.3474 - val_accuracy: 0.8478\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.36598 to 0.34737, saving model to ./model_100\\18-0.3474_train.hdf5\n",
      "Epoch 19/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3813 - accuracy: 0.8545 - val_loss: 0.3369 - val_accuracy: 0.8540\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.34737 to 0.33686, saving model to ./model_100\\19-0.3369_train.hdf5\n",
      "Epoch 20/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3763 - accuracy: 0.8576 - val_loss: 0.3289 - val_accuracy: 0.8602\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.33686 to 0.32892, saving model to ./model_100\\20-0.3289_train.hdf5\n",
      "Epoch 21/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3710 - accuracy: 0.8591 - val_loss: 0.3216 - val_accuracy: 0.8634\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.32892 to 0.32158, saving model to ./model_100\\21-0.3216_train.hdf5\n",
      "Epoch 22/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3605 - accuracy: 0.8760 - val_loss: 0.3175 - val_accuracy: 0.8789\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.32158 to 0.31745, saving model to ./model_100\\22-0.3175_train.hdf5\n",
      "Epoch 23/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3501 - accuracy: 0.8851 - val_loss: 0.3187 - val_accuracy: 0.8851\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31745\n",
      "Epoch 24/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3453 - accuracy: 0.9020 - val_loss: 0.3209 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.31745\n",
      "Epoch 25/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3422 - accuracy: 0.9020 - val_loss: 0.3151 - val_accuracy: 0.8944\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.31745 to 0.31513, saving model to ./model_100\\25-0.3151_train.hdf5\n",
      "Epoch 26/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.3365 - accuracy: 0.9035 - val_loss: 0.3033 - val_accuracy: 0.8944\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.31513 to 0.30334, saving model to ./model_100\\26-0.3033_train.hdf5\n",
      "Epoch 27/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3287 - accuracy: 0.9051 - val_loss: 0.2917 - val_accuracy: 0.8944\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.30334 to 0.29166, saving model to ./model_100\\27-0.2917_train.hdf5\n",
      "Epoch 28/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3232 - accuracy: 0.9066 - val_loss: 0.2836 - val_accuracy: 0.8913\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.29166 to 0.28358, saving model to ./model_100\\28-0.2836_train.hdf5\n",
      "Epoch 29/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3194 - accuracy: 0.9066 - val_loss: 0.2780 - val_accuracy: 0.8913\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.28358 to 0.27803, saving model to ./model_100\\29-0.2780_train.hdf5\n",
      "Epoch 30/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.3165 - accuracy: 0.9066 - val_loss: 0.2736 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.27803 to 0.27361, saving model to ./model_100\\30-0.2736_train.hdf5\n",
      "Epoch 31/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3128 - accuracy: 0.9066 - val_loss: 0.2703 - val_accuracy: 0.8975\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.27361 to 0.27031, saving model to ./model_100\\31-0.2703_train.hdf5\n",
      "Epoch 32/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3079 - accuracy: 0.9081 - val_loss: 0.2690 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.27031 to 0.26904, saving model to ./model_100\\32-0.2690_train.hdf5\n",
      "Epoch 33/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3036 - accuracy: 0.9096 - val_loss: 0.2697 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.26904\n",
      "Epoch 34/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.3011 - accuracy: 0.9112 - val_loss: 0.2690 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.26904 to 0.26902, saving model to ./model_100\\34-0.2690_train.hdf5\n",
      "Epoch 35/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2985 - accuracy: 0.9142 - val_loss: 0.2655 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.26902 to 0.26552, saving model to ./model_100\\35-0.2655_train.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2957 - accuracy: 0.9142 - val_loss: 0.2614 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.26552 to 0.26141, saving model to ./model_100\\36-0.2614_train.hdf5\n",
      "Epoch 37/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2925 - accuracy: 0.9158 - val_loss: 0.2554 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.26141 to 0.25538, saving model to ./model_100\\37-0.2554_train.hdf5\n",
      "Epoch 38/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2910 - accuracy: 0.9127 - val_loss: 0.2507 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.25538 to 0.25066, saving model to ./model_100\\38-0.2507_train.hdf5\n",
      "Epoch 39/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2877 - accuracy: 0.9127 - val_loss: 0.2483 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.25066 to 0.24829, saving model to ./model_100\\39-0.2483_train.hdf5\n",
      "Epoch 40/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2853 - accuracy: 0.9127 - val_loss: 0.2459 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.24829 to 0.24593, saving model to ./model_100\\40-0.2459_train.hdf5\n",
      "Epoch 41/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2831 - accuracy: 0.9127 - val_loss: 0.2440 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.24593 to 0.24398, saving model to ./model_100\\41-0.2440_train.hdf5\n",
      "Epoch 42/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2810 - accuracy: 0.9142 - val_loss: 0.2423 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.24398 to 0.24229, saving model to ./model_100\\42-0.2423_train.hdf5\n",
      "Epoch 43/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2786 - accuracy: 0.9142 - val_loss: 0.2418 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.24229 to 0.24185, saving model to ./model_100\\43-0.2418_train.hdf5\n",
      "Epoch 44/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2764 - accuracy: 0.9173 - val_loss: 0.2411 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.24185 to 0.24108, saving model to ./model_100\\44-0.2411_train.hdf5\n",
      "Epoch 45/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2748 - accuracy: 0.9158 - val_loss: 0.2395 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.24108 to 0.23953, saving model to ./model_100\\45-0.2395_train.hdf5\n",
      "Epoch 46/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2731 - accuracy: 0.9158 - val_loss: 0.2372 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.23953 to 0.23721, saving model to ./model_100\\46-0.2372_train.hdf5\n",
      "Epoch 47/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2708 - accuracy: 0.9158 - val_loss: 0.2358 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.23721 to 0.23581, saving model to ./model_100\\47-0.2358_train.hdf5\n",
      "Epoch 48/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2688 - accuracy: 0.9158 - val_loss: 0.2333 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.23581 to 0.23332, saving model to ./model_100\\48-0.2333_train.hdf5\n",
      "Epoch 49/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2660 - accuracy: 0.9173 - val_loss: 0.2293 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.23332 to 0.22928, saving model to ./model_100\\49-0.2293_train.hdf5\n",
      "Epoch 50/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2636 - accuracy: 0.9188 - val_loss: 0.2257 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.22928 to 0.22569, saving model to ./model_100\\50-0.2257_train.hdf5\n",
      "Epoch 51/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2618 - accuracy: 0.9188 - val_loss: 0.2232 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.22569 to 0.22319, saving model to ./model_100\\51-0.2232_train.hdf5\n",
      "Epoch 52/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2609 - accuracy: 0.9219 - val_loss: 0.2225 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.22319 to 0.22255, saving model to ./model_100\\52-0.2225_train.hdf5\n",
      "Epoch 53/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2592 - accuracy: 0.9234 - val_loss: 0.2238 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22255\n",
      "Epoch 54/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2585 - accuracy: 0.9204 - val_loss: 0.2249 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.22255\n",
      "Epoch 55/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2581 - accuracy: 0.9204 - val_loss: 0.2245 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.22255\n",
      "Epoch 56/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2569 - accuracy: 0.9188 - val_loss: 0.2224 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.22255 to 0.22245, saving model to ./model_100\\56-0.2224_train.hdf5\n",
      "Epoch 57/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2557 - accuracy: 0.9204 - val_loss: 0.2208 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.22245 to 0.22078, saving model to ./model_100\\57-0.2208_train.hdf5\n",
      "Epoch 58/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2550 - accuracy: 0.9219 - val_loss: 0.2204 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.22078 to 0.22043, saving model to ./model_100\\58-0.2204_train.hdf5\n",
      "Epoch 59/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2540 - accuracy: 0.9234 - val_loss: 0.2214 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.22043\n",
      "Epoch 60/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2535 - accuracy: 0.9204 - val_loss: 0.2212 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.22043\n",
      "Epoch 61/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2524 - accuracy: 0.9204 - val_loss: 0.2193 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.22043 to 0.21935, saving model to ./model_100\\61-0.2193_train.hdf5\n",
      "Epoch 62/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2516 - accuracy: 0.9234 - val_loss: 0.2175 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.21935 to 0.21754, saving model to ./model_100\\62-0.2175_train.hdf5\n",
      "Epoch 63/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2511 - accuracy: 0.9234 - val_loss: 0.2162 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.21754 to 0.21622, saving model to ./model_100\\63-0.2162_train.hdf5\n",
      "Epoch 64/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2508 - accuracy: 0.9234 - val_loss: 0.2153 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.21622 to 0.21528, saving model to ./model_100\\64-0.2153_train.hdf5\n",
      "Epoch 65/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2505 - accuracy: 0.9234 - val_loss: 0.2145 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.21528 to 0.21454, saving model to ./model_100\\65-0.2145_train.hdf5\n",
      "Epoch 66/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2503 - accuracy: 0.9219 - val_loss: 0.2145 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.21454 to 0.21449, saving model to ./model_100\\66-0.2145_train.hdf5\n",
      "Epoch 67/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.2499 - accuracy: 0.9219 - val_loss: 0.2153 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.21449\n",
      "Epoch 68/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2486 - accuracy: 0.9204 - val_loss: 0.2179 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.21449\n",
      "Epoch 69/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2472 - accuracy: 0.9234 - val_loss: 0.2210 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.21449\n",
      "Epoch 70/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2468 - accuracy: 0.9234 - val_loss: 0.2223 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.21449\n",
      "Epoch 71/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2462 - accuracy: 0.9250 - val_loss: 0.2211 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.21449\n",
      "Epoch 72/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2454 - accuracy: 0.9234 - val_loss: 0.2191 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.21449\n",
      "Epoch 73/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2445 - accuracy: 0.9234 - val_loss: 0.2169 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.21449\n",
      "Epoch 74/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2437 - accuracy: 0.9234 - val_loss: 0.2152 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.21449\n",
      "Epoch 75/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2438 - accuracy: 0.9219 - val_loss: 0.2144 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.21449 to 0.21438, saving model to ./model_100\\75-0.2144_train.hdf5\n",
      "Epoch 76/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2436 - accuracy: 0.9219 - val_loss: 0.2146 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.21438\n",
      "Epoch 77/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2424 - accuracy: 0.9219 - val_loss: 0.2146 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.21438\n",
      "Epoch 78/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2419 - accuracy: 0.9234 - val_loss: 0.2155 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.21438\n",
      "Epoch 79/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2406 - accuracy: 0.9234 - val_loss: 0.2158 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.21438\n",
      "Epoch 80/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2399 - accuracy: 0.9250 - val_loss: 0.2166 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.21438\n",
      "Epoch 81/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2396 - accuracy: 0.9250 - val_loss: 0.2176 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.21438\n",
      "Epoch 82/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2397 - accuracy: 0.9219 - val_loss: 0.2189 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21438\n",
      "Epoch 83/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2389 - accuracy: 0.9219 - val_loss: 0.2157 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.21438\n",
      "Epoch 84/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2378 - accuracy: 0.9234 - val_loss: 0.2118 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.21438 to 0.21177, saving model to ./model_100\\84-0.2118_train.hdf5\n",
      "Epoch 85/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2371 - accuracy: 0.9265 - val_loss: 0.2094 - val_accuracy: 0.9068\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.21177 to 0.20936, saving model to ./model_100\\85-0.2094_train.hdf5\n",
      "Epoch 86/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2372 - accuracy: 0.9250 - val_loss: 0.2082 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.20936 to 0.20822, saving model to ./model_100\\86-0.2082_train.hdf5\n",
      "Epoch 87/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2377 - accuracy: 0.9219 - val_loss: 0.2077 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.20822 to 0.20773, saving model to ./model_100\\87-0.2077_train.hdf5\n",
      "Epoch 88/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2379 - accuracy: 0.9219 - val_loss: 0.2080 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.20773\n",
      "Epoch 89/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2365 - accuracy: 0.9219 - val_loss: 0.2090 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.20773\n",
      "Epoch 90/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2347 - accuracy: 0.9250 - val_loss: 0.2121 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.20773\n",
      "Epoch 91/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2341 - accuracy: 0.9265 - val_loss: 0.2168 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.20773\n",
      "Epoch 92/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2349 - accuracy: 0.9234 - val_loss: 0.2187 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.20773\n",
      "Epoch 93/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2351 - accuracy: 0.9219 - val_loss: 0.2162 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.20773\n",
      "Epoch 94/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2336 - accuracy: 0.9219 - val_loss: 0.2107 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.20773\n",
      "Epoch 95/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2334 - accuracy: 0.9265 - val_loss: 0.2076 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.20773 to 0.20758, saving model to ./model_100\\95-0.2076_train.hdf5\n",
      "Epoch 96/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2321 - accuracy: 0.9250 - val_loss: 0.2074 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.20758 to 0.20743, saving model to ./model_100\\96-0.2074_train.hdf5\n",
      "Epoch 97/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2316 - accuracy: 0.9250 - val_loss: 0.2082 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.20743\n",
      "Epoch 98/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2311 - accuracy: 0.9250 - val_loss: 0.2097 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.20743\n",
      "Epoch 99/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2306 - accuracy: 0.9265 - val_loss: 0.2106 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.20743\n",
      "Epoch 100/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2305 - accuracy: 0.9265 - val_loss: 0.2117 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.20743\n",
      "Epoch 101/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2308 - accuracy: 0.9265 - val_loss: 0.2124 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.20743\n",
      "Epoch 102/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2299 - accuracy: 0.9265 - val_loss: 0.2095 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.20743\n",
      "Epoch 103/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2288 - accuracy: 0.9265 - val_loss: 0.2068 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.20743 to 0.20680, saving model to ./model_100\\103-0.2068_train.hdf5\n",
      "Epoch 104/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2289 - accuracy: 0.9265 - val_loss: 0.2057 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.20680 to 0.20573, saving model to ./model_100\\104-0.2057_train.hdf5\n",
      "Epoch 105/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2283 - accuracy: 0.9265 - val_loss: 0.2068 - val_accuracy: 0.9099\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.20573\n",
      "Epoch 106/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2277 - accuracy: 0.9265 - val_loss: 0.2059 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.20573\n",
      "Epoch 107/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2270 - accuracy: 0.9265 - val_loss: 0.2040 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.20573 to 0.20401, saving model to ./model_100\\107-0.2040_train.hdf5\n",
      "Epoch 108/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2268 - accuracy: 0.9265 - val_loss: 0.2033 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.20401 to 0.20333, saving model to ./model_100\\108-0.2033_train.hdf5\n",
      "Epoch 109/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2265 - accuracy: 0.9265 - val_loss: 0.2045 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.20333\n",
      "Epoch 110/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2264 - accuracy: 0.9265 - val_loss: 0.2059 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.20333\n",
      "Epoch 111/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2260 - accuracy: 0.9265 - val_loss: 0.2064 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.20333\n",
      "Epoch 112/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2257 - accuracy: 0.9265 - val_loss: 0.2075 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.20333\n",
      "Epoch 113/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2254 - accuracy: 0.9280 - val_loss: 0.2088 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.20333\n",
      "Epoch 114/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2250 - accuracy: 0.9280 - val_loss: 0.2066 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.20333\n",
      "Epoch 115/1500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.2240 - accuracy: 0.9280 - val_loss: 0.2031 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.20333 to 0.20310, saving model to ./model_100\\115-0.2031_train.hdf5\n",
      "Epoch 116/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2232 - accuracy: 0.9280 - val_loss: 0.2006 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.20310 to 0.20064, saving model to ./model_100\\116-0.2006_train.hdf5\n",
      "Epoch 117/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2235 - accuracy: 0.9280 - val_loss: 0.1997 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.20064 to 0.19969, saving model to ./model_100\\117-0.1997_train.hdf5\n",
      "Epoch 118/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2233 - accuracy: 0.9280 - val_loss: 0.2000 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.19969\n",
      "Epoch 119/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2225 - accuracy: 0.9280 - val_loss: 0.2003 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.19969\n",
      "Epoch 120/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2218 - accuracy: 0.9280 - val_loss: 0.2002 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.19969\n",
      "Epoch 121/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2212 - accuracy: 0.9280 - val_loss: 0.1998 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.19969\n",
      "Epoch 122/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2207 - accuracy: 0.9280 - val_loss: 0.1997 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.19969\n",
      "Epoch 123/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2203 - accuracy: 0.9280 - val_loss: 0.2008 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.19969\n",
      "Epoch 124/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2197 - accuracy: 0.9280 - val_loss: 0.2018 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.19969\n",
      "Epoch 125/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2193 - accuracy: 0.9280 - val_loss: 0.2025 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.19969\n",
      "Epoch 126/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2189 - accuracy: 0.9280 - val_loss: 0.2020 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.19969\n",
      "Epoch 127/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2184 - accuracy: 0.9280 - val_loss: 0.2005 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.19969\n",
      "Epoch 128/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2183 - accuracy: 0.9280 - val_loss: 0.1994 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.19969 to 0.19942, saving model to ./model_100\\128-0.1994_train.hdf5\n",
      "Epoch 129/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2177 - accuracy: 0.9280 - val_loss: 0.2005 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.19942\n",
      "Epoch 130/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2177 - accuracy: 0.9280 - val_loss: 0.2009 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.19942\n",
      "Epoch 131/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2171 - accuracy: 0.9280 - val_loss: 0.1988 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.19942 to 0.19879, saving model to ./model_100\\131-0.1988_train.hdf5\n",
      "Epoch 132/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2166 - accuracy: 0.9280 - val_loss: 0.1967 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.19879 to 0.19665, saving model to ./model_100\\132-0.1967_train.hdf5\n",
      "Epoch 133/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2160 - accuracy: 0.9280 - val_loss: 0.1964 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.19665 to 0.19642, saving model to ./model_100\\133-0.1964_train.hdf5\n",
      "Epoch 134/1500\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2155 - accuracy: 0.9280 - val_loss: 0.1971 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.19642\n",
      "Epoch 135/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2151 - accuracy: 0.9280 - val_loss: 0.1975 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.19642\n",
      "Epoch 136/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2147 - accuracy: 0.9280 - val_loss: 0.1967 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.19642\n",
      "Epoch 137/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2140 - accuracy: 0.9280 - val_loss: 0.1950 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.19642 to 0.19495, saving model to ./model_100\\137-0.1950_train.hdf5\n",
      "Epoch 138/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2139 - accuracy: 0.9265 - val_loss: 0.1940 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.19495 to 0.19401, saving model to ./model_100\\138-0.1940_train.hdf5\n",
      "Epoch 139/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2145 - accuracy: 0.9265 - val_loss: 0.1939 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.19401 to 0.19391, saving model to ./model_100\\139-0.1939_train.hdf5\n",
      "Epoch 140/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2143 - accuracy: 0.9265 - val_loss: 0.1946 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.19391\n",
      "Epoch 141/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2128 - accuracy: 0.9280 - val_loss: 0.1984 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.19391\n",
      "Epoch 142/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2117 - accuracy: 0.9280 - val_loss: 0.2019 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.19391\n",
      "Epoch 143/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2136 - accuracy: 0.9296 - val_loss: 0.2015 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.19391\n",
      "Epoch 144/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2135 - accuracy: 0.9265 - val_loss: 0.1967 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.19391\n",
      "Epoch 145/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2117 - accuracy: 0.9280 - val_loss: 0.1937 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.19391 to 0.19374, saving model to ./model_100\\145-0.1937_train.hdf5\n",
      "Epoch 146/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2106 - accuracy: 0.9280 - val_loss: 0.1923 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.19374 to 0.19231, saving model to ./model_100\\146-0.1923_train.hdf5\n",
      "Epoch 147/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2103 - accuracy: 0.9280 - val_loss: 0.1919 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.19231 to 0.19189, saving model to ./model_100\\147-0.1919_train.hdf5\n",
      "Epoch 148/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2106 - accuracy: 0.9265 - val_loss: 0.1934 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.19189\n",
      "Epoch 149/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2102 - accuracy: 0.9265 - val_loss: 0.1972 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.19189\n",
      "Epoch 150/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2101 - accuracy: 0.9280 - val_loss: 0.1990 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.19189\n",
      "Epoch 151/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2095 - accuracy: 0.9280 - val_loss: 0.1991 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.19189\n",
      "Epoch 152/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2088 - accuracy: 0.9265 - val_loss: 0.1991 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.19189\n",
      "Epoch 153/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2086 - accuracy: 0.9280 - val_loss: 0.1977 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.19189\n",
      "Epoch 154/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2078 - accuracy: 0.9265 - val_loss: 0.1983 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.19189\n",
      "Epoch 155/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2077 - accuracy: 0.9265 - val_loss: 0.1978 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.19189\n",
      "Epoch 156/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2061 - accuracy: 0.9280 - val_loss: 0.1919 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.19189 to 0.19185, saving model to ./model_100\\156-0.1919_train.hdf5\n",
      "Epoch 157/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2067 - accuracy: 0.9296 - val_loss: 0.1883 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.19185 to 0.18833, saving model to ./model_100\\157-0.1883_train.hdf5\n",
      "Epoch 158/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2077 - accuracy: 0.9204 - val_loss: 0.1880 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.18833 to 0.18801, saving model to ./model_100\\158-0.1880_train.hdf5\n",
      "Epoch 159/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2068 - accuracy: 0.9219 - val_loss: 0.1894 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.18801\n",
      "Epoch 160/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2047 - accuracy: 0.9296 - val_loss: 0.1939 - val_accuracy: 0.9161\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.18801\n",
      "Epoch 161/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2044 - accuracy: 0.9280 - val_loss: 0.1973 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.18801\n",
      "Epoch 162/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2046 - accuracy: 0.9280 - val_loss: 0.1955 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.18801\n",
      "Epoch 163/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2037 - accuracy: 0.9280 - val_loss: 0.1921 - val_accuracy: 0.9193\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.18801\n",
      "Epoch 164/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2026 - accuracy: 0.9280 - val_loss: 0.1901 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.18801\n",
      "Epoch 165/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2024 - accuracy: 0.9296 - val_loss: 0.1892 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.18801\n",
      "Epoch 166/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2024 - accuracy: 0.9296 - val_loss: 0.1899 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.18801\n",
      "Epoch 167/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2014 - accuracy: 0.9296 - val_loss: 0.1936 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.18801\n",
      "Epoch 168/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2024 - accuracy: 0.9280 - val_loss: 0.1966 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.18801\n",
      "Epoch 169/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2018 - accuracy: 0.9296 - val_loss: 0.1923 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.18801\n",
      "Epoch 170/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2008 - accuracy: 0.9280 - val_loss: 0.1896 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.18801\n",
      "Epoch 171/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2005 - accuracy: 0.9280 - val_loss: 0.1900 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.18801\n",
      "Epoch 172/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1993 - accuracy: 0.9280 - val_loss: 0.1941 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.18801\n",
      "Epoch 173/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2002 - accuracy: 0.9311 - val_loss: 0.1973 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.18801\n",
      "Epoch 174/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.2005 - accuracy: 0.9311 - val_loss: 0.1918 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.18801\n",
      "Epoch 175/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1983 - accuracy: 0.9280 - val_loss: 0.1858 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.18801 to 0.18582, saving model to ./model_100\\175-0.1858_train.hdf5\n",
      "Epoch 176/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1978 - accuracy: 0.9280 - val_loss: 0.1830 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.18582 to 0.18305, saving model to ./model_100\\176-0.1830_train.hdf5\n",
      "Epoch 177/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1986 - accuracy: 0.9280 - val_loss: 0.1830 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.18305 to 0.18299, saving model to ./model_100\\177-0.1830_train.hdf5\n",
      "Epoch 178/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1978 - accuracy: 0.9296 - val_loss: 0.1856 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.18299\n",
      "Epoch 179/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1960 - accuracy: 0.9280 - val_loss: 0.1903 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.18299\n",
      "Epoch 180/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1969 - accuracy: 0.9296 - val_loss: 0.1939 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.18299\n",
      "Epoch 181/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1969 - accuracy: 0.9342 - val_loss: 0.1888 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.18299\n",
      "Epoch 182/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1953 - accuracy: 0.9280 - val_loss: 0.1834 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.18299\n",
      "Epoch 183/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1950 - accuracy: 0.9280 - val_loss: 0.1811 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.18299 to 0.18114, saving model to ./model_100\\183-0.1811_train.hdf5\n",
      "Epoch 184/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1962 - accuracy: 0.9296 - val_loss: 0.1816 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.18114\n",
      "Epoch 185/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1950 - accuracy: 0.9280 - val_loss: 0.1860 - val_accuracy: 0.9224\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.18114\n",
      "Epoch 186/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1943 - accuracy: 0.9311 - val_loss: 0.1904 - val_accuracy: 0.9255\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.18114\n",
      "Epoch 187/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1943 - accuracy: 0.9357 - val_loss: 0.1867 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18114\n",
      "Epoch 188/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1923 - accuracy: 0.9311 - val_loss: 0.1806 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.18114 to 0.18059, saving model to ./model_100\\188-0.1806_train.hdf5\n",
      "Epoch 189/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1934 - accuracy: 0.9265 - val_loss: 0.1791 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.18059 to 0.17913, saving model to ./model_100\\189-0.1791_train.hdf5\n",
      "Epoch 190/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1948 - accuracy: 0.9234 - val_loss: 0.1797 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17913\n",
      "Epoch 191/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1926 - accuracy: 0.9280 - val_loss: 0.1851 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17913\n",
      "Epoch 192/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1923 - accuracy: 0.9296 - val_loss: 0.1937 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17913\n",
      "Epoch 193/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1939 - accuracy: 0.9357 - val_loss: 0.1895 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17913\n",
      "Epoch 194/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1919 - accuracy: 0.9357 - val_loss: 0.1818 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17913\n",
      "Epoch 195/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1891 - accuracy: 0.9326 - val_loss: 0.1778 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.17913 to 0.17778, saving model to ./model_100\\195-0.1778_train.hdf5\n",
      "Epoch 196/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1908 - accuracy: 0.9326 - val_loss: 0.1769 - val_accuracy: 0.9317\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.17778 to 0.17686, saving model to ./model_100\\196-0.1769_train.hdf5\n",
      "Epoch 197/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1892 - accuracy: 0.9311 - val_loss: 0.1810 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17686\n",
      "Epoch 198/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1881 - accuracy: 0.9342 - val_loss: 0.1873 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17686\n",
      "Epoch 199/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1903 - accuracy: 0.9372 - val_loss: 0.1849 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17686\n",
      "Epoch 200/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1893 - accuracy: 0.9342 - val_loss: 0.1780 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17686\n",
      "Epoch 201/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1871 - accuracy: 0.9326 - val_loss: 0.1757 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.17686 to 0.17570, saving model to ./model_100\\201-0.1757_train.hdf5\n",
      "Epoch 202/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1870 - accuracy: 0.9342 - val_loss: 0.1748 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.17570 to 0.17478, saving model to ./model_100\\202-0.1748_train.hdf5\n",
      "Epoch 203/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1869 - accuracy: 0.9342 - val_loss: 0.1761 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17478\n",
      "Epoch 204/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1866 - accuracy: 0.9372 - val_loss: 0.1802 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17478\n",
      "Epoch 205/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1856 - accuracy: 0.9372 - val_loss: 0.1793 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17478\n",
      "Epoch 206/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1851 - accuracy: 0.9372 - val_loss: 0.1765 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17478\n",
      "Epoch 207/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1841 - accuracy: 0.9372 - val_loss: 0.1731 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.17478 to 0.17308, saving model to ./model_100\\207-0.1731_train.hdf5\n",
      "Epoch 208/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1847 - accuracy: 0.9326 - val_loss: 0.1719 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.17308 to 0.17193, saving model to ./model_100\\208-0.1719_train.hdf5\n",
      "Epoch 209/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1846 - accuracy: 0.9311 - val_loss: 0.1730 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17193\n",
      "Epoch 210/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1839 - accuracy: 0.9357 - val_loss: 0.1751 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17193\n",
      "Epoch 211/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1826 - accuracy: 0.9372 - val_loss: 0.1744 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17193\n",
      "Epoch 212/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1825 - accuracy: 0.9387 - val_loss: 0.1726 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17193\n",
      "Epoch 213/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1827 - accuracy: 0.9403 - val_loss: 0.1713 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.17193 to 0.17128, saving model to ./model_100\\213-0.1713_train.hdf5\n",
      "Epoch 214/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1823 - accuracy: 0.9387 - val_loss: 0.1726 - val_accuracy: 0.9348\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17128\n",
      "Epoch 215/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1804 - accuracy: 0.9418 - val_loss: 0.1794 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17128\n",
      "Epoch 216/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1815 - accuracy: 0.9387 - val_loss: 0.1883 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17128\n",
      "Epoch 217/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1855 - accuracy: 0.9433 - val_loss: 0.1816 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17128\n",
      "Epoch 218/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1814 - accuracy: 0.9403 - val_loss: 0.1704 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.17128 to 0.17035, saving model to ./model_100\\218-0.1704_train.hdf5\n",
      "Epoch 219/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1793 - accuracy: 0.9387 - val_loss: 0.1669 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.17035 to 0.16689, saving model to ./model_100\\219-0.1669_train.hdf5\n",
      "Epoch 220/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1819 - accuracy: 0.9357 - val_loss: 0.1661 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.16689 to 0.16607, saving model to ./model_100\\220-0.1661_train.hdf5\n",
      "Epoch 221/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1823 - accuracy: 0.9357 - val_loss: 0.1663 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.16607\n",
      "Epoch 222/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1797 - accuracy: 0.9403 - val_loss: 0.1719 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.16607\n",
      "Epoch 223/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1786 - accuracy: 0.9418 - val_loss: 0.1816 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.16607\n",
      "Epoch 224/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1810 - accuracy: 0.9449 - val_loss: 0.1761 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.16607\n",
      "Epoch 225/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1770 - accuracy: 0.9464 - val_loss: 0.1663 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.16607\n",
      "Epoch 226/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1775 - accuracy: 0.9403 - val_loss: 0.1649 - val_accuracy: 0.9379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00226: val_loss improved from 0.16607 to 0.16495, saving model to ./model_100\\226-0.1649_train.hdf5\n",
      "Epoch 227/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1806 - accuracy: 0.9326 - val_loss: 0.1658 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.16495\n",
      "Epoch 228/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1787 - accuracy: 0.9342 - val_loss: 0.1701 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.16495\n",
      "Epoch 229/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1757 - accuracy: 0.9433 - val_loss: 0.1770 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.16495\n",
      "Epoch 230/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1775 - accuracy: 0.9479 - val_loss: 0.1779 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.16495\n",
      "Epoch 231/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1768 - accuracy: 0.9479 - val_loss: 0.1686 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.16495\n",
      "Epoch 232/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1737 - accuracy: 0.9464 - val_loss: 0.1629 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.16495 to 0.16291, saving model to ./model_100\\232-0.1629_train.hdf5\n",
      "Epoch 233/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1752 - accuracy: 0.9403 - val_loss: 0.1620 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.16291 to 0.16203, saving model to ./model_100\\233-0.1620_train.hdf5\n",
      "Epoch 234/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1749 - accuracy: 0.9387 - val_loss: 0.1647 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.16203\n",
      "Epoch 235/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1736 - accuracy: 0.9449 - val_loss: 0.1732 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.16203\n",
      "Epoch 236/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1740 - accuracy: 0.9495 - val_loss: 0.1717 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.16203\n",
      "Epoch 237/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1729 - accuracy: 0.9479 - val_loss: 0.1652 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.16203\n",
      "Epoch 238/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1715 - accuracy: 0.9464 - val_loss: 0.1627 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.16203\n",
      "Epoch 239/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1708 - accuracy: 0.9479 - val_loss: 0.1637 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.16203\n",
      "Epoch 240/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1708 - accuracy: 0.9479 - val_loss: 0.1646 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.16203\n",
      "Epoch 241/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1701 - accuracy: 0.9479 - val_loss: 0.1623 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.16203\n",
      "Epoch 242/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1700 - accuracy: 0.9479 - val_loss: 0.1602 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.16203 to 0.16021, saving model to ./model_100\\242-0.1602_train.hdf5\n",
      "Epoch 243/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1706 - accuracy: 0.9479 - val_loss: 0.1601 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.16021 to 0.16006, saving model to ./model_100\\243-0.1601_train.hdf5\n",
      "Epoch 244/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1703 - accuracy: 0.9495 - val_loss: 0.1622 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.16006\n",
      "Epoch 245/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1685 - accuracy: 0.9495 - val_loss: 0.1631 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.16006\n",
      "Epoch 246/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1679 - accuracy: 0.9495 - val_loss: 0.1656 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.16006\n",
      "Epoch 247/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1687 - accuracy: 0.9510 - val_loss: 0.1648 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.16006\n",
      "Epoch 248/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1668 - accuracy: 0.9495 - val_loss: 0.1580 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.16006 to 0.15803, saving model to ./model_100\\248-0.1580_train.hdf5\n",
      "Epoch 249/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1675 - accuracy: 0.9449 - val_loss: 0.1564 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.15803 to 0.15635, saving model to ./model_100\\249-0.1564_train.hdf5\n",
      "Epoch 250/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1679 - accuracy: 0.9433 - val_loss: 0.1575 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.15635\n",
      "Epoch 251/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1659 - accuracy: 0.9495 - val_loss: 0.1579 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.15635\n",
      "Epoch 252/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1651 - accuracy: 0.9510 - val_loss: 0.1569 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.15635\n",
      "Epoch 253/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1648 - accuracy: 0.9510 - val_loss: 0.1555 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.15635 to 0.15550, saving model to ./model_100\\253-0.1555_train.hdf5\n",
      "Epoch 254/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1648 - accuracy: 0.9510 - val_loss: 0.1551 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.15550 to 0.15512, saving model to ./model_100\\254-0.1551_train.hdf5\n",
      "Epoch 255/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1645 - accuracy: 0.9510 - val_loss: 0.1559 - val_accuracy: 0.9441\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.15512\n",
      "Epoch 256/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1633 - accuracy: 0.9510 - val_loss: 0.1572 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.15512\n",
      "Epoch 257/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1627 - accuracy: 0.9510 - val_loss: 0.1584 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.15512\n",
      "Epoch 258/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1623 - accuracy: 0.9525 - val_loss: 0.1584 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.15512\n",
      "Epoch 259/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1616 - accuracy: 0.9510 - val_loss: 0.1605 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.15512\n",
      "Epoch 260/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1620 - accuracy: 0.9525 - val_loss: 0.1586 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.15512\n",
      "Epoch 261/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1607 - accuracy: 0.9510 - val_loss: 0.1532 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.15512 to 0.15319, saving model to ./model_100\\261-0.1532_train.hdf5\n",
      "Epoch 262/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1605 - accuracy: 0.9495 - val_loss: 0.1516 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.15319 to 0.15159, saving model to ./model_100\\262-0.1516_train.hdf5\n",
      "Epoch 263/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1608 - accuracy: 0.9495 - val_loss: 0.1526 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.15159\n",
      "Epoch 264/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1598 - accuracy: 0.9510 - val_loss: 0.1553 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.15159\n",
      "Epoch 265/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1591 - accuracy: 0.9510 - val_loss: 0.1559 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00265: val_loss did not improve from 0.15159\n",
      "Epoch 266/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1585 - accuracy: 0.9510 - val_loss: 0.1528 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.15159\n",
      "Epoch 267/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1581 - accuracy: 0.9510 - val_loss: 0.1510 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.15159 to 0.15100, saving model to ./model_100\\267-0.1510_train.hdf5\n",
      "Epoch 268/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1577 - accuracy: 0.9495 - val_loss: 0.1528 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.15100\n",
      "Epoch 269/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1570 - accuracy: 0.9495 - val_loss: 0.1587 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.15100\n",
      "Epoch 270/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1582 - accuracy: 0.9541 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.15100\n",
      "Epoch 271/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1578 - accuracy: 0.9525 - val_loss: 0.1533 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.15100\n",
      "Epoch 272/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1562 - accuracy: 0.9510 - val_loss: 0.1515 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.15100\n",
      "Epoch 273/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1550 - accuracy: 0.9510 - val_loss: 0.1516 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.15100\n",
      "Epoch 274/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1546 - accuracy: 0.9525 - val_loss: 0.1524 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.15100\n",
      "Epoch 275/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1544 - accuracy: 0.9525 - val_loss: 0.1511 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.15100\n",
      "Epoch 276/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1539 - accuracy: 0.9510 - val_loss: 0.1492 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.15100 to 0.14924, saving model to ./model_100\\276-0.1492_train.hdf5\n",
      "Epoch 277/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1532 - accuracy: 0.9510 - val_loss: 0.1491 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.14924 to 0.14906, saving model to ./model_100\\277-0.1491_train.hdf5\n",
      "Epoch 278/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1530 - accuracy: 0.9510 - val_loss: 0.1530 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.14906\n",
      "Epoch 279/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1545 - accuracy: 0.9541 - val_loss: 0.1546 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.14906\n",
      "Epoch 280/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1528 - accuracy: 0.9541 - val_loss: 0.1459 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.14906 to 0.14587, saving model to ./model_100\\280-0.1459_train.hdf5\n",
      "Epoch 281/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1542 - accuracy: 0.9495 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.14587 to 0.14433, saving model to ./model_100\\281-0.1443_train.hdf5\n",
      "Epoch 282/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1530 - accuracy: 0.9510 - val_loss: 0.1498 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.14433\n",
      "Epoch 283/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1523 - accuracy: 0.9525 - val_loss: 0.1574 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.14433\n",
      "Epoch 284/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1524 - accuracy: 0.9541 - val_loss: 0.1475 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.14433\n",
      "Epoch 285/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1512 - accuracy: 0.9510 - val_loss: 0.1440 - val_accuracy: 0.9472\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.14433 to 0.14403, saving model to ./model_100\\285-0.1440_train.hdf5\n",
      "Epoch 286/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1502 - accuracy: 0.9510 - val_loss: 0.1475 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.14403\n",
      "Epoch 287/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1488 - accuracy: 0.9541 - val_loss: 0.1528 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.14403\n",
      "Epoch 288/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1499 - accuracy: 0.9541 - val_loss: 0.1483 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.14403\n",
      "Epoch 289/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1489 - accuracy: 0.9525 - val_loss: 0.1432 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.14403 to 0.14318, saving model to ./model_100\\289-0.1432_train.hdf5\n",
      "Epoch 290/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1489 - accuracy: 0.9510 - val_loss: 0.1443 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.14318\n",
      "Epoch 291/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1473 - accuracy: 0.9525 - val_loss: 0.1506 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.14318\n",
      "Epoch 292/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1487 - accuracy: 0.9541 - val_loss: 0.1501 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.14318\n",
      "Epoch 293/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1479 - accuracy: 0.9525 - val_loss: 0.1430 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.14318 to 0.14298, saving model to ./model_100\\293-0.1430_train.hdf5\n",
      "Epoch 294/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1471 - accuracy: 0.9510 - val_loss: 0.1426 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.14298 to 0.14255, saving model to ./model_100\\294-0.1426_train.hdf5\n",
      "Epoch 295/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1468 - accuracy: 0.9510 - val_loss: 0.1450 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.14255\n",
      "Epoch 296/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1454 - accuracy: 0.9510 - val_loss: 0.1475 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.14255\n",
      "Epoch 297/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1459 - accuracy: 0.9541 - val_loss: 0.1451 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.14255\n",
      "Epoch 298/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1444 - accuracy: 0.9525 - val_loss: 0.1400 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.14255 to 0.14000, saving model to ./model_100\\298-0.1400_train.hdf5\n",
      "Epoch 299/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1460 - accuracy: 0.9525 - val_loss: 0.1394 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.14000 to 0.13943, saving model to ./model_100\\299-0.1394_train.hdf5\n",
      "Epoch 300/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1457 - accuracy: 0.9525 - val_loss: 0.1436 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.13943\n",
      "Epoch 301/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1464 - accuracy: 0.9556 - val_loss: 0.1477 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.13943\n",
      "Epoch 302/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1444 - accuracy: 0.9556 - val_loss: 0.1405 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.13943\n",
      "Epoch 303/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1438 - accuracy: 0.9525 - val_loss: 0.1391 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.13943 to 0.13908, saving model to ./model_100\\303-0.1391_train.hdf5\n",
      "Epoch 304/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1431 - accuracy: 0.9525 - val_loss: 0.1426 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.13908\n",
      "Epoch 305/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1430 - accuracy: 0.9556 - val_loss: 0.1446 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.13908\n",
      "Epoch 306/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1427 - accuracy: 0.9556 - val_loss: 0.1410 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.13908\n",
      "Epoch 307/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1424 - accuracy: 0.9525 - val_loss: 0.1415 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.13908\n",
      "Epoch 308/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1413 - accuracy: 0.9525 - val_loss: 0.1468 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.13908\n",
      "Epoch 309/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1430 - accuracy: 0.9556 - val_loss: 0.1440 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.13908\n",
      "Epoch 310/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1400 - accuracy: 0.9556 - val_loss: 0.1367 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.13908 to 0.13674, saving model to ./model_100\\310-0.1367_train.hdf5\n",
      "Epoch 311/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1432 - accuracy: 0.9525 - val_loss: 0.1364 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.13674 to 0.13640, saving model to ./model_100\\311-0.1364_train.hdf5\n",
      "Epoch 312/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1419 - accuracy: 0.9510 - val_loss: 0.1436 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.13640\n",
      "Epoch 313/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1420 - accuracy: 0.9617 - val_loss: 0.1532 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.13640\n",
      "Epoch 314/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1426 - accuracy: 0.9648 - val_loss: 0.1382 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.13640\n",
      "Epoch 315/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1421 - accuracy: 0.9541 - val_loss: 0.1352 - val_accuracy: 0.9503\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.13640 to 0.13524, saving model to ./model_100\\315-0.1352_train.hdf5\n",
      "Epoch 316/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1426 - accuracy: 0.9510 - val_loss: 0.1373 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.13524\n",
      "Epoch 317/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1380 - accuracy: 0.9525 - val_loss: 0.1445 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.13524\n",
      "Epoch 318/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1390 - accuracy: 0.9602 - val_loss: 0.1482 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.13524\n",
      "Epoch 319/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1399 - accuracy: 0.9617 - val_loss: 0.1394 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.13524\n",
      "Epoch 320/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1386 - accuracy: 0.9525 - val_loss: 0.1350 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.13524 to 0.13501, saving model to ./model_100\\320-0.1350_train.hdf5\n",
      "Epoch 321/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1394 - accuracy: 0.9525 - val_loss: 0.1351 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.13501\n",
      "Epoch 322/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1376 - accuracy: 0.9525 - val_loss: 0.1380 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.13501\n",
      "Epoch 323/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1360 - accuracy: 0.9541 - val_loss: 0.1421 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.13501\n",
      "Epoch 324/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1371 - accuracy: 0.9632 - val_loss: 0.1374 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.13501\n",
      "Epoch 325/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1359 - accuracy: 0.9556 - val_loss: 0.1333 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.13501 to 0.13326, saving model to ./model_100\\325-0.1333_train.hdf5\n",
      "Epoch 326/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1366 - accuracy: 0.9525 - val_loss: 0.1330 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.13326 to 0.13301, saving model to ./model_100\\326-0.1330_train.hdf5\n",
      "Epoch 327/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1361 - accuracy: 0.9525 - val_loss: 0.1332 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.13301\n",
      "Epoch 328/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1350 - accuracy: 0.9525 - val_loss: 0.1376 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.13301\n",
      "Epoch 329/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1362 - accuracy: 0.9602 - val_loss: 0.1399 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.13301\n",
      "Epoch 330/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1341 - accuracy: 0.9648 - val_loss: 0.1320 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.13301 to 0.13201, saving model to ./model_100\\330-0.1320_train.hdf5\n",
      "Epoch 331/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1360 - accuracy: 0.9525 - val_loss: 0.1311 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.13201 to 0.13113, saving model to ./model_100\\331-0.1311_train.hdf5\n",
      "Epoch 332/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1349 - accuracy: 0.9525 - val_loss: 0.1372 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.13113\n",
      "Epoch 333/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1339 - accuracy: 0.9617 - val_loss: 0.1429 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.13113\n",
      "Epoch 334/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1349 - accuracy: 0.9663 - val_loss: 0.1352 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.13113\n",
      "Epoch 335/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1322 - accuracy: 0.9602 - val_loss: 0.1315 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.13113\n",
      "Epoch 336/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1326 - accuracy: 0.9525 - val_loss: 0.1314 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.13113\n",
      "Epoch 337/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1321 - accuracy: 0.9525 - val_loss: 0.1368 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.13113\n",
      "Epoch 338/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1321 - accuracy: 0.9648 - val_loss: 0.1429 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.13113\n",
      "Epoch 339/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1336 - accuracy: 0.9663 - val_loss: 0.1345 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.13113\n",
      "Epoch 340/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1311 - accuracy: 0.9587 - val_loss: 0.1305 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.13113 to 0.13046, saving model to ./model_100\\340-0.1305_train.hdf5\n",
      "Epoch 341/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1313 - accuracy: 0.9525 - val_loss: 0.1317 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.13046\n",
      "Epoch 342/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1302 - accuracy: 0.9602 - val_loss: 0.1372 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.13046\n",
      "Epoch 343/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1308 - accuracy: 0.9648 - val_loss: 0.1338 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00343: val_loss did not improve from 0.13046\n",
      "Epoch 344/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1290 - accuracy: 0.9632 - val_loss: 0.1287 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.13046 to 0.12874, saving model to ./model_100\\344-0.1287_train.hdf5\n",
      "Epoch 345/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1298 - accuracy: 0.9556 - val_loss: 0.1283 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.12874 to 0.12833, saving model to ./model_100\\345-0.1283_train.hdf5\n",
      "Epoch 346/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1305 - accuracy: 0.9556 - val_loss: 0.1306 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.12833\n",
      "Epoch 347/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1285 - accuracy: 0.9632 - val_loss: 0.1407 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.12833\n",
      "Epoch 348/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1319 - accuracy: 0.9740 - val_loss: 0.1370 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.12833\n",
      "Epoch 349/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1281 - accuracy: 0.9678 - val_loss: 0.1264 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.12833 to 0.12641, saving model to ./model_100\\349-0.1264_train.hdf5\n",
      "Epoch 350/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1303 - accuracy: 0.9556 - val_loss: 0.1262 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.12641 to 0.12618, saving model to ./model_100\\350-0.1262_train.hdf5\n",
      "Epoch 351/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1279 - accuracy: 0.9541 - val_loss: 0.1385 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.12618\n",
      "Epoch 352/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1340 - accuracy: 0.9678 - val_loss: 0.1461 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.12618\n",
      "Epoch 353/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1336 - accuracy: 0.9709 - val_loss: 0.1269 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.12618\n",
      "Epoch 354/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1308 - accuracy: 0.9525 - val_loss: 0.1260 - val_accuracy: 0.9534\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.12618 to 0.12603, saving model to ./model_100\\354-0.1260_train.hdf5\n",
      "Epoch 355/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1326 - accuracy: 0.9525 - val_loss: 0.1270 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.12603\n",
      "Epoch 356/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1281 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.12603\n",
      "Epoch 357/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1271 - accuracy: 0.9678 - val_loss: 0.1287 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.12603\n",
      "Epoch 358/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1243 - accuracy: 0.9632 - val_loss: 0.1249 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.12603 to 0.12488, saving model to ./model_100\\358-0.1249_train.hdf5\n",
      "Epoch 359/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1266 - accuracy: 0.9556 - val_loss: 0.1245 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.12488 to 0.12449, saving model to ./model_100\\359-0.1245_train.hdf5\n",
      "Epoch 360/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1263 - accuracy: 0.9571 - val_loss: 0.1283 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.12449\n",
      "Epoch 361/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1244 - accuracy: 0.9648 - val_loss: 0.1346 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.12449\n",
      "Epoch 362/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1259 - accuracy: 0.9724 - val_loss: 0.1285 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.12449\n",
      "Epoch 363/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1243 - accuracy: 0.9663 - val_loss: 0.1247 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.12449\n",
      "Epoch 364/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1240 - accuracy: 0.9602 - val_loss: 0.1293 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.12449\n",
      "Epoch 365/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1229 - accuracy: 0.9694 - val_loss: 0.1424 - val_accuracy: 0.9565\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.12449\n",
      "Epoch 366/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1290 - accuracy: 0.9740 - val_loss: 0.1301 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.12449\n",
      "Epoch 367/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1226 - accuracy: 0.9678 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.12449 to 0.12320, saving model to ./model_100\\367-0.1232_train.hdf5\n",
      "Epoch 368/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1256 - accuracy: 0.9556 - val_loss: 0.1236 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.12320\n",
      "Epoch 369/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1248 - accuracy: 0.9541 - val_loss: 0.1298 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.12320\n",
      "Epoch 370/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1259 - accuracy: 0.9648 - val_loss: 0.1361 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.12320\n",
      "Epoch 371/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1265 - accuracy: 0.9678 - val_loss: 0.1257 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.12320\n",
      "Epoch 372/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1238 - accuracy: 0.9602 - val_loss: 0.1250 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.12320\n",
      "Epoch 373/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1214 - accuracy: 0.9632 - val_loss: 0.1314 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.12320\n",
      "Epoch 374/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1217 - accuracy: 0.9709 - val_loss: 0.1278 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.12320\n",
      "Epoch 375/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1201 - accuracy: 0.9694 - val_loss: 0.1234 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.12320\n",
      "Epoch 376/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1207 - accuracy: 0.9617 - val_loss: 0.1226 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.12320 to 0.12257, saving model to ./model_100\\376-0.1226_train.hdf5\n",
      "Epoch 377/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1245 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.12257\n",
      "Epoch 378/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1212 - accuracy: 0.9663 - val_loss: 0.1276 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.12257\n",
      "Epoch 379/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1211 - accuracy: 0.9678 - val_loss: 0.1253 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.12257\n",
      "Epoch 380/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1194 - accuracy: 0.9663 - val_loss: 0.1233 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.12257\n",
      "Epoch 381/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1186 - accuracy: 0.9663 - val_loss: 0.1244 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.12257\n",
      "Epoch 382/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1196 - accuracy: 0.9678 - val_loss: 0.1266 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.12257\n",
      "Epoch 383/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1191 - accuracy: 0.9663 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00383: val_loss improved from 0.12257 to 0.12244, saving model to ./model_100\\383-0.1224_train.hdf5\n",
      "Epoch 384/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1202 - accuracy: 0.9617 - val_loss: 0.1212 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.12244 to 0.12116, saving model to ./model_100\\384-0.1212_train.hdf5\n",
      "Epoch 385/1500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.12116\n",
      "Epoch 386/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1185 - accuracy: 0.9709 - val_loss: 0.1366 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.12116\n",
      "Epoch 387/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1218 - accuracy: 0.9724 - val_loss: 0.1275 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.12116\n",
      "Epoch 388/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1184 - accuracy: 0.9694 - val_loss: 0.1214 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.12116\n",
      "Epoch 389/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1211 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.12116 to 0.12108, saving model to ./model_100\\389-0.1211_train.hdf5\n",
      "Epoch 390/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.12108\n",
      "Epoch 391/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1168 - accuracy: 0.9678 - val_loss: 0.1242 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.12108\n",
      "Epoch 392/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1163 - accuracy: 0.9663 - val_loss: 0.1217 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.12108\n",
      "Epoch 393/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1160 - accuracy: 0.9678 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.12108\n",
      "Epoch 394/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1157 - accuracy: 0.9663 - val_loss: 0.1241 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.12108\n",
      "Epoch 395/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1154 - accuracy: 0.9678 - val_loss: 0.1227 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.12108\n",
      "Epoch 396/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1150 - accuracy: 0.9678 - val_loss: 0.1232 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.12108\n",
      "Epoch 397/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1149 - accuracy: 0.9663 - val_loss: 0.1266 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.12108\n",
      "Epoch 398/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1169 - accuracy: 0.9709 - val_loss: 0.1242 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.12108\n",
      "Epoch 399/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1156 - accuracy: 0.9678 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.12108 to 0.11939, saving model to ./model_100\\399-0.1194_train.hdf5\n",
      "Epoch 400/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1158 - accuracy: 0.9632 - val_loss: 0.1197 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.11939\n",
      "Epoch 401/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1140 - accuracy: 0.9678 - val_loss: 0.1278 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.11939\n",
      "Epoch 402/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1163 - accuracy: 0.9724 - val_loss: 0.1244 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.11939\n",
      "Epoch 403/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1145 - accuracy: 0.9724 - val_loss: 0.1186 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.11939 to 0.11860, saving model to ./model_100\\403-0.1186_train.hdf5\n",
      "Epoch 404/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1145 - accuracy: 0.9648 - val_loss: 0.1182 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00404: val_loss improved from 0.11860 to 0.11820, saving model to ./model_100\\404-0.1182_train.hdf5\n",
      "Epoch 405/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.11820\n",
      "Epoch 406/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1145 - accuracy: 0.9648 - val_loss: 0.1222 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.11820\n",
      "Epoch 407/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1126 - accuracy: 0.9709 - val_loss: 0.1194 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.11820\n",
      "Epoch 408/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1134 - accuracy: 0.9663 - val_loss: 0.1201 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.11820\n",
      "Epoch 409/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1124 - accuracy: 0.9694 - val_loss: 0.1288 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.11820\n",
      "Epoch 410/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1156 - accuracy: 0.9724 - val_loss: 0.1302 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.11820\n",
      "Epoch 411/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1141 - accuracy: 0.9724 - val_loss: 0.1176 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.11820 to 0.11759, saving model to ./model_100\\411-0.1176_train.hdf5\n",
      "Epoch 412/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1137 - accuracy: 0.9648 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.11759 to 0.11705, saving model to ./model_100\\412-0.1170_train.hdf5\n",
      "Epoch 413/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1161 - accuracy: 0.9602 - val_loss: 0.1199 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.11705\n",
      "Epoch 414/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1127 - accuracy: 0.9709 - val_loss: 0.1319 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.11705\n",
      "Epoch 415/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1178 - accuracy: 0.9724 - val_loss: 0.1224 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.11705\n",
      "Epoch 416/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1117 - accuracy: 0.9724 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.11705\n",
      "Epoch 417/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1111 - accuracy: 0.9663 - val_loss: 0.1197 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.11705\n",
      "Epoch 418/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1127 - accuracy: 0.9678 - val_loss: 0.1236 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.11705\n",
      "Epoch 419/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1134 - accuracy: 0.9709 - val_loss: 0.1212 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.11705\n",
      "Epoch 420/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1111 - accuracy: 0.9709 - val_loss: 0.1187 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.11705\n",
      "Epoch 421/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.11705 to 0.11698, saving model to ./model_100\\421-0.1170_train.hdf5\n",
      "Epoch 422/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1097 - accuracy: 0.9678 - val_loss: 0.1164 - val_accuracy: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00422: val_loss improved from 0.11698 to 0.11645, saving model to ./model_100\\422-0.1164_train.hdf5\n",
      "Epoch 423/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1113 - accuracy: 0.9663 - val_loss: 0.1184 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.11645\n",
      "Epoch 424/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1114 - accuracy: 0.9694 - val_loss: 0.1208 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.11645\n",
      "Epoch 425/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1106 - accuracy: 0.9724 - val_loss: 0.1186 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.11645\n",
      "Epoch 426/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1096 - accuracy: 0.9694 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.11645\n",
      "Epoch 427/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1094 - accuracy: 0.9694 - val_loss: 0.1184 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.11645\n",
      "Epoch 428/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1083 - accuracy: 0.9694 - val_loss: 0.1231 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.11645\n",
      "Epoch 429/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1104 - accuracy: 0.9740 - val_loss: 0.1223 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.11645\n",
      "Epoch 430/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1117 - accuracy: 0.9678 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.11645\n",
      "Epoch 431/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1098 - accuracy: 0.9663 - val_loss: 0.1171 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.11645\n",
      "Epoch 432/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1084 - accuracy: 0.9678 - val_loss: 0.1167 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.11645\n",
      "Epoch 433/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1079 - accuracy: 0.9694 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.11645\n",
      "Epoch 434/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1075 - accuracy: 0.9724 - val_loss: 0.1198 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.11645\n",
      "Epoch 435/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1074 - accuracy: 0.9724 - val_loss: 0.1153 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.11645 to 0.11535, saving model to ./model_100\\435-0.1153_train.hdf5\n",
      "Epoch 436/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1065 - accuracy: 0.9678 - val_loss: 0.1135 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.11535 to 0.11350, saving model to ./model_100\\436-0.1135_train.hdf5\n",
      "Epoch 437/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1090 - accuracy: 0.9663 - val_loss: 0.1137 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.11350\n",
      "Epoch 438/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1076 - accuracy: 0.9694 - val_loss: 0.1166 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.11350\n",
      "Epoch 439/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1078 - accuracy: 0.9724 - val_loss: 0.1172 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.11350\n",
      "Epoch 440/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1067 - accuracy: 0.9709 - val_loss: 0.1139 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.11350\n",
      "Epoch 441/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1064 - accuracy: 0.9678 - val_loss: 0.1150 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.11350\n",
      "Epoch 442/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1058 - accuracy: 0.9694 - val_loss: 0.1175 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.11350\n",
      "Epoch 443/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1057 - accuracy: 0.9724 - val_loss: 0.1175 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.11350\n",
      "Epoch 444/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1056 - accuracy: 0.9724 - val_loss: 0.1151 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.11350\n",
      "Epoch 445/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1049 - accuracy: 0.9694 - val_loss: 0.1128 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00445: val_loss improved from 0.11350 to 0.11279, saving model to ./model_100\\445-0.1128_train.hdf5\n",
      "Epoch 446/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1077 - accuracy: 0.9648 - val_loss: 0.1127 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.11279 to 0.11272, saving model to ./model_100\\446-0.1127_train.hdf5\n",
      "Epoch 447/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1078 - accuracy: 0.9648 - val_loss: 0.1154 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.11272\n",
      "Epoch 448/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1050 - accuracy: 0.9724 - val_loss: 0.1200 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.11272\n",
      "Epoch 449/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1061 - accuracy: 0.9724 - val_loss: 0.1164 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.11272\n",
      "Epoch 450/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1041 - accuracy: 0.9694 - val_loss: 0.1132 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.11272\n",
      "Epoch 451/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.1133 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.11272\n",
      "Epoch 452/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1047 - accuracy: 0.9678 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.11272\n",
      "Epoch 453/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1039 - accuracy: 0.9724 - val_loss: 0.1188 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.11272\n",
      "Epoch 454/1500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1043 - accuracy: 0.9724 - val_loss: 0.1152 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.11272\n",
      "Epoch 455/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1034 - accuracy: 0.9724 - val_loss: 0.1125 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.11272 to 0.11253, saving model to ./model_100\\455-0.1125_train.hdf5\n",
      "Epoch 456/1500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1044 - accuracy: 0.9724 - val_loss: 0.1127 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.11253\n",
      "Epoch 457/1500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1048 - accuracy: 0.9709 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.11253\n",
      "Epoch 458/1500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1035 - accuracy: 0.9709 - val_loss: 0.1111 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.11253 to 0.11109, saving model to ./model_100\\458-0.1111_train.hdf5\n",
      "Epoch 459/1500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1046 - accuracy: 0.9663 - val_loss: 0.1117 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.11109\n",
      "Epoch 460/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1039 - accuracy: 0.9724 - val_loss: 0.1191 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.11109\n",
      "Epoch 461/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1045 - accuracy: 0.9709 - val_loss: 0.1134 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.11109\n",
      "Epoch 462/1500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1033 - accuracy: 0.9709 - val_loss: 0.1106 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.11109 to 0.11064, saving model to ./model_100\\462-0.1106_train.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1035 - accuracy: 0.9678 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.11064\n",
      "Epoch 464/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1018 - accuracy: 0.9709 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.11064\n",
      "Epoch 465/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1016 - accuracy: 0.9724 - val_loss: 0.1145 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.11064\n",
      "Epoch 466/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1027 - accuracy: 0.9724 - val_loss: 0.1139 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.11064\n",
      "Epoch 467/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1024 - accuracy: 0.9724 - val_loss: 0.1130 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.11064\n",
      "Epoch 468/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1017 - accuracy: 0.9724 - val_loss: 0.1115 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.11064\n",
      "Epoch 469/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1020 - accuracy: 0.9709 - val_loss: 0.1122 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.11064\n",
      "Epoch 470/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1012 - accuracy: 0.9724 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.11064\n",
      "Epoch 471/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1012 - accuracy: 0.9724 - val_loss: 0.1138 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.11064\n",
      "Epoch 472/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1011 - accuracy: 0.9724 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.11064\n",
      "Epoch 473/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1018 - accuracy: 0.9678 - val_loss: 0.1105 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.11064 to 0.11047, saving model to ./model_100\\473-0.1105_train.hdf5\n",
      "Epoch 474/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1011 - accuracy: 0.9694 - val_loss: 0.1121 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.11047\n",
      "Epoch 475/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1002 - accuracy: 0.9724 - val_loss: 0.1141 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.11047\n",
      "Epoch 476/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1003 - accuracy: 0.9724 - val_loss: 0.1144 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.11047\n",
      "Epoch 477/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1002 - accuracy: 0.9724 - val_loss: 0.1100 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.11047 to 0.11003, saving model to ./model_100\\477-0.1100_train.hdf5\n",
      "Epoch 478/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1000 - accuracy: 0.9724 - val_loss: 0.1089 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00478: val_loss improved from 0.11003 to 0.10886, saving model to ./model_100\\478-0.1089_train.hdf5\n",
      "Epoch 479/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1024 - accuracy: 0.9678 - val_loss: 0.1091 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.10886\n",
      "Epoch 480/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1006 - accuracy: 0.9709 - val_loss: 0.1147 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.10886\n",
      "Epoch 481/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1014 - accuracy: 0.9740 - val_loss: 0.1147 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.10886\n",
      "Epoch 482/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0997 - accuracy: 0.9724 - val_loss: 0.1085 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00482: val_loss improved from 0.10886 to 0.10848, saving model to ./model_100\\482-0.1085_train.hdf5\n",
      "Epoch 483/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1005 - accuracy: 0.9678 - val_loss: 0.1081 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.10848 to 0.10811, saving model to ./model_100\\483-0.1081_train.hdf5\n",
      "Epoch 484/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0999 - accuracy: 0.9678 - val_loss: 0.1121 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.10811\n",
      "Epoch 485/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0993 - accuracy: 0.9724 - val_loss: 0.1111 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.10811\n",
      "Epoch 486/1500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0988 - accuracy: 0.9724 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.10811 to 0.10800, saving model to ./model_100\\486-0.1080_train.hdf5\n",
      "Epoch 487/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0988 - accuracy: 0.9709 - val_loss: 0.1092 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.10800\n",
      "Epoch 488/1500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0992 - accuracy: 0.9709 - val_loss: 0.1114 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.10800\n",
      "Epoch 489/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0979 - accuracy: 0.9709 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.10800\n",
      "Epoch 490/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0990 - accuracy: 0.9709 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.10800 to 0.10798, saving model to ./model_100\\490-0.1080_train.hdf5\n",
      "Epoch 491/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1016 - accuracy: 0.9678 - val_loss: 0.1086 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.10798\n",
      "Epoch 492/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0989 - accuracy: 0.9709 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.10798\n",
      "Epoch 493/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0998 - accuracy: 0.9740 - val_loss: 0.1094 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.10798\n",
      "Epoch 494/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0972 - accuracy: 0.9724 - val_loss: 0.1066 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.10798 to 0.10657, saving model to ./model_100\\494-0.1066_train.hdf5\n",
      "Epoch 495/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0999 - accuracy: 0.9678 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00495: val_loss improved from 0.10657 to 0.10642, saving model to ./model_100\\495-0.1064_train.hdf5\n",
      "Epoch 496/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.1105 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.10642\n",
      "Epoch 497/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0983 - accuracy: 0.9724 - val_loss: 0.1124 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.10642\n",
      "Epoch 498/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0985 - accuracy: 0.9724 - val_loss: 0.1078 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.10642\n",
      "Epoch 499/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0965 - accuracy: 0.9709 - val_loss: 0.1074 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.10642\n",
      "Epoch 500/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0961 - accuracy: 0.9724 - val_loss: 0.1064 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.10642\n",
      "Epoch 501/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.1069 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.10642\n",
      "Epoch 502/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0964 - accuracy: 0.9724 - val_loss: 0.1126 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.10642\n",
      "Epoch 503/1500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0970 - accuracy: 0.9724 - val_loss: 0.1156 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.10642\n",
      "Epoch 504/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0973 - accuracy: 0.9755 - val_loss: 0.1103 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.10642\n",
      "Epoch 505/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0963 - accuracy: 0.9694 - val_loss: 0.1103 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.10642\n",
      "Epoch 506/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0989 - accuracy: 0.9678 - val_loss: 0.1118 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.10642\n",
      "Epoch 507/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0991 - accuracy: 0.9678 - val_loss: 0.1097 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.10642\n",
      "Epoch 508/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0957 - accuracy: 0.9709 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.10642\n",
      "Epoch 509/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0950 - accuracy: 0.9709 - val_loss: 0.1071 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.10642\n",
      "Epoch 510/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0960 - accuracy: 0.9709 - val_loss: 0.1066 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.10642\n",
      "Epoch 511/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0979 - accuracy: 0.9724 - val_loss: 0.1071 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.10642\n",
      "Epoch 512/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0977 - accuracy: 0.9709 - val_loss: 0.1081 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.10642\n",
      "Epoch 513/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0962 - accuracy: 0.9709 - val_loss: 0.1095 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.10642\n",
      "Epoch 514/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0942 - accuracy: 0.9724 - val_loss: 0.1118 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.10642\n",
      "Epoch 515/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.1082 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.10642\n",
      "Epoch 516/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0955 - accuracy: 0.9709 - val_loss: 0.1066 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.10642\n",
      "Epoch 517/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0967 - accuracy: 0.9678 - val_loss: 0.1088 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.10642\n",
      "Epoch 518/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0957 - accuracy: 0.9709 - val_loss: 0.1120 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.10642\n",
      "Epoch 519/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.1075 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.10642\n",
      "Epoch 520/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.1057 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.10642 to 0.10569, saving model to ./model_100\\520-0.1057_train.hdf5\n",
      "Epoch 521/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0937 - accuracy: 0.9709 - val_loss: 0.1076 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.10569\n",
      "Epoch 522/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0940 - accuracy: 0.9724 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.10569\n",
      "Epoch 523/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0948 - accuracy: 0.9740 - val_loss: 0.1072 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.10569\n",
      "Epoch 524/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0923 - accuracy: 0.9709 - val_loss: 0.1050 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00524: val_loss improved from 0.10569 to 0.10504, saving model to ./model_100\\524-0.1050_train.hdf5\n",
      "Epoch 525/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0933 - accuracy: 0.9724 - val_loss: 0.1055 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.10504\n",
      "Epoch 526/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0929 - accuracy: 0.9724 - val_loss: 0.1084 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.10504\n",
      "Epoch 527/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0925 - accuracy: 0.9724 - val_loss: 0.1069 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.10504\n",
      "Epoch 528/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0921 - accuracy: 0.9709 - val_loss: 0.1056 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.10504\n",
      "Epoch 529/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0924 - accuracy: 0.9709 - val_loss: 0.1073 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.10504\n",
      "Epoch 530/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0924 - accuracy: 0.9724 - val_loss: 0.1063 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.10504\n",
      "Epoch 531/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0919 - accuracy: 0.9709 - val_loss: 0.1040 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00531: val_loss improved from 0.10504 to 0.10396, saving model to ./model_100\\531-0.1040_train.hdf5\n",
      "Epoch 532/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0925 - accuracy: 0.9724 - val_loss: 0.1045 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.10396\n",
      "Epoch 533/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0917 - accuracy: 0.9709 - val_loss: 0.1060 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.10396\n",
      "Epoch 534/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0919 - accuracy: 0.9755 - val_loss: 0.1056 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.10396\n",
      "Epoch 535/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0916 - accuracy: 0.9709 - val_loss: 0.1041 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.10396\n",
      "Epoch 536/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0910 - accuracy: 0.9709 - val_loss: 0.1060 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.10396\n",
      "Epoch 537/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0913 - accuracy: 0.9786 - val_loss: 0.1054 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.10396\n",
      "Epoch 538/1500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0915 - accuracy: 0.9740 - val_loss: 0.1034 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00538: val_loss improved from 0.10396 to 0.10344, saving model to ./model_100\\538-0.1034_train.hdf5\n",
      "Epoch 539/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0918 - accuracy: 0.9709 - val_loss: 0.1037 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.10344\n",
      "Epoch 540/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0918 - accuracy: 0.9709 - val_loss: 0.1067 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.10344\n",
      "Epoch 541/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0921 - accuracy: 0.9770 - val_loss: 0.1124 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.10344\n",
      "Epoch 542/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0921 - accuracy: 0.9755 - val_loss: 0.1036 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.10344\n",
      "Epoch 543/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0905 - accuracy: 0.9724 - val_loss: 0.1031 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.10344 to 0.10310, saving model to ./model_100\\543-0.1031_train.hdf5\n",
      "Epoch 544/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0937 - accuracy: 0.9678 - val_loss: 0.1041 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.10310\n",
      "Epoch 545/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0905 - accuracy: 0.9740 - val_loss: 0.1119 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.10310\n",
      "Epoch 546/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0915 - accuracy: 0.9755 - val_loss: 0.1056 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.10310\n",
      "Epoch 547/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0888 - accuracy: 0.9709 - val_loss: 0.1026 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.10310 to 0.10257, saving model to ./model_100\\547-0.1026_train.hdf5\n",
      "Epoch 548/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0936 - accuracy: 0.9678 - val_loss: 0.1024 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00548: val_loss improved from 0.10257 to 0.10241, saving model to ./model_100\\548-0.1024_train.hdf5\n",
      "Epoch 549/1500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0909 - accuracy: 0.9709 - val_loss: 0.1107 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.10241\n",
      "Epoch 550/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0933 - accuracy: 0.9755 - val_loss: 0.1118 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.10241\n",
      "Epoch 551/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0911 - accuracy: 0.9724 - val_loss: 0.1021 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00551: val_loss improved from 0.10241 to 0.10212, saving model to ./model_100\\551-0.1021_train.hdf5\n",
      "Epoch 552/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0916 - accuracy: 0.9694 - val_loss: 0.1026 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.10212\n",
      "Epoch 553/1500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0924 - accuracy: 0.9694 - val_loss: 0.1053 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.10212\n",
      "Epoch 554/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0894 - accuracy: 0.9755 - val_loss: 0.1093 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.10212\n",
      "Epoch 555/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0898 - accuracy: 0.9755 - val_loss: 0.1030 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.10212\n",
      "Epoch 556/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0896 - accuracy: 0.9724 - val_loss: 0.1017 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00556: val_loss improved from 0.10212 to 0.10173, saving model to ./model_100\\556-0.1017_train.hdf5\n",
      "Epoch 557/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0896 - accuracy: 0.9709 - val_loss: 0.1046 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.10173\n",
      "Epoch 558/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0881 - accuracy: 0.9755 - val_loss: 0.1089 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.10173\n",
      "Epoch 559/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0892 - accuracy: 0.9786 - val_loss: 0.1028 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.10173\n",
      "Epoch 560/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0886 - accuracy: 0.9724 - val_loss: 0.1015 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00560: val_loss improved from 0.10173 to 0.10145, saving model to ./model_100\\560-0.1015_train.hdf5\n",
      "Epoch 561/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0899 - accuracy: 0.9709 - val_loss: 0.1031 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.10145\n",
      "Epoch 562/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.1082 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.10145\n",
      "Epoch 563/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0883 - accuracy: 0.9755 - val_loss: 0.1055 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.10145\n",
      "Epoch 564/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0871 - accuracy: 0.9740 - val_loss: 0.1025 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.10145\n",
      "Epoch 565/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0875 - accuracy: 0.9724 - val_loss: 0.1019 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.10145\n",
      "Epoch 566/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0878 - accuracy: 0.9709 - val_loss: 0.1028 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.10145\n",
      "Epoch 567/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0864 - accuracy: 0.9740 - val_loss: 0.1061 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.10145\n",
      "Epoch 568/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0868 - accuracy: 0.9786 - val_loss: 0.1058 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.10145\n",
      "Epoch 569/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0866 - accuracy: 0.9770 - val_loss: 0.1023 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.10145\n",
      "Epoch 570/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0876 - accuracy: 0.9724 - val_loss: 0.1017 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.10145\n",
      "Epoch 571/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0880 - accuracy: 0.9724 - val_loss: 0.1035 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.10145\n",
      "Epoch 572/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0860 - accuracy: 0.9755 - val_loss: 0.1061 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.10145\n",
      "Epoch 573/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0866 - accuracy: 0.9770 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.10145\n",
      "Epoch 574/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0870 - accuracy: 0.9770 - val_loss: 0.1024 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.10145\n",
      "Epoch 575/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0858 - accuracy: 0.9755 - val_loss: 0.1000 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.10145 to 0.09999, saving model to ./model_100\\575-0.1000_train.hdf5\n",
      "Epoch 576/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0862 - accuracy: 0.9709 - val_loss: 0.1030 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.09999\n",
      "Epoch 577/1500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0863 - accuracy: 0.9740 - val_loss: 0.1090 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.09999\n",
      "Epoch 578/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0872 - accuracy: 0.9755 - val_loss: 0.1005 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.09999\n",
      "Epoch 579/1500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0877 - accuracy: 0.9724 - val_loss: 0.0998 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00579: val_loss improved from 0.09999 to 0.09978, saving model to ./model_100\\579-0.0998_train.hdf5\n",
      "Epoch 580/1500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0878 - accuracy: 0.9694 - val_loss: 0.1026 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.09978\n",
      "Epoch 581/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0842 - accuracy: 0.9755 - val_loss: 0.1086 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.09978\n",
      "Epoch 582/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0870 - accuracy: 0.9755 - val_loss: 0.1052 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.09978\n",
      "Epoch 583/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0840 - accuracy: 0.9770 - val_loss: 0.1003 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.09978\n",
      "Epoch 584/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0904 - accuracy: 0.9678 - val_loss: 0.1002 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.09978\n",
      "Epoch 585/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0842 - accuracy: 0.9740 - val_loss: 0.1171 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.09978\n",
      "Epoch 586/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0922 - accuracy: 0.9755 - val_loss: 0.1140 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.09978\n",
      "Epoch 587/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0882 - accuracy: 0.9740 - val_loss: 0.1004 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.09978\n",
      "Epoch 588/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0909 - accuracy: 0.9663 - val_loss: 0.1008 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.09978\n",
      "Epoch 589/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0887 - accuracy: 0.9678 - val_loss: 0.1074 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.09978\n",
      "Epoch 590/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0859 - accuracy: 0.9755 - val_loss: 0.1158 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.09978\n",
      "Epoch 591/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0880 - accuracy: 0.9755 - val_loss: 0.1000 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.09978\n",
      "Epoch 592/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.1017 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.09978\n",
      "Epoch 593/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0916 - accuracy: 0.9663 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.09978\n",
      "Epoch 594/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0835 - accuracy: 0.9724 - val_loss: 0.1091 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.09978\n",
      "Epoch 595/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0867 - accuracy: 0.9755 - val_loss: 0.1099 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.09978\n",
      "Epoch 596/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0850 - accuracy: 0.9755 - val_loss: 0.0993 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00596: val_loss improved from 0.09978 to 0.09928, saving model to ./model_100\\596-0.0993_train.hdf5\n",
      "Epoch 597/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0852 - accuracy: 0.9709 - val_loss: 0.0989 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00597: val_loss improved from 0.09928 to 0.09894, saving model to ./model_100\\597-0.0989_train.hdf5\n",
      "Epoch 598/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0834 - accuracy: 0.9709 - val_loss: 0.1052 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.09894\n",
      "Epoch 599/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.1099 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.09894\n",
      "Epoch 600/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0849 - accuracy: 0.9755 - val_loss: 0.0990 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.09894\n",
      "Epoch 601/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.0988 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00601: val_loss improved from 0.09894 to 0.09879, saving model to ./model_100\\601-0.0988_train.hdf5\n",
      "Epoch 602/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0858 - accuracy: 0.9694 - val_loss: 0.1006 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.09879\n",
      "Epoch 603/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0814 - accuracy: 0.9786 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.09879\n",
      "Epoch 604/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0896 - accuracy: 0.9755 - val_loss: 0.1045 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.09879\n",
      "Epoch 605/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0831 - accuracy: 0.9755 - val_loss: 0.0981 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00605: val_loss improved from 0.09879 to 0.09805, saving model to ./model_100\\605-0.0981_train.hdf5\n",
      "Epoch 606/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0823 - accuracy: 0.9724 - val_loss: 0.0986 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.09805\n",
      "Epoch 607/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0809 - accuracy: 0.9770 - val_loss: 0.1036 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.09805\n",
      "Epoch 608/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0814 - accuracy: 0.9740 - val_loss: 0.1025 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.09805\n",
      "Epoch 609/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0817 - accuracy: 0.9740 - val_loss: 0.1001 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.09805\n",
      "Epoch 610/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0810 - accuracy: 0.9770 - val_loss: 0.1002 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.09805\n",
      "Epoch 611/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0808 - accuracy: 0.9770 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.09805\n",
      "Epoch 612/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0803 - accuracy: 0.9770 - val_loss: 0.0998 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.09805\n",
      "Epoch 613/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0798 - accuracy: 0.9770 - val_loss: 0.0991 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.09805\n",
      "Epoch 614/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0798 - accuracy: 0.9770 - val_loss: 0.0998 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.09805\n",
      "Epoch 615/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0793 - accuracy: 0.9770 - val_loss: 0.1028 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.09805\n",
      "Epoch 616/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0801 - accuracy: 0.9755 - val_loss: 0.1031 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.09805\n",
      "Epoch 617/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0801 - accuracy: 0.9755 - val_loss: 0.1012 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.09805\n",
      "Epoch 618/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0810 - accuracy: 0.9770 - val_loss: 0.1022 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.09805\n",
      "Epoch 619/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0812 - accuracy: 0.9755 - val_loss: 0.1021 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.09805\n",
      "Epoch 620/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0793 - accuracy: 0.9755 - val_loss: 0.0979 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00620: val_loss improved from 0.09805 to 0.09786, saving model to ./model_100\\620-0.0979_train.hdf5\n",
      "Epoch 621/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0809 - accuracy: 0.9740 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.09786\n",
      "Epoch 622/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0802 - accuracy: 0.9770 - val_loss: 0.1068 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.09786\n",
      "Epoch 623/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0821 - accuracy: 0.9740 - val_loss: 0.1012 - val_accuracy: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00623: val_loss did not improve from 0.09786\n",
      "Epoch 624/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0794 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.09786\n",
      "Epoch 625/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0785 - accuracy: 0.9770 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.09786\n",
      "Epoch 626/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0781 - accuracy: 0.9755 - val_loss: 0.1006 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.09786\n",
      "Epoch 627/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0782 - accuracy: 0.9755 - val_loss: 0.1018 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.09786\n",
      "Epoch 628/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0783 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.09786\n",
      "Epoch 629/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0774 - accuracy: 0.9770 - val_loss: 0.0977 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00629: val_loss improved from 0.09786 to 0.09766, saving model to ./model_100\\629-0.0977_train.hdf5\n",
      "Epoch 630/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 0.0984 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.09766\n",
      "Epoch 631/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 0.1026 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.09766\n",
      "Epoch 632/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0786 - accuracy: 0.9740 - val_loss: 0.0985 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.09766\n",
      "Epoch 633/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0780 - accuracy: 0.9786 - val_loss: 0.0967 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00633: val_loss improved from 0.09766 to 0.09669, saving model to ./model_100\\633-0.0967_train.hdf5\n",
      "Epoch 634/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0775 - accuracy: 0.9755 - val_loss: 0.0996 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.09669\n",
      "Epoch 635/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0765 - accuracy: 0.9770 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.09669\n",
      "Epoch 636/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0802 - accuracy: 0.9724 - val_loss: 0.1031 - val_accuracy: 0.9596\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.09669\n",
      "Epoch 637/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0769 - accuracy: 0.9755 - val_loss: 0.0968 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.09669\n",
      "Epoch 638/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0791 - accuracy: 0.9694 - val_loss: 0.0983 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.09669\n",
      "Epoch 639/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0835 - accuracy: 0.9678 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.09669\n",
      "Epoch 640/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0749 - accuracy: 0.9770 - val_loss: 0.1199 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.09669\n",
      "Epoch 641/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0893 - accuracy: 0.9724 - val_loss: 0.1029 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.09669\n",
      "Epoch 642/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0767 - accuracy: 0.9740 - val_loss: 0.0987 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.09669\n",
      "Epoch 643/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0862 - accuracy: 0.9678 - val_loss: 0.0984 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.09669\n",
      "Epoch 644/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0836 - accuracy: 0.9678 - val_loss: 0.0996 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.09669\n",
      "Epoch 645/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0766 - accuracy: 0.9755 - val_loss: 0.1155 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.09669\n",
      "Epoch 646/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0855 - accuracy: 0.9740 - val_loss: 0.0991 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.09669\n",
      "Epoch 647/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0765 - accuracy: 0.9770 - val_loss: 0.0959 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00647: val_loss improved from 0.09669 to 0.09586, saving model to ./model_100\\647-0.0959_train.hdf5\n",
      "Epoch 648/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0756 - accuracy: 0.9770 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.09586\n",
      "Epoch 649/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.1023 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.09586\n",
      "Epoch 650/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0770 - accuracy: 0.9740 - val_loss: 0.1014 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.09586\n",
      "Epoch 651/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0763 - accuracy: 0.9755 - val_loss: 0.0995 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.09586\n",
      "Epoch 652/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0753 - accuracy: 0.9770 - val_loss: 0.0994 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.09586\n",
      "Epoch 653/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0751 - accuracy: 0.9770 - val_loss: 0.1002 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.09586\n",
      "Epoch 654/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0749 - accuracy: 0.9770 - val_loss: 0.0965 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.09586\n",
      "Epoch 655/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0743 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00655: val_loss improved from 0.09586 to 0.09532, saving model to ./model_100\\655-0.0953_train.hdf5\n",
      "Epoch 656/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0755 - accuracy: 0.9786 - val_loss: 0.0957 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.09532\n",
      "Epoch 657/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0751 - accuracy: 0.9786 - val_loss: 0.0968 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.09532\n",
      "Epoch 658/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0749 - accuracy: 0.9786 - val_loss: 0.0985 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.09532\n",
      "Epoch 659/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0756 - accuracy: 0.9786 - val_loss: 0.0999 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.09532\n",
      "Epoch 660/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0760 - accuracy: 0.9755 - val_loss: 0.0991 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.09532\n",
      "Epoch 661/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0752 - accuracy: 0.9801 - val_loss: 0.0958 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.09532\n",
      "Epoch 662/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0744 - accuracy: 0.9786 - val_loss: 0.0952 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00662: val_loss improved from 0.09532 to 0.09525, saving model to ./model_100\\662-0.0952_train.hdf5\n",
      "Epoch 663/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0739 - accuracy: 0.9770 - val_loss: 0.0966 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.09525\n",
      "Epoch 664/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0995 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.09525\n",
      "Epoch 665/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0735 - accuracy: 0.9770 - val_loss: 0.1003 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.09525\n",
      "Epoch 666/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0735 - accuracy: 0.9770 - val_loss: 0.0966 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.09525\n",
      "Epoch 667/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0950 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00667: val_loss improved from 0.09525 to 0.09505, saving model to ./model_100\\667-0.0950_train.hdf5\n",
      "Epoch 668/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0737 - accuracy: 0.9755 - val_loss: 0.0960 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.09505\n",
      "Epoch 669/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0724 - accuracy: 0.9786 - val_loss: 0.0976 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.09505\n",
      "Epoch 670/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0722 - accuracy: 0.9770 - val_loss: 0.1005 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.09505\n",
      "Epoch 671/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0730 - accuracy: 0.9786 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.09505\n",
      "Epoch 672/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0724 - accuracy: 0.9770 - val_loss: 0.0961 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.09505\n",
      "Epoch 673/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0722 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.09505\n",
      "Epoch 674/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.0968 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.09505\n",
      "Epoch 675/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0716 - accuracy: 0.9770 - val_loss: 0.0960 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.09505\n",
      "Epoch 676/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0725 - accuracy: 0.9786 - val_loss: 0.0973 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.09505\n",
      "Epoch 677/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.0995 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.09505\n",
      "Epoch 678/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0716 - accuracy: 0.9770 - val_loss: 0.0956 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.09505\n",
      "Epoch 679/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0708 - accuracy: 0.9786 - val_loss: 0.0940 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00679: val_loss improved from 0.09505 to 0.09403, saving model to ./model_100\\679-0.0940_train.hdf5\n",
      "Epoch 680/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0715 - accuracy: 0.9770 - val_loss: 0.0952 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.09403\n",
      "Epoch 681/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0718 - accuracy: 0.9786 - val_loss: 0.0959 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.09403\n",
      "Epoch 682/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0711 - accuracy: 0.9786 - val_loss: 0.0947 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.09403\n",
      "Epoch 683/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0707 - accuracy: 0.9786 - val_loss: 0.1005 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.09403\n",
      "Epoch 684/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0727 - accuracy: 0.9740 - val_loss: 0.1043 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.09403\n",
      "Epoch 685/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0731 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.09403 to 0.09328, saving model to ./model_100\\685-0.0933_train.hdf5\n",
      "Epoch 686/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0714 - accuracy: 0.9786 - val_loss: 0.0947 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.09328\n",
      "Epoch 687/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0774 - accuracy: 0.9709 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.09328\n",
      "Epoch 688/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0717 - accuracy: 0.9786 - val_loss: 0.1120 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.09328\n",
      "Epoch 689/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0778 - accuracy: 0.9709 - val_loss: 0.0951 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.09328\n",
      "Epoch 690/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0688 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.09328\n",
      "Epoch 691/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0792 - accuracy: 0.9694 - val_loss: 0.0934 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.09328\n",
      "Epoch 692/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0731 - accuracy: 0.9755 - val_loss: 0.1100 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.09328\n",
      "Epoch 693/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0774 - accuracy: 0.9740 - val_loss: 0.0970 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.09328\n",
      "Epoch 694/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0948 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.09328\n",
      "Epoch 695/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0786 - accuracy: 0.9694 - val_loss: 0.0936 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.09328\n",
      "Epoch 696/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0741 - accuracy: 0.9724 - val_loss: 0.1015 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.09328\n",
      "Epoch 697/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0727 - accuracy: 0.9755 - val_loss: 0.1035 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.09328\n",
      "Epoch 698/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0718 - accuracy: 0.9755 - val_loss: 0.0926 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00698: val_loss improved from 0.09328 to 0.09255, saving model to ./model_100\\698-0.0926_train.hdf5\n",
      "Epoch 699/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0733 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00699: val_loss improved from 0.09255 to 0.09250, saving model to ./model_100\\699-0.0925_train.hdf5\n",
      "Epoch 700/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0696 - accuracy: 0.9755 - val_loss: 0.1035 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.09250\n",
      "Epoch 701/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0730 - accuracy: 0.9755 - val_loss: 0.1028 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.09250\n",
      "Epoch 702/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0739 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.09250\n",
      "Epoch 703/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0687 - accuracy: 0.9786 - val_loss: 0.0941 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.09250\n",
      "Epoch 704/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00704: val_loss did not improve from 0.09250\n",
      "Epoch 705/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0946 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.09250\n",
      "Epoch 706/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0681 - accuracy: 0.9770 - val_loss: 0.0972 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.09250\n",
      "Epoch 707/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0690 - accuracy: 0.9770 - val_loss: 0.0983 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.09250\n",
      "Epoch 708/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.0969 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.09250\n",
      "Epoch 709/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0676 - accuracy: 0.9770 - val_loss: 0.0941 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.09250\n",
      "Epoch 710/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0950 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.09250\n",
      "Epoch 711/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0702 - accuracy: 0.9724 - val_loss: 0.0983 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.09250\n",
      "Epoch 712/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0701 - accuracy: 0.9755 - val_loss: 0.0974 - val_accuracy: 0.9627\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.09250\n",
      "Epoch 713/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0701 - accuracy: 0.9770 - val_loss: 0.0975 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.09250\n",
      "Epoch 714/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0682 - accuracy: 0.9755 - val_loss: 0.0966 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.09250\n",
      "Epoch 715/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0680 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.09250\n",
      "Epoch 716/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.0944 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.09250\n",
      "Epoch 717/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 0.0964 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.09250\n",
      "Epoch 718/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0675 - accuracy: 0.9755 - val_loss: 0.0957 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.09250\n",
      "Epoch 719/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0674 - accuracy: 0.9770 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.09250\n",
      "Epoch 720/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0668 - accuracy: 0.9786 - val_loss: 0.0971 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.09250\n",
      "Epoch 721/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0670 - accuracy: 0.9770 - val_loss: 0.0988 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.09250\n",
      "Epoch 722/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0685 - accuracy: 0.9755 - val_loss: 0.0968 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.09250\n",
      "Epoch 723/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.09250\n",
      "Epoch 724/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0671 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.09250\n",
      "Epoch 725/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.09250\n",
      "Epoch 726/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0662 - accuracy: 0.9786 - val_loss: 0.0937 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.09250\n",
      "Epoch 727/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0666 - accuracy: 0.9786 - val_loss: 0.0963 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.09250\n",
      "Epoch 728/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0676 - accuracy: 0.9770 - val_loss: 0.0963 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.09250\n",
      "Epoch 729/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0667 - accuracy: 0.9770 - val_loss: 0.0927 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.09250\n",
      "Epoch 730/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.0926 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.09250\n",
      "Epoch 731/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0672 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.09250\n",
      "Epoch 732/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0657 - accuracy: 0.9770 - val_loss: 0.0958 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.09250\n",
      "Epoch 733/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.0975 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.09250\n",
      "Epoch 734/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.09250\n",
      "Epoch 735/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 0.0923 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00735: val_loss improved from 0.09250 to 0.09229, saving model to ./model_100\\735-0.0923_train.hdf5\n",
      "Epoch 736/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0658 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.09229\n",
      "Epoch 737/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0653 - accuracy: 0.9770 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.09229\n",
      "Epoch 738/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0651 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.09229\n",
      "Epoch 739/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.0955 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.09229\n",
      "Epoch 740/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0658 - accuracy: 0.9755 - val_loss: 0.0973 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.09229\n",
      "Epoch 741/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.09229\n",
      "Epoch 742/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00742: val_loss improved from 0.09229 to 0.09158, saving model to ./model_100\\742-0.0916_train.hdf5\n",
      "Epoch 743/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0656 - accuracy: 0.9786 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.09158\n",
      "Epoch 744/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.0941 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.09158\n",
      "Epoch 745/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.0965 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.09158\n",
      "Epoch 746/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0648 - accuracy: 0.9755 - val_loss: 0.0992 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.09158\n",
      "Epoch 747/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0651 - accuracy: 0.9755 - val_loss: 0.0943 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.09158\n",
      "Epoch 748/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0648 - accuracy: 0.9770 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.09158\n",
      "Epoch 749/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0652 - accuracy: 0.9740 - val_loss: 0.0967 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.09158\n",
      "Epoch 750/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0654 - accuracy: 0.9755 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.09158\n",
      "Epoch 751/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.0930 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.09158\n",
      "Epoch 752/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.09158\n",
      "Epoch 753/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.1008 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.09158\n",
      "Epoch 754/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.0975 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.09158\n",
      "Epoch 755/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0688 - accuracy: 0.9740 - val_loss: 0.0915 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00755: val_loss improved from 0.09158 to 0.09152, saving model to ./model_100\\755-0.0915_train.hdf5\n",
      "Epoch 756/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0659 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.09152\n",
      "Epoch 757/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.0974 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.09152\n",
      "Epoch 758/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.09152\n",
      "Epoch 759/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0627 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.09152\n",
      "Epoch 760/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0629 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.09152\n",
      "Epoch 761/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0622 - accuracy: 0.9770 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.09152\n",
      "Epoch 762/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0630 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.09152\n",
      "Epoch 763/1500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0635 - accuracy: 0.9786 - val_loss: 0.0930 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.09152\n",
      "Epoch 764/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0622 - accuracy: 0.9770 - val_loss: 0.0968 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.09152\n",
      "Epoch 765/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0635 - accuracy: 0.9755 - val_loss: 0.0982 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.09152\n",
      "Epoch 766/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0637 - accuracy: 0.9755 - val_loss: 0.0917 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.09152\n",
      "Epoch 767/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0632 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.09152\n",
      "Epoch 768/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0665 - accuracy: 0.9770 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.09152\n",
      "Epoch 769/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0634 - accuracy: 0.9786 - val_loss: 0.0994 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.09152\n",
      "Epoch 770/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0652 - accuracy: 0.9740 - val_loss: 0.1000 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.09152\n",
      "Epoch 771/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0650 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.09152\n",
      "Epoch 772/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0631 - accuracy: 0.9770 - val_loss: 0.0933 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.09152\n",
      "Epoch 773/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0674 - accuracy: 0.9740 - val_loss: 0.0949 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.09152\n",
      "Epoch 774/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 0.1040 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.09152\n",
      "Epoch 775/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0657 - accuracy: 0.9770 - val_loss: 0.0922 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.09152\n",
      "Epoch 776/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0638 - accuracy: 0.9740 - val_loss: 0.0950 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.09152\n",
      "Epoch 777/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0707 - accuracy: 0.9724 - val_loss: 0.0929 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.09152\n",
      "Epoch 778/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0978 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.09152\n",
      "Epoch 779/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0671 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.09152\n",
      "Epoch 780/1500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0626 - accuracy: 0.9801 - val_loss: 0.0911 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00780: val_loss improved from 0.09152 to 0.09110, saving model to ./model_100\\780-0.0911_train.hdf5\n",
      "Epoch 781/1500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.0910 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00781: val_loss improved from 0.09110 to 0.09101, saving model to ./model_100\\781-0.0910_train.hdf5\n",
      "Epoch 782/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.0968 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.09101\n",
      "Epoch 783/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.0996 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.09101\n",
      "Epoch 784/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0649 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.09101\n",
      "Epoch 785/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.0917 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.09101\n",
      "Epoch 786/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.1012 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.09101\n",
      "Epoch 787/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0647 - accuracy: 0.9755 - val_loss: 0.0969 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.09101\n",
      "Epoch 788/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0604 - accuracy: 0.9786 - val_loss: 0.0912 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.09101\n",
      "Epoch 789/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0629 - accuracy: 0.9786 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.09101\n",
      "Epoch 790/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0647 - accuracy: 0.9740 - val_loss: 0.0920 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.09101\n",
      "Epoch 791/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.1014 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.09101\n",
      "Epoch 792/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0640 - accuracy: 0.9740 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00792: val_loss improved from 0.09101 to 0.09082, saving model to ./model_100\\792-0.0908_train.hdf5\n",
      "Epoch 793/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0589 - accuracy: 0.9801 - val_loss: 0.0941 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.09082\n",
      "Epoch 794/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.09082\n",
      "Epoch 795/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0648 - accuracy: 0.9755 - val_loss: 0.0955 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.09082\n",
      "Epoch 796/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0612 - accuracy: 0.9770 - val_loss: 0.0954 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.09082\n",
      "Epoch 797/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.09082\n",
      "Epoch 798/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0606 - accuracy: 0.9801 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.09082\n",
      "Epoch 799/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0607 - accuracy: 0.9801 - val_loss: 0.0912 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.09082\n",
      "Epoch 800/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0588 - accuracy: 0.9786 - val_loss: 0.0959 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.09082\n",
      "Epoch 801/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0604 - accuracy: 0.9770 - val_loss: 0.0987 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.09082\n",
      "Epoch 802/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0615 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.09082\n",
      "Epoch 803/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0594 - accuracy: 0.9786 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.09082\n",
      "Epoch 804/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0592 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.09082\n",
      "Epoch 805/1500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.09082\n",
      "Epoch 806/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0582 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.09082\n",
      "Epoch 807/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.09082\n",
      "Epoch 808/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0596 - accuracy: 0.9801 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.09082\n",
      "Epoch 809/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0598 - accuracy: 0.9770 - val_loss: 0.0932 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.09082\n",
      "Epoch 810/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0587 - accuracy: 0.9801 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00810: val_loss improved from 0.09082 to 0.09078, saving model to ./model_100\\810-0.0908_train.hdf5\n",
      "Epoch 811/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.0910 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.09078\n",
      "Epoch 812/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0599 - accuracy: 0.9770 - val_loss: 0.0911 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.09078\n",
      "Epoch 813/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0582 - accuracy: 0.9801 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.09078\n",
      "Epoch 814/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0576 - accuracy: 0.9786 - val_loss: 0.0948 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.09078\n",
      "Epoch 815/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 0.0952 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.09078\n",
      "Epoch 816/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0603 - accuracy: 0.9786 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.09078\n",
      "Epoch 817/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0596 - accuracy: 0.9801 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.09078\n",
      "Epoch 818/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.09078\n",
      "Epoch 819/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.0935 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.09078\n",
      "Epoch 820/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0577 - accuracy: 0.9786 - val_loss: 0.0933 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.09078\n",
      "Epoch 821/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0916 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.09078\n",
      "Epoch 822/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0586 - accuracy: 0.9801 - val_loss: 0.0919 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.09078\n",
      "Epoch 823/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.0963 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.09078\n",
      "Epoch 824/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0583 - accuracy: 0.9786 - val_loss: 0.0944 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.09078\n",
      "Epoch 825/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0565 - accuracy: 0.9786 - val_loss: 0.0920 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.09078\n",
      "Epoch 826/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0591 - accuracy: 0.9770 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.09078\n",
      "Epoch 827/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.0954 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.09078\n",
      "Epoch 828/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0579 - accuracy: 0.9770 - val_loss: 0.0977 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00828: val_loss did not improve from 0.09078\n",
      "Epoch 829/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0595 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.09078\n",
      "Epoch 830/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.0920 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.09078\n",
      "Epoch 831/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0571 - accuracy: 0.9816 - val_loss: 0.0942 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.09078\n",
      "Epoch 832/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0583 - accuracy: 0.9801 - val_loss: 0.0952 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.09078\n",
      "Epoch 833/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.09078\n",
      "Epoch 834/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.09078\n",
      "Epoch 835/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0563 - accuracy: 0.9786 - val_loss: 0.0953 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.09078\n",
      "Epoch 836/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0588 - accuracy: 0.9770 - val_loss: 0.0943 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.09078\n",
      "Epoch 837/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0587 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.09078\n",
      "Epoch 838/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0611 - accuracy: 0.9816 - val_loss: 0.0910 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.09078\n",
      "Epoch 839/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0567 - accuracy: 0.9786 - val_loss: 0.0986 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.09078\n",
      "Epoch 840/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.0985 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.09078\n",
      "Epoch 841/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0585 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.09078\n",
      "Epoch 842/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0595 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.09078\n",
      "Epoch 843/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0617 - accuracy: 0.9770 - val_loss: 0.0916 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.09078\n",
      "Epoch 844/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.0996 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.09078\n",
      "Epoch 845/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.09078\n",
      "Epoch 846/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0601 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.09078\n",
      "Epoch 847/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0580 - accuracy: 0.9816 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.09078\n",
      "Epoch 848/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0564 - accuracy: 0.9770 - val_loss: 0.0976 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.09078\n",
      "Epoch 849/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0590 - accuracy: 0.9770 - val_loss: 0.0950 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.09078\n",
      "Epoch 850/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0565 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.09078\n",
      "Epoch 851/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0560 - accuracy: 0.9786 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.09078\n",
      "Epoch 852/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.09078\n",
      "Epoch 853/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0568 - accuracy: 0.9801 - val_loss: 0.0987 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.09078\n",
      "Epoch 854/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0572 - accuracy: 0.9786 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.09078\n",
      "Epoch 855/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0587 - accuracy: 0.9786 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00855: val_loss improved from 0.09078 to 0.09068, saving model to ./model_100\\855-0.0907_train.hdf5\n",
      "Epoch 856/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 0.0957 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.09068\n",
      "Epoch 857/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0583 - accuracy: 0.9770 - val_loss: 0.0938 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.09068\n",
      "Epoch 858/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0561 - accuracy: 0.9770 - val_loss: 0.0894 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00858: val_loss improved from 0.09068 to 0.08940, saving model to ./model_100\\858-0.0894_train.hdf5\n",
      "Epoch 859/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0578 - accuracy: 0.9816 - val_loss: 0.0898 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.08940\n",
      "Epoch 860/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.0992 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.08940\n",
      "Epoch 861/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0582 - accuracy: 0.9770 - val_loss: 0.0907 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.08940\n",
      "Epoch 862/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0548 - accuracy: 0.9801 - val_loss: 0.0921 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.08940\n",
      "Epoch 863/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0596 - accuracy: 0.9770 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.08940\n",
      "Epoch 864/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0546 - accuracy: 0.9801 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.08940\n",
      "Epoch 865/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0610 - accuracy: 0.9801 - val_loss: 0.0952 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.08940\n",
      "Epoch 866/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0525 - accuracy: 0.9801 - val_loss: 0.0934 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.08940\n",
      "Epoch 867/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0638 - accuracy: 0.9740 - val_loss: 0.0925 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.08940\n",
      "Epoch 868/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.1050 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.08940\n",
      "Epoch 869/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0619 - accuracy: 0.9816 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.08940\n",
      "Epoch 870/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0595 - accuracy: 0.9786 - val_loss: 0.0910 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.08940\n",
      "Epoch 871/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.0909 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.08940\n",
      "Epoch 872/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0587 - accuracy: 0.9786 - val_loss: 0.0927 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.08940\n",
      "Epoch 873/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0544 - accuracy: 0.9770 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.08940\n",
      "Epoch 874/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0545 - accuracy: 0.9770 - val_loss: 0.0908 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.08940\n",
      "Epoch 875/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.9801 - val_loss: 0.0899 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.08940\n",
      "Epoch 876/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.0902 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.08940\n",
      "Epoch 877/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0554 - accuracy: 0.9786 - val_loss: 0.0906 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.08940\n",
      "Epoch 878/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0547 - accuracy: 0.9816 - val_loss: 0.0932 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.08940\n",
      "Epoch 879/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0550 - accuracy: 0.9801 - val_loss: 0.0981 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.08940\n",
      "Epoch 880/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0553 - accuracy: 0.9786 - val_loss: 0.0905 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.08940\n",
      "Epoch 881/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0545 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.08940\n",
      "Epoch 882/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0586 - accuracy: 0.9816 - val_loss: 0.0925 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.08940\n",
      "Epoch 883/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0548 - accuracy: 0.9801 - val_loss: 0.1103 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.08940\n",
      "Epoch 884/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0635 - accuracy: 0.9801 - val_loss: 0.0912 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.08940\n",
      "Epoch 885/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0565 - accuracy: 0.9816 - val_loss: 0.0960 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.08940\n",
      "Epoch 886/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0638 - accuracy: 0.9770 - val_loss: 0.0915 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.08940\n",
      "Epoch 887/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.1084 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.08940\n",
      "Epoch 888/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0633 - accuracy: 0.9801 - val_loss: 0.0970 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.08940\n",
      "Epoch 889/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0538 - accuracy: 0.9801 - val_loss: 0.0924 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.08940\n",
      "Epoch 890/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0584 - accuracy: 0.9801 - val_loss: 0.0922 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.08940\n",
      "Epoch 891/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.0922 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.08940\n",
      "Epoch 892/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0529 - accuracy: 0.9786 - val_loss: 0.0979 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.08940\n",
      "Epoch 893/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0560 - accuracy: 0.9755 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.08940\n",
      "Epoch 894/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0527 - accuracy: 0.9770 - val_loss: 0.0922 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.08940\n",
      "Epoch 895/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.0920 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.08940\n",
      "Epoch 896/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.08940\n",
      "Epoch 897/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0524 - accuracy: 0.9816 - val_loss: 0.0918 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.08940\n",
      "Epoch 898/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0531 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.08940\n",
      "Epoch 899/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0540 - accuracy: 0.9801 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.08940\n",
      "Epoch 900/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0522 - accuracy: 0.9801 - val_loss: 0.0977 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.08940\n",
      "Epoch 901/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0544 - accuracy: 0.9770 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.08940\n",
      "Epoch 902/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0512 - accuracy: 0.9770 - val_loss: 0.0931 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.08940\n",
      "Epoch 903/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0549 - accuracy: 0.9801 - val_loss: 0.0939 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.08940\n",
      "Epoch 904/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0569 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.08940\n",
      "Epoch 905/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.0970 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.08940\n",
      "Epoch 906/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0535 - accuracy: 0.9786 - val_loss: 0.0943 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.08940\n",
      "Epoch 907/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0526 - accuracy: 0.9786 - val_loss: 0.0913 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.08940\n",
      "Epoch 908/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.08940\n",
      "Epoch 909/1500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.0928 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.08940\n",
      "Epoch 910/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0505 - accuracy: 0.9801 - val_loss: 0.0918 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.08940\n",
      "Epoch 911/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0517 - accuracy: 0.9816 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.08940\n",
      "Epoch 912/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0939 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.08940\n",
      "Epoch 913/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.0979 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.08940\n",
      "Epoch 914/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0531 - accuracy: 0.9786 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.08940\n",
      "Epoch 915/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0534 - accuracy: 0.9801 - val_loss: 0.0928 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.08940\n",
      "Epoch 916/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.0965 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.08940\n",
      "Epoch 917/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0520 - accuracy: 0.9770 - val_loss: 0.1000 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.08940\n",
      "Epoch 918/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0524 - accuracy: 0.9786 - val_loss: 0.0921 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.08940\n",
      "Epoch 919/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.0923 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.08940\n",
      "Epoch 920/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.0911 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.08940\n",
      "Epoch 921/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0520 - accuracy: 0.9801 - val_loss: 0.0918 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.08940\n",
      "Epoch 922/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0503 - accuracy: 0.9801 - val_loss: 0.0901 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.08940\n",
      "Epoch 923/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0901 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.08940\n",
      "Epoch 924/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.0916 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.08940\n",
      "Epoch 925/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0510 - accuracy: 0.9816 - val_loss: 0.0922 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.08940\n",
      "Epoch 926/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0505 - accuracy: 0.9801 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.08940\n",
      "Epoch 927/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0504 - accuracy: 0.9786 - val_loss: 0.0909 - val_accuracy: 0.9689\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.08940\n",
      "Epoch 928/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0520 - accuracy: 0.9832 - val_loss: 0.0911 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.08940\n",
      "Epoch 929/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0510 - accuracy: 0.9847 - val_loss: 0.0963 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.08940\n",
      "Epoch 930/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0514 - accuracy: 0.9786 - val_loss: 0.0996 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.08940\n",
      "Epoch 931/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0518 - accuracy: 0.9770 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.08940\n",
      "Epoch 932/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0514 - accuracy: 0.9832 - val_loss: 0.0929 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.08940\n",
      "Epoch 933/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0532 - accuracy: 0.9801 - val_loss: 0.0938 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.08940\n",
      "Epoch 934/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0507 - accuracy: 0.9786 - val_loss: 0.0968 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.08940\n",
      "Epoch 935/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0502 - accuracy: 0.9786 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.08940\n",
      "Epoch 936/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.0925 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.08940\n",
      "Epoch 937/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.0964 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.08940\n",
      "Epoch 938/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0502 - accuracy: 0.9786 - val_loss: 0.0945 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.08940\n",
      "Epoch 939/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0490 - accuracy: 0.9801 - val_loss: 0.0927 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.08940\n",
      "Epoch 940/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.0931 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.08940\n",
      "Epoch 941/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0493 - accuracy: 0.9816 - val_loss: 0.0981 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.08940\n",
      "Epoch 942/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0509 - accuracy: 0.9786 - val_loss: 0.0982 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.08940\n",
      "Epoch 943/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0503 - accuracy: 0.9770 - val_loss: 0.0946 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.08940\n",
      "Epoch 944/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0507 - accuracy: 0.9862 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.08940\n",
      "Epoch 945/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0511 - accuracy: 0.9816 - val_loss: 0.0951 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.08940\n",
      "Epoch 946/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.0931 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.08940\n",
      "Epoch 947/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0492 - accuracy: 0.9847 - val_loss: 0.0933 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.08940\n",
      "Epoch 948/1500\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 0.0942 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.08940\n",
      "Epoch 949/1500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0488 - accuracy: 0.9786 - val_loss: 0.0949 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.08940\n",
      "Epoch 950/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0493 - accuracy: 0.9770 - val_loss: 0.0935 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.08940\n",
      "Epoch 951/1500\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.0924 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.08940\n",
      "Epoch 952/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.0923 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.08940\n",
      "Epoch 953/1500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.0928 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.08940\n",
      "Epoch 954/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0489 - accuracy: 0.9816 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.08940\n",
      "Epoch 955/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.0928 - val_accuracy: 0.9720\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.08940\n",
      "Epoch 956/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0485 - accuracy: 0.9847 - val_loss: 0.0947 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.08940\n",
      "Epoch 957/1500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 0.0961 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.08940\n",
      "Epoch 958/1500\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.0936 - val_accuracy: 0.9752\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.08940\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일 및 학습\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 저장 폴더 지정\n",
    "MODEL_DIR = './model_100/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "\n",
    "# 모델 저장방법\n",
    "modelpath = './model_100/{epoch:02d}-{val_loss:.4f}_train.hdf5'\n",
    "\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath,\n",
    "                               monitor = 'val_loss',\n",
    "                               verbose = 1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "# 학습 조기 종료\n",
    "# from keras.callbacks import EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience = 100)\n",
    "# 100번동안 테스트 셋 오차가 나아지지 않으면 중단\n",
    "\n",
    "history = model_2.fit(X, Y, validation_split = 0.33,\n",
    "                    epochs=1500, batch_size = 500,\n",
    "                    callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94046acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 905us/step - loss: 0.0744 - accuracy: 0.9766\n",
      "[0.07436311990022659, 0.9766045808792114]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_2_test = load_model('./model_100/858-0.0894_train.hdf5')\n",
    "\n",
    "#학습할 데이터 설정\n",
    "df = pd.read_csv('../dataset/wine.csv', header = None)\n",
    "X =df.values[:,0:12]\n",
    "Y =df.values[:,12]\n",
    "#평가\n",
    "print(model_2_test.evaluate(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "096d9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9625\n",
      "[0.10228940844535828, 0.9624573588371277]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.92105770e-04],\n",
       "       [4.33242321e-03],\n",
       "       [1.26123428e-03],\n",
       "       [7.09366560e-01],\n",
       "       [2.49615014e-02],\n",
       "       [2.18909979e-03],\n",
       "       [1.34682655e-03],\n",
       "       [9.81753469e-01],\n",
       "       [5.06579876e-04],\n",
       "       [4.63862270e-05],\n",
       "       [1.04374886e-02],\n",
       "       [5.22482395e-03],\n",
       "       [6.68105640e-05],\n",
       "       [1.03935599e-03],\n",
       "       [9.99976754e-01],\n",
       "       [1.69932842e-04],\n",
       "       [1.21314526e-02],\n",
       "       [4.60326672e-02],\n",
       "       [7.67320395e-04],\n",
       "       [5.66393137e-04],\n",
       "       [9.19014215e-04],\n",
       "       [1.01870298e-02],\n",
       "       [3.14980745e-02],\n",
       "       [8.15987587e-04],\n",
       "       [9.98662293e-01],\n",
       "       [9.70673084e-01],\n",
       "       [8.60781074e-02],\n",
       "       [9.98550415e-01],\n",
       "       [1.12060276e-04],\n",
       "       [2.56001949e-04],\n",
       "       [9.84213114e-01],\n",
       "       [2.24834681e-03],\n",
       "       [9.95114088e-01],\n",
       "       [2.80106068e-03],\n",
       "       [6.02424145e-04],\n",
       "       [1.06132030e-03],\n",
       "       [8.25944662e-01],\n",
       "       [8.84633660e-02],\n",
       "       [3.16023827e-04],\n",
       "       [2.29749143e-01],\n",
       "       [9.97521996e-01],\n",
       "       [1.34974718e-04],\n",
       "       [9.88573074e-01],\n",
       "       [9.99461472e-01],\n",
       "       [2.42948532e-04],\n",
       "       [9.81623173e-01],\n",
       "       [9.99276876e-01],\n",
       "       [9.95733023e-01],\n",
       "       [9.87086833e-01],\n",
       "       [6.94536805e-01],\n",
       "       [9.96407568e-01],\n",
       "       [9.93957400e-01],\n",
       "       [1.50781870e-03],\n",
       "       [2.35319138e-04],\n",
       "       [6.43894746e-05],\n",
       "       [2.13056803e-04],\n",
       "       [1.61707401e-04],\n",
       "       [1.14044249e-02],\n",
       "       [4.64242697e-03],\n",
       "       [1.52767599e-02],\n",
       "       [3.99580598e-03],\n",
       "       [1.89439654e-02],\n",
       "       [9.99389827e-01],\n",
       "       [3.34331095e-02],\n",
       "       [1.55717134e-04],\n",
       "       [4.51529026e-03],\n",
       "       [7.57777691e-03],\n",
       "       [9.16033983e-04],\n",
       "       [1.61111355e-04],\n",
       "       [9.74849820e-01],\n",
       "       [2.25189030e-02],\n",
       "       [5.96593916e-02],\n",
       "       [2.08995938e-02],\n",
       "       [7.09263040e-05],\n",
       "       [1.24011040e-02],\n",
       "       [4.09930944e-04],\n",
       "       [3.22162232e-05],\n",
       "       [1.36655569e-03],\n",
       "       [9.32178497e-01],\n",
       "       [9.98809218e-01],\n",
       "       [9.57543910e-01],\n",
       "       [3.33492398e-01],\n",
       "       [1.87371671e-01],\n",
       "       [9.26091313e-01],\n",
       "       [1.24007463e-03],\n",
       "       [1.61448121e-03],\n",
       "       [9.91578221e-01],\n",
       "       [9.96430993e-01],\n",
       "       [1.94697082e-02],\n",
       "       [3.28340828e-02],\n",
       "       [9.22542810e-03],\n",
       "       [1.88171864e-04],\n",
       "       [1.09794736e-03],\n",
       "       [1.37192875e-01],\n",
       "       [4.26024199e-04],\n",
       "       [9.98861849e-01],\n",
       "       [5.35604358e-03],\n",
       "       [8.50677073e-01],\n",
       "       [2.95103490e-02],\n",
       "       [3.14980745e-02],\n",
       "       [5.46345115e-03],\n",
       "       [4.11182642e-04],\n",
       "       [9.98224914e-01],\n",
       "       [4.68811393e-03],\n",
       "       [7.35120511e-06],\n",
       "       [1.14735961e-03],\n",
       "       [1.36607587e-02],\n",
       "       [6.70805275e-02],\n",
       "       [2.07453966e-04],\n",
       "       [9.87628460e-01],\n",
       "       [9.83285372e-06],\n",
       "       [3.95783782e-03],\n",
       "       [9.67987180e-01],\n",
       "       [1.02539062e-02],\n",
       "       [1.48791075e-03],\n",
       "       [1.74526870e-02],\n",
       "       [2.39658654e-02],\n",
       "       [6.56935573e-03],\n",
       "       [6.91378882e-05],\n",
       "       [5.13881445e-04],\n",
       "       [1.24709666e-01],\n",
       "       [8.56664777e-03],\n",
       "       [1.85284704e-01],\n",
       "       [9.82065678e-01],\n",
       "       [1.23316944e-02],\n",
       "       [2.11709738e-03],\n",
       "       [4.35900539e-01],\n",
       "       [9.91866887e-01],\n",
       "       [1.56125426e-03],\n",
       "       [5.56319952e-04],\n",
       "       [7.33104348e-03],\n",
       "       [2.77873874e-03],\n",
       "       [8.13931227e-04],\n",
       "       [2.61056423e-03],\n",
       "       [1.17933452e-02],\n",
       "       [9.98836696e-01],\n",
       "       [1.65671110e-04],\n",
       "       [1.43469870e-02],\n",
       "       [3.35201621e-03],\n",
       "       [2.24477053e-03],\n",
       "       [1.45995617e-02],\n",
       "       [9.53111594e-05],\n",
       "       [2.62365937e-02],\n",
       "       [3.40637565e-03],\n",
       "       [8.98127764e-05],\n",
       "       [1.03049874e-02],\n",
       "       [1.79711897e-05],\n",
       "       [4.39171781e-05],\n",
       "       [2.68652439e-02],\n",
       "       [9.73880291e-04],\n",
       "       [1.02221966e-03],\n",
       "       [8.77778530e-01],\n",
       "       [2.25681394e-01],\n",
       "       [9.89620209e-01],\n",
       "       [9.12818313e-03],\n",
       "       [9.60851252e-01],\n",
       "       [1.23921037e-03],\n",
       "       [9.99587953e-01],\n",
       "       [1.74284250e-01],\n",
       "       [2.04214454e-03],\n",
       "       [4.12008166e-03],\n",
       "       [6.85191699e-05],\n",
       "       [1.63316727e-04],\n",
       "       [1.66755915e-03],\n",
       "       [9.10550356e-04],\n",
       "       [2.51223901e-05],\n",
       "       [1.02239705e-04],\n",
       "       [4.77081537e-03],\n",
       "       [9.85794306e-01],\n",
       "       [5.47063351e-03],\n",
       "       [2.18462944e-03],\n",
       "       [4.92751598e-04],\n",
       "       [2.13533640e-04],\n",
       "       [4.21524048e-04],\n",
       "       [9.98139143e-01],\n",
       "       [6.88546206e-05],\n",
       "       [9.77277040e-01],\n",
       "       [6.06155882e-05],\n",
       "       [1.35147572e-03],\n",
       "       [1.77592039e-04],\n",
       "       [1.14639181e-04],\n",
       "       [2.84818113e-02],\n",
       "       [8.79225135e-01],\n",
       "       [3.32526267e-02],\n",
       "       [6.07980146e-05],\n",
       "       [5.26266932e-01],\n",
       "       [9.74724472e-01],\n",
       "       [9.98355150e-01],\n",
       "       [6.98036909e-01],\n",
       "       [3.17037106e-04],\n",
       "       [6.68049543e-05],\n",
       "       [1.30682588e-02],\n",
       "       [2.55736709e-03],\n",
       "       [9.65519667e-01],\n",
       "       [9.98117805e-01],\n",
       "       [9.99434114e-01],\n",
       "       [5.18620014e-04],\n",
       "       [1.40040517e-02],\n",
       "       [1.30036175e-02],\n",
       "       [4.51350212e-03],\n",
       "       [1.18291378e-03],\n",
       "       [9.95836616e-01],\n",
       "       [1.26689672e-04],\n",
       "       [9.96980369e-01],\n",
       "       [2.65091658e-04],\n",
       "       [3.82840633e-04],\n",
       "       [9.97425675e-01],\n",
       "       [2.25552052e-01],\n",
       "       [9.95137751e-01],\n",
       "       [1.28945708e-03],\n",
       "       [4.26140428e-03],\n",
       "       [5.85359412e-05],\n",
       "       [1.16428733e-03],\n",
       "       [4.97957766e-02],\n",
       "       [1.85975432e-03],\n",
       "       [9.86230493e-01],\n",
       "       [1.56316161e-03],\n",
       "       [9.79954541e-01],\n",
       "       [9.86133456e-01],\n",
       "       [3.39139402e-02],\n",
       "       [3.66240740e-04],\n",
       "       [2.65359879e-04],\n",
       "       [2.95014743e-05],\n",
       "       [3.16182077e-02],\n",
       "       [3.84032726e-04],\n",
       "       [9.23297584e-01],\n",
       "       [9.99909997e-01],\n",
       "       [4.55230474e-04],\n",
       "       [9.90216970e-01],\n",
       "       [3.35597506e-05],\n",
       "       [1.15416348e-02],\n",
       "       [1.77831948e-02],\n",
       "       [5.55324793e-01],\n",
       "       [4.80699539e-03],\n",
       "       [2.43158937e-02],\n",
       "       [9.90925908e-01],\n",
       "       [4.31776047e-04],\n",
       "       [9.99653459e-01],\n",
       "       [1.40368938e-04],\n",
       "       [8.00803304e-03],\n",
       "       [1.33133829e-02],\n",
       "       [3.45855951e-04],\n",
       "       [1.31421493e-05],\n",
       "       [9.22169565e-05],\n",
       "       [1.43170357e-04],\n",
       "       [6.41775131e-03],\n",
       "       [9.84430313e-04],\n",
       "       [4.60535288e-04],\n",
       "       [1.07333064e-03],\n",
       "       [8.69680047e-01],\n",
       "       [6.10798597e-04],\n",
       "       [9.95908618e-01],\n",
       "       [1.01783872e-03],\n",
       "       [2.52979994e-03],\n",
       "       [2.78179646e-02],\n",
       "       [2.24834681e-03],\n",
       "       [9.95125175e-01],\n",
       "       [3.44262719e-02],\n",
       "       [9.99317527e-01],\n",
       "       [9.98014331e-01],\n",
       "       [4.73760556e-05],\n",
       "       [6.13659620e-03],\n",
       "       [6.18636608e-04],\n",
       "       [3.03640962e-02],\n",
       "       [2.08150148e-02],\n",
       "       [1.80587173e-03],\n",
       "       [1.27807260e-03],\n",
       "       [1.00421828e-04],\n",
       "       [1.41208470e-01],\n",
       "       [4.39316034e-03],\n",
       "       [1.86562538e-04],\n",
       "       [9.98860240e-01],\n",
       "       [3.36650014e-03],\n",
       "       [9.95252192e-01],\n",
       "       [3.30230862e-01],\n",
       "       [9.95125175e-01],\n",
       "       [3.16904225e-05],\n",
       "       [9.97650027e-01],\n",
       "       [1.41710043e-04],\n",
       "       [5.70841432e-02],\n",
       "       [3.41325998e-04],\n",
       "       [9.56773162e-01],\n",
       "       [8.84188652e-01],\n",
       "       [2.11864710e-04],\n",
       "       [2.01115012e-03],\n",
       "       [5.75423241e-04],\n",
       "       [2.02855468e-03],\n",
       "       [7.33415484e-02],\n",
       "       [4.43774462e-03],\n",
       "       [9.83285372e-06],\n",
       "       [1.48329139e-03],\n",
       "       [3.45051289e-04],\n",
       "       [2.20596790e-04]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2_test.evaluate(X_test, Y_test))\n",
    "y_pred = model_2_test.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89bab05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b82f4aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred.reshape(-1,1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd35ef77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_real = Y_test.reshape(-1,1)\n",
    "Y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df5724ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   7,  14,  24,  25,  27,  30,  32,  36,  40,  42,  43,  45,\n",
       "        46,  47,  48,  49,  50,  51,  62,  78,  79,  80,  81,  82,  83,\n",
       "        86,  87,  95,  97, 102, 109, 112, 123, 127, 135, 151, 153, 155,\n",
       "       157, 158, 168, 174, 176, 182, 186, 187, 188, 193, 194, 195, 201,\n",
       "       203, 206, 207, 208, 215, 217, 218, 225, 226, 228, 233, 235, 237,\n",
       "       249, 251, 256, 257, 258, 259, 271, 273, 274, 275, 277, 279, 281,\n",
       "       282], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss = np.where(Y_real != y_pred)[0]\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b78e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : 47 의 실제값 : [1.], 예측값 : [0]\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(miss)\n",
    "print(\"index : {} 의 실제값 : {}, 예측값 : {}\".format(i, Y_real[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ffa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
